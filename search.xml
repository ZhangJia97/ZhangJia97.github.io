<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring核心容器简介BeanFactory，ApplicationContext]]></title>
    <url>%2F2021%2F08%2F17%2FSpring%E6%A0%B8%E5%BF%83%E5%AE%B9%E5%99%A8%E7%AE%80%E4%BB%8B-BeanFactory%EF%BC%8CApplicationContext%2F</url>
    <content type="text"><![CDATA[在Spring中容器的实现类并不是唯一的，Spring框架中提供了多个容器的实现。主要分为两套体系：一套是早期的BeanFactory体系，还有一个就是ApplicationContext，也被成为服务上下文，它继承了BeanFactory，除了BeanFactory的功能外还提供了事务，AOP，国际化的消息源以及应用程序事务处理等企业级服务。 1.BeanFactory首先我们来看一下BeanFacotry源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public interface BeanFactory &#123; /** * 主要是用于FactoryBean的转义进行定义，因为当使用FactoryBean创建Bean时，使用名字进行检索获取的是FactoryBean所创建的Bean对象 * 而需要使用转义来获取FactoryBean本身。 */ String FACTORY_BEAN_PREFIX = "&amp;"; /** * 根据bean的名称获取容器中的实例，这个bean可以是单例bean也可以是原型bean。 * 当无法获取指定名称的bean将会抛出NoSuchBeanDefinitionException异常。 */ Object getBean(String name) throws BeansException; /** * 根据bean的名称和bean的类型来获取容器中的bean实例，这个bean可以是单例bean也可以是原型bean。 * 当获取的bean不是所需类型，则抛出BeanNotOfRequiredTypeException异常。可以简单理解成增加了安全验证机制。 */ &lt;T&gt; T getBean(String name, Class&lt;T&gt; requiredType) throws BeansException; /** * 根据名称来获取bean，第二个参数args可以用来给bean进行赋值。复制的方式有两种，构造方法和工厂方法。 * 但是通过这种方式获取的bean必须为原型bean而不是单例bean。 */ Object getBean(String name, Object... args) throws BeansException; /** * 根据bean类型来获取容器中的bean，当bean不唯一是抛出NoUniqueBeanDefinitionException异常。 * 当没有获得对应bean时抛出NoSuchBeanDefinitionException异常。 */ &lt;T&gt; T getBean(Class&lt;T&gt; requiredType) throws BeansException; /** * 该方法与getBean(String name, Class&lt;T&gt; requiredType)方法类似 */ &lt;T&gt; T getBean(Class&lt;T&gt; requiredType, Object... args) throws BeansException; /** * 获取指定bean的提供者，可以简单理解为，假设一个bean是通过FactoryBean生成，此处将返回创建该bean的FactoryBean。 * 调用这个方法返回的是一个对象的实例。此接口通常用于封装一个泛型工厂，在每次调用的时候返回一些目标对象新的实例。 * ObjectFactory和FactoryBean是类似的，只不过FactoryBean通常被定义为BeanFactory中的服务提供者（SPI）实例， * 而ObjectFactory通常是以API的形式提供给其他的bean。简单的来说，ObjectFactory一般是提供给开发者使用的， * FactoryBean一般是提供给BeanFactory使用的。 */ &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(Class&lt;T&gt; requiredType); /** * 同上 */ &lt;T&gt; ObjectProvider&lt;T&gt; getBeanProvider(ResolvableType requiredType); /** * 判断当前容器是否存在某种名称的bean */ boolean containsBean(String name); /** * 判断bean是否为单例bean */ boolean isSingleton(String name) throws NoSuchBeanDefinitionException; /** * 判断bean是否为原型bean */ boolean isPrototype(String name) throws NoSuchBeanDefinitionException; /** * 根据bean名称，判断是否有指定类型匹配 */ boolean isTypeMatch(String name, ResolvableType typeToMatch) throws NoSuchBeanDefinitionException; /** * 同上 */ boolean isTypeMatch(String name, Class&lt;?&gt; typeToMatch) throws NoSuchBeanDefinitionException; /** * 获取bean的类型，对于FactoryBean，返回Factory.getObjectType()返回的类型。 */ @Nullable Class&lt;?&gt; getType(String name) throws NoSuchBeanDefinitionException; /** * 返回给定bean的别名（如果有的话），在getBean调用中，所有这些别名都指向同一个bean。 * 如果给定的名称是别名，则返回相应的原始bean名称和其他别名。 */ String[] getAliases(String name);&#125; 2、ApplicationContextApplicationContext为BeanFactory的子类，它不仅包含BeanFactory的所有功能，还对其进行了扩展。1234567891011121314151617181920212223242526272829303132333435363738public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver &#123; /** * 返回当前应用程序上下文的唯一ID */ @Nullable String getId(); /** * 返回当前应用程序上下文所属应用程序的名称 */ String getApplicationName(); /** * 获取当前应用程序上下文的具象化类名 */ String getDisplayName(); /** * 获取当前应用上下文第一次加载时的时间戳 */ long getStartupDate(); /** * 获取父级应用程序上下文，如果没有则返回null */ @Nullable ApplicationContext getParent(); /** * 通过getAutowireCapableBeanFactory这个方法将 AutowireCapableBeanFactory这个接口暴露给外部使用， * AutowireCapableBeanFactory这个接口一般在applicationContext的内部是较少使用的， * 它的功能主要是为了装配applicationContext管理之外的Bean。 */ AutowireCapableBeanFactory getAutowireCapableBeanFactory() throws IllegalStateException;&#125; ApplicationContext本身实现的方法较少，也比较简单，但是他通过继承六个接口来实现了Bean相关的方法等常用功能。ApplicationContext有多种实现类，其中最重要的两类是ConfigurableApplicationContext和WebApplicationContext。 2.1、ConfigurableApplicationContext123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public interface ConfigurableApplicationContext extends ApplicationContext, Lifecycle, Closeable &#123; /** * 下面这些字符会被认为是用于分割上下文配置路径的分隔符 */ String CONFIG_LOCATION_DELIMITERS = ",; \t\n"; /** * 在BeanFactory中ConversionService对应的bean名称。如果没有实现该类的实例，则使用默认的转换规则。 */ String CONVERSION_SERVICE_BEAN_NAME = "conversionService"; /** * LoadTimeWaver类所对应的Bean在容器中的名字。如果提供了该实例，上下文会使用临时的 ClassLoader。 * 这样，LoadTimeWaver就可以使用bean确切的类型了 */ String LOAD_TIME_WEAVER_BEAN_NAME = "loadTimeWeaver"; /** * Environment在容器中的Bean名称 */ String ENVIRONMENT_BEAN_NAME = "environment"; /** * System系统变量在容器中对应的Bean名称 */ String SYSTEM_PROPERTIES_BEAN_NAME = "systemProperties"; /** * System环境变量在容器中对应的Bean名称 */ String SYSTEM_ENVIRONMENT_BEAN_NAME = "systemEnvironment"; /** * 给当前应用上下文设置唯一ID */ void setId(String id); /** * 为当前容器设置父容器 */ void setParent(@Nullable ApplicationContext parent); /** * 设置当前容器的环境变量 */ void setEnvironment(ConfigurableEnvironment environment); /** * 以ConfigurableEnvironment的形式返回当前容器的环境变量。 */ @Override ConfigurableEnvironment getEnvironment(); /** * 为当前容器新增一个BeanFactoryPostProcessor。增加的BeanFactoryPostProcessor将在容器进行refresh操作后使用。 */ void addBeanFactoryPostProcessor(BeanFactoryPostProcessor postProcessor); /** * 为当前容器新增一个ApplicationListener，增加的Listener将用于发布上下文时间。如初始化刷新容器，关闭容器。 */ void addApplicationListener(ApplicationListener&lt;?&gt; listener); /** * 为当前容器新增一个ProtocolResolver，ProtocolResolver主要用于自定义协议的解析。 * 比如spring就有一个 “classpath:”开头的特定协议（但是spring并不是自定义ProtocolResolver 实现来完成这个功能的） */ void addProtocolResolver(ProtocolResolver resolver); /** * 加载或者刷新配置，可能是xml文件或者是properties文件，或者是连接数据源。 * 由于这是一个初始化方法，当前方法执行失败时，则已经创建的Bean也会销毁。 * 所以调用此方法时，要么所有的Bean都实例化成功，要么一个Bean都没有实例化 */ void refresh() throws BeansException, IllegalStateException; /** * 向JVM注册一个关闭当前应用上下文的回调函数。当前方法可以多次调用，但是一个应用上下文只会有一个回调函数。 */ void registerShutdownHook(); /** * 关闭当前应用程序上下文，释放所占用对的资源和锁，并且销毁所有已经创建的单例Bean * 该close()方法不会调用父类的close方法。父级应用上下文有自己的生命周期。 * 这个方法可以被多次调用而没有副作用：对已经关闭的上下文的后续close调用将被忽略。 */ @Override void close(); /** * 判断当前应用上下文是否处于启动状态，即是否至少执行一次refresh() */ boolean isActive(); /** * 获取当前应用上下文的BeanFactory容器。不要使用当前方法对BeanFactory生成的Bean进行后置处理，因为此时Bean已经初始化完成。 * 应该是用BeanFactoryPostProcessor来在Bean声称之前对其进行处理。在获取容器是应该保证容器是在启动状态，即在refresh()和close()之间。 */ ConfigurableListableBeanFactory getBeanFactory() throws IllegalStateException;&#125; 2.2、WebApplicationContextWebApplicationCOntext是专门为Web应用所准备的，其允许从相对于Web根目录的路径中加载配置文件完成初始化。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface WebApplicationContext extends ApplicationContext &#123; /** * 整个Web应用上下文是作为属性放置在ServletContext中的，该常量就是应用上下文在ServletContext属性列表中的key */ String ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE = WebApplicationContext.class.getName() + ".ROOT"; /** * request作用域，只要发出一个请求就会创建一个request，它的作用域：尽在当前请求中有效。 * 用处：常用于服务器间同一请求不同页面之间的参数传递，常应用于表单的控件值传递。 * 方法：request.setAttribute(); request.getAttribute(); request.removeAttribute(); request.getParameter(). */ String SCOPE_REQUEST = "request"; /** * 服务器会为每个会话创建一个session对象，所以session中的数据可供当前会话中所有servlet共享。 * 会话：用户打开浏览器会话开始，直到关闭浏览器会话才会结束。一次会话期间只会创建一个session对象。 * 用处：常用于web开发中的登陆验证界面（当用户登录成功后浏览器分配其一个session键值对）。 * 方法：session.setAttribute(); session.getAttribute(); session.removeAttribute(); */ String SCOPE_SESSION = "session"; /** * 作用范围：所有的用户都可以取得此信息，此信息在整个服务器上被保留。Application属性范围值，只要设置一次，则所有的网页窗口都可以取得数据。 * ServletContext在服务器启动时创建，在服务器关闭时销毁，一个JavaWeb应用只创建一个ServletContext对象， * 所有的客户端在访问服务器时都共享同一个ServletContext对象;ServletContext对象一般用于在多个客户端间共享数据时使用; */ String SCOPE_APPLICATION = "application"; /** * 在工厂中的bean名称 */ String SERVLET_CONTEXT_BEAN_NAME = "servletContext"; /** * ServletContext 初始化参数名称 */ String CONTEXT_PARAMETERS_BEAN_NAME = "contextParameters"; /** * 在工厂中 ServletContext 属性值环境bean的名称 */ String CONTEXT_ATTRIBUTES_BEAN_NAME = "contextAttributes"; /** * 用来获取 ServletContext 对象 */ @Nullable ServletContext getServletContext();&#125; 3、ApplicationContext和BeanFactory对比1.ApplicationContext包含BeanFactory的所有特性，所以一般情况下我们都选择使用ApplicationContext，但是在一些限制情况下，如对于内存消耗要求比较严格的时候使用轻量的BeanFactory是更合理的选择。2.在BeanFactory采用延迟加载的形式来注入Bean。只有使用到某个Bean是，才会对Bean进行加载实例化。而ApplicationContext则相反，它在容器启动时，一次性创建了所有Bean。这样我们就能在容器启动时发现Spring存在的配置错误。3.BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 功能/特点 BeanFactory ApplicationContext Bean加载方式 延迟加载 容器启动时加载所有Bean Bean 实例化/装配 有 有 BeanPostProcessor 自动注册 没有 有 BeanFactoryPostProcessor 自动注册 没有 有 MessageSource 便捷访问（针对i18n） 没有 有 ApplicationEvent 发布 没有 有 3、ApplicationContext准备启动 ps:ContextLoaderListener简介https://www.cnblogs.com/xunyi/p/10363290.html 在容器启动时会调用ContextLoaderListener中的contextInitialized()方法 123456789101112131415161718192021222324252627public class ContextLoaderListener extends ContextLoader implements ServletContextListener &#123; public ContextLoaderListener() &#123; &#125; public ContextLoaderListener(WebApplicationContext context) &#123; super(context); &#125; /** * 初始化根web应用程序上下文 */ @Override public void contextInitialized(ServletContextEvent event) &#123; // 调用父类ContextLoader的方法进行web容器初始化 initWebApplicationContext(event.getServletContext()); &#125; /** * 关闭根web应用程序上下文 */ @Override public void contextDestroyed(ServletContextEvent event) &#123; closeWebApplicationContext(event.getServletContext()); ContextCleanupListener.cleanupAttributes(event.getServletContext()); &#125;&#125; 接下来我们查看父类的initWebApplicationListener123456789101112131415161718192021222324252627282930313233343536public class ContextLoader &#123; // ... public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; // .. try &#123; // 将上下文存储在本地实例变量中，以确保在ServletContext关闭时可用。 if (this.context == null) &#123; // 通过 createWebApplicationContext 方法创建应用上下文 this.context = createWebApplicationContext(servletContext); &#125; if (this.context instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context; if (!cwac.isActive()) &#123; ... // 在该方法中调用上下文的 refresh 方法，refresh 就是启动上下文的入口 configureAndRefreshWebApplicationContext(cwac, servletContext); &#125; &#125; ... &#125; ... &#125; ... protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac, ServletContext sc) &#123; ... wac.refresh(); &#125; ...&#125; 使用SpringBoot启动ApplicationContext时首先我们可以看SpringBoot的启动类123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 点击进入SpringApplication的run方法我们可以得知1234567891011121314151617181920public class SpringApplication &#123; public ConfigurableApplicationContext run(String... args) &#123; // ... ConfigurableApplicationContext context = null; try &#123; // ... // 创建一个应用上下文 context = createApplicationContext(); // ... // 进入refreshContext方法可以发现实际是调用refresh启动上下文方法 refreshContext(context); // ... &#125; catch (Throwable ex) &#123; // ... &#125; // ... return context; &#125;&#125; 通过对ContextLoaderListener和SpringApplication两个类的分析我们可以发现，实际最终他们都调用了refresh()方法，所以我们接下来对refresh()进行分析。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 预刷新，做一些准备工作。记录了启动时间戳，标记为活动，非关闭状态，并处理配置文件中的占位符 prepareRefresh(); // 解析配置文件，创建BeanFactory，对Bean进行包装成BeanDefinition ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 对BeanFactory进行增强处理，比如添加BeanPostProcessor，手动注册部分bean等。 prepareBeanFactory(beanFactory); try &#123; // 钩子方法，BeanFactory创建后，对BeanFactory的自定义操作。 postProcessBeanFactory(beanFactory); // 重点：调用了postProcessBeanDefinitionRegistry(registry); // springboot中很多激活自动配置的注解都是通过这里导入的。 invokeBeanFactoryPostProcessors(beanFactory); // 重点：从beanFactory中获取所有的BeanPostProcessor，优先进行getBean操作，实例化 registerBeanPostProcessors(beanFactory); // 国际化支持 initMessageSource(); // 初始化ApplicationEventMulticaster。 如果上下文中未定义，则使用SimpleApplicationEventMulticaster。 // 这里是监听器的支持，监听器时间多播。 initApplicationEventMulticaster(); // 钩子方法，springBoot中的嵌入式tomcat就是通过此方法实现的 onRefresh(); // 监听器注册 registerListeners(); // 重点方法：完成容器中bean的实例化，及代理的生成等操作。 finishBeanFactoryInitialization(beanFactory); // 完成此上下文的刷新，调用LifecycleProcessor的onRefresh（）方法并发布 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("Exception encountered during context initialization - " + "cancelling refresh attempt: " + ex); &#125; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 总结： 容器是Spring中最为重要的概念之一，也是Spring大厦的基石。通过总结我们可以发现，Spring Bean的自动注入大大简化了操作，下一步我们将分析，Spring是如何通过Xml文件或者注解的方式将我们需要的Bean放入容器中的。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>BeanFactory</tag>
        <tag>ApplicationContext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive之行列互转]]></title>
    <url>%2F2020%2F12%2F07%2FHive%E4%B9%8B%E8%A1%8C%E5%88%97%E4%BA%92%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[一、行转列1.1 函数说明CONCAT(string A/col, string B/col...) 返回输入字符串连接后的结果，支持任意个输入字符串 CONCAT_WS(separator, str1, str2,...) 他是一个特殊形式的CONCAT()，第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是NULL返回值也是NULL。这个函数会跳过分隔符参数后的任何NULL和空字符串。分隔符将被加到被连接的字符串之间; COLLECT_SET(col) 函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。 1.2 准备数据 name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 1.3 需求描述将星座和血型一样的人归类到一起123射手座,A 大海|凤姐 白羊座,A 孙悟空|猪八戒 白羊座,B 宋宋 1.4 实现SQLsql1234SELECT t1.base, CONCAT_WS("|", CONCAT_SET(t1.name)) nameFROM(SELECT name, CONCAT(constellation, ",", blood_type) base FROM person_info) t1GROUP BY t1.base; 二、列转行2.1 函数说明EXPLODE(col) 将hive一列中复杂的array或者map结构拆分成多行。 LATERAL VIEW 用法:LATERAL VIEW udtf(expression) tableAlias AS columnAlias 解释:用于和split,explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。 2.2 数据准备 movie category 《疑犯追踪》 悬疑,动作,科幻,剧情 《Lie to me》 悬疑,警匪,动作,心理,剧情 《战狼 2》 战争,动作,灾难 2.3 需求描述将电影分类中的数组数据展开。结果如下:123456789101112《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《Lie to me》 悬疑《Lie to me》 警匪《Lie to me》 动作《Lie to me》 心理《Lie to me》 剧情《战狼 2》 战争《战狼 2》 动作《战狼 2》 灾难 2.4 实现SQLsql12345selectmovie,category_namefrommovie_info lateral view explode(category) table_tmp as category_name;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive之窗口函数]]></title>
    <url>%2F2020%2F11%2F30%2FHive%E4%B9%8B%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[一、常见窗口函数函数 OVER() 指定分析函数工作的数据窗口大小，这个数据窗口的大小可能会随着行的变化而变化。 CURRENT ROW 当前行 n PRECEDING 往前n行数据 n FOLLOWING 往后n行数据 UNBOUNDED 起点，UNBOUNDED PRECEDING表示从前面的起点，UNBOUNDED FOLLOWING表示到后面的终点 LAG(col, n) 往前第n行数据 LEAD(col, n) 往后第n行数据 NTILE(n) 把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。 二、案例实操2.1 数据准备 business表123456789101112131415name，orderdate，costjack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 2.2 查询在2017年4月份购买过的顾客及总人数1234select name, count(*) over()from businesswhere substring(ordergate, 1, 7) = '2017-04'group by name; 2.3 查询顾客的购买明细及月购买总额12select name, orderdate, cost, sum(cost) over(partition by substring(ordergate, 1, 7))from business 2.4 上述的场景,要将cost按照日期进行累加123456789select name,orderdate,cost,sum(cost) over() as sample1,--所有行相加sum(cost) over(partition by name) as sample2,--按name分组，组内 数据相加sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business; 2.5 查询顾客上次的购买时间12select name, orderdate, cost, lag(orderdate, 1, '1900-01-01') over(partition by name, order by orderdate) as time1from business; 2.6 查询前20%时间的订单信息123select *from (select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business)twhere sorted = 1;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive之分桶]]></title>
    <url>%2F2020%2F11%2F27%2FHive%E4%B9%8B%E5%88%86%E6%A1%B6%2F</url>
    <content type="text"><![CDATA[一、分桶及抽样数据存储分区针对的是数据的存储路径；分桶针对的是数据文件。 分区提供一个隔离数据和优化查询到的便利方式。不过并非所有的数据集都可形成合理的分区，特别是之前所提到的要确定合适的划分大小这个疑虑。分桶试讲数据及分解成更容易管理的若干部分的另一个技术。 二、创建分桶表2.1 首先设置相关属性12set hive.ecforce.bucketing=true;set mapreduce.job.reduces=-1; 2.2 创建一个分桶表123create table stu_bucket(id int, name string)clustered by (id) into 4 bucketsrow format delimited field terminated by '\n'; 2.3 将stu表的数据导入分桶表中12insert into table stu_buckselect id, name from stu; 三、分桶抽样调查对于非常大的数据集，又是用户需要使用过的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表的抽样来满足这个需求查询表stu_buck中的数据1select * from stu_buck tablesample(bucket 1 out of 4 on id); tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定取样的比例。例如table总共有4份，如果y=2是，则抽取2个bucket数据， 如果y=8，则抽取1/2个bucket的数据。x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如tablesample(bucket 1 out of 2)，表示总共抽取(4/2=)2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveDML数据操作之导入导出]]></title>
    <url>%2F2020%2F11%2F24%2FHiveDML%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B9%8B%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%2F</url>
    <content type="text"><![CDATA[一、数据导入向表中加载数据Load语法1load data [local] inpath '/opt/module/datas/student.txt' [overwrite] | into table student [partition (partcol1=val1,...)]; load data：表示加载数据local： 表示从本地数据中加载数据到Hive表中，否则是从HDFS中加载数据到Hive表inpath： 表示加载数据的路径overwrite： 表示覆盖表中的已有数据，否则表示追加into table： 表示加载到哪张表student： 表示表名partition： 表示上传到指定的分区 案例实操 创建一张表12create table student(id string, name string)row format fields terminated by '\t'; 加载本地文件到hive1load data local inpath '/opt/module/datas/student.txt' into table default.student; 加载HDFS文件到Hive1load data inpath '/user/suiwo/hive/student.txt' into table default.student; 加载数据覆盖表中的已有的数据上传文件到HDFS1dfs -put /opt/module/datas/student.txt /user/suiwo/hive; 加载数据覆盖表中已有的数据1load data inpath "/user/suiwo/hive/student.txt" overwrite into table default.student; 通过查询语句向表中插入数据12insert overwrite table student partition(month='201708')select id, name from student where month='201709'; 查询语句中创建表并加载数据12create table if not exists student3as select id, name from student; import数据到指定的Hive表中（此处导入的数据格式应该是和导出的数据格式相同）1import table student2 partition(month='201709') from '/user/hive/warehouse/export/student'; 二、数据导出将查询的结果导出到本地（将local去掉便是将数据导出到HDFS）1insert overwrite local directory '/opt/module/datas/export/student1' select * from student; 将查询的结果格式化导出到本地12insert overwrite local directory '/opt/module/datas/export/student1' row format delimited dields terminated by '\t'select * from student; Hadoop命令导出到本地12dfs -get /user/hive/warehouse/student/month=2017/00000_0/opt/module/datas/export/studemt3.txt Hive Shell 命令导出1bin/hive -e 'select * from default.student;' &gt; /opt/module/datas/export/student3.txt; export 导出到HDFS上1export table default.student to '/user/hive/warehouse/export/student']]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveDDL数据操作]]></title>
    <url>%2F2020%2F11%2F23%2FHiveDDL%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一、 Hive集合数据类型除了常见的基本数据类型，Hive还支持三种复杂的数据类型ARRAY、MAP和STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 数据类型 描述 语法示例 STRUCT 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。 struct() MAP MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。 Array() 1. 案例实操假设某表有如下一行，我们用JSON格式来表示其数据结构。下面这个json字符串存在列表，键值对，以及结构体123456789101112131415&#123; "name":"songsong", "friends":[ // Array "bingbing", "lili" ], "children":&#123; // Map "xiao song":18, "xiaoxiao song":19 &#125;, "address":&#123; // Struct "street":"huilongguan", "city":"beijing" &#125;&#125; 现在我们尝试创建对应的表，并将上述的json中包含的数据导入到这个表中。首先我们创建本地的测试文件test.txt12songsong,bingbing_lili,xiaosong:18_xiaoxiaosong:19,huilongguan_beijingyangyang,caicai_susu,xiaoyang:18_xiaoxiaoyang:19,chaoyang_beijing 注意:MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用“_”。 首先我们创建一个数据表12345678910create table test(name string,friend array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by ',' -- 列分隔符collection items terminated by '_' -- MAP，ARRAY，STRUCT的分隔符map keys terminated by ':' -- MAP中的key与value的分隔符lines terminated by '\n'; -- 行分隔符 将文本数据导入到测试表中1load data local inpath "/filepath/test.txt" into table test; 之后我们尝试访问三种集合列中的数据1select friend[1], children['xiaosong'], address.city from test where name = 'songsong'; 执行后可以发现返回结果1234OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 2. 类型转换使用CAST操作显示进行数据类型转换例如CAST(‘1’ AS INT)将把字符串’1’转换成整数1;如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值NULL。 二、 DDL数据操作1.创建数据库为了避免已创建的数据库已存在，建议增加if not exists进行判断1create database if not exists db_hive; 创建数据库可以指定HDFS上存放的位置1create database db_hive location '/db_hive2.db'; 3. 查询数据库显示数据库1show databases; 过滤显示查询的数据库1show databases like 'db_hive*'; 4. 查看数据库详细显示数据库信息1desc database db_hive; 查询数据库详细信息， extended1desc database extended db_hive; 5. 切换当前数据库1use db_hive; 6. 修改数据库用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。1alter database hive set dbproperties('createtime'='20170830'); 7. 删除数据库最好采用if exists判断数据库是否存在1drop database if exists db_hive2; 对于数据库不为空的时候，可以使用cascade命令强制删除1drop database db_hive cascade; 8. 创建表12345678CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name[(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...)[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] 字段解释说明(1)CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常;用户可以用IFNOTEXISTS选项来忽略这个异常。(2)EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际 数据的路径(LOCATION)，Hive创建内部表时，会将数据移动到数据仓库指向的路径;若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。(3)COMMENT:为表和列添加注释。(4)PARTITIONED BY创建分区表(5)CLUSTERED BY创建分桶表(6)SORTED BY不常用(7)ROW FORMATDELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, …)]用户在建表的时候可以自定义SerDe或者使用自带的 SerDe。如果没有指定ROW FORMAT或者ROW FORMAT DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive 通过SerDe确定表的具体的列的数据。SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。(8)STORED AS指定存储文件类型,常用的存储文件类型:SEQUENCEFILE(二进制序列文件)、TEXTFILE(文本)、 RCFILE(列式存储格式文件),如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用STORED AS SEQUENCEFILE。(9)LOCATION:指定表在HDFS上的存储位置。(10)LIKE允许用户复制现有的表结构，但是不复制数据。 9. 内部表，外部表，分区表略 三. 分区表常见操作 创建分区表 123create table dept_partition(deptno int, dname string, loc string)partitioned by (month string)row format delimited fields terminated by '\t'; 加载数据到分区表 123load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201709');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201708');hive (default)&gt; load data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201707’); 创建单个分区 1alter table dept_partition add partition(month='201706') ; 创建多个分区 1alter table dept_partition add partition(month='201705') partition(month='201704'); 删除单个分区 1alter table dept_partition drop partition (month='201704'); 删除多个分区 1alter table dept_partition drop partition (month='201705'), partition (month='201706'); 记得区分创建多个和删除多个的时候，分隔符一个为空格一个为逗号。 创建二级分区(1) 首先创建一个表123create table dept_partition2(deptno int, dname string, loc string)partitioned by (month string, day string)row format delimited fields terminated by '\t'; (2) 正常的加载数据,加载数据到二级分区表中1load data local inpath '/filepath/test.txt' into table default.dept_partition2 partition(moth='201709', day='13'); (3) 查询分区数据1select * from dept_partition2 where month='201709' and day='13'; 当我们把数据直接上传到分区目录上的时候，我们需要让分区表和数据产生关联，一共有三种方式 (1) 上传数据后修复12dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 此时我们是查询不到数据的，需要执行修复指令1msck repair table dept_partition2; 这个时候便可以查询到数据了 (2) 上传数据后添加分区12dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 执行添加分区1alter table dept_partition2 add partition(month='201709',day='11'); (3) 创建文件夹后load数据到分区12dfs -mkdir -p /user/hive/warehouse/dept_partition2/month=201709/day=12;dfs -put /opt/module/datas/dept.txt /user/hive/warehouse/dept_partition2/month=201709/day=12; 上传数据1load data local inpath '/opt/module/datas/dept.txt' into table dept_partition2 partition(month='201709',day='10');]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveDML数据操作之查询]]></title>
    <url>%2F2020%2F11%2F23%2FHiveDML%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C%E4%B9%8B%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[一、常用函数总行数1select count(*) from emp; 最大值1select max(id) from emp; 最小值1select min(id) from emp; 总和1select sum(id) from emp; 平均值1select avg(id) from emp; 二、案例实操2.1 比较运算符查出薪水大于1000的虽有员工emp1select * from emp where sal &gt; 1000; 查出薪水等于1000的员工1select * from emp where sal = 1000; 查询工资在1000到5000的员工1select * from emp where sal between 1000 and 5000; 查询comm为空的所有员工1select * from emp where comm is null; 查询工资是1000或者5000的员工1select * from emp where sal in (1000, 5000); 2.2 Like RLike查询薪资是以2开头的员工1select * from emp where sal like '2%'; 查询薪水中第二个数值为2的员工信息1select * from emp where sal like '_2%'; 查询薪水中含有2的员工信息 (rlike是Hive的一个扩展功能，可以通过正则表达式来指定匹配条件)1select * from emp where sal rlike '[2]'; 2.3 Group By计算各个部门的平均工资1select deptno, avg(sal) from emp group by deptno; 计算各部门中各个岗位的最高薪水1select deptno, job, max(sql) from emp group by deptno, job; 2.4 Having1.having 与 where 不同点(1)where 针对表中的列发挥作用，查询数据;having 针对查询结果中的列发挥作用， 筛选数据。(2)where 后面不能写聚合函数，而 having 后面可以使用聚合函数。 (3)having 只用于 group by 分组统计语句。计算平均薪资大于2000的部门1select deptno, avg(sal) as avg_sal from emp group by deptno having avg_sal &gt; 2000; 2.5 JOINJoin 连接谓词中不支持 or左连接1select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno; 右连接1select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno; 内连接1select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno; 满外连接（将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。）1select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno; 笛卡尔积1.笛卡尔集会在下面条件下产生(1)省略连接条件 (2)连接条件无效 (3)所有表中的所有行互相连接1select empno, dname from emp, dept; 2.6 排序 Order By: 全局排序，一个Reducer Sort By: 每个Reducer内部进行排序，对全局结果集来说不是排序。 Distribute By: 类似MR中partition，进行分区，结合sort by使用。 Cluster By: 当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。 2.7 分桶表略 2.8 CASE WHEN 数据准备 name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 123456selectdept_id,sum(case sex when '男' then 1 else 0 end) male_count, sum(case sex when '女' then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 2.9 其他常用的查询函数空字段赋值NVL赋值，它的格式是NVL(string1, replace_with)。它的功能是如果string1是NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都是NULL，则返回NULL。 1)date_format:格式化时间1234hive(default)&gt; select date_format('2019-06-29','yyyy-MM-dd');OK_c02019-06-29 2)date_add:时间跟天数相加12345678hive (default)&gt; select date_add('2019-06-29',5);OK_c02019-07-04hive (default)&gt; select date_add('2019-06-29',-5);OK_c02019-06-24 3)date_sub:时间跟天数相减123456789101112hive (default)&gt; select date_sub('2019-06-29',5);OK_c02019-06-24hive (default)&gt; select date_sub('2019-06-29 12:12:12',5);OK_c02019-06-24hive (default)&gt; select date_sub('2019-06-29',-5);OK_c02019-07-04 4)datediff:两个时间相减12345678910111213141516hive (default)&gt; select datediff('2019-06-29','2019-06-24');OK_c05hive (default)&gt; select datediff('2019-06-24','2019-06-29');OK_c0-5hive (default)&gt; select datediff('2019-06-24 12:12:12','2019-06-29');OK_c0-5hive (default)&gt; select datediff('2019-06-24 12:12:12','2019-06-29 13:13:13');OK_c0-5]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive基础]]></title>
    <url>%2F2020%2F11%2F23%2FHive%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[一、 Hive介绍Hive是为了解决海量结构化日志的数据统计框架，他是一个基于Hadoop的数据仓库工具，可以将结构化的数据映射为一张表，并提供类SQL的查询功能。本质就是将HQL转化成MR程序。简化流程可以理解成： 编写SQL。 Hive匹配出相对应的MR模板并将将SQL转化成MR程序。 运行MR程序，生成相应的分析结果。 将结果写入持久化存储。 Hive优点： 类SQL语句，简单易上手。 自动转化成MR程序，减少开发人员学习成本。 Hive支持自定义函数，用户可以根据自己需求来实现自己的函数。 Hive缺点： 效率低，自动生成的MR程序通常情况下不够智能，并且调优比较困难。 HQL表达能力有限。 只适合实时性要求不高的场合。 二、 Hive架构 Hive提供了三种用户接口CLI(Hive shell)： CLI启动的时候，会同时启动一个Hive副本，CLI会连接到client是Hive的客户端会连接到Hive Server。在启动Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。JDBC/ODBC：使用Java访问Hive。WUI：通过浏览器访问Hive。 元数据MetastoreHive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表所属的数据库(默认是 default)，表的拥有者，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 HadoopHive使用HDFS进行存储，使用MR进行计算。 驱动器:Driver解析器(SQL Parser):将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr;对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。编译器(Physical Plan):将AST编译生成逻辑执行计划。优化器(Query Optimizer):对逻辑执行计划进行优化。执行器(Execution):把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是将其转化成MR/Spark。 三、 Hive与数据库进行比较查询语言： SQL被广泛应用在数据仓库中，因此专门针对Hive的提醒设计了类SQL的查询语句HQL。熟悉SQL的开发者可以很方便的使用Hive进行开发。数据存储位： Hive是数据Hadoop生态下的一个数据仓库工具，显然，Hive中的数据是存放在HDFS中的。而数据库一般则是将数据保存在块设备或者本地文件系统中。数据更新：Hive是针对数据仓库应用设计的，而数据仓库的内容一般是读多写少。因此Hive中不建议对数据进行改写。所有数据应当在加载时就确认好。而数据库中的数据通常是需要经常进行修改的，所以使用INSERT，UPDATE比较常见。索引：Hive在加载数据的过程中不会对数据进行任何的处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key创建索引。Hive想要访问满足条件的特定值的时候，需要使用暴力扫描，因此访问数据的延迟比较大。也正因为延迟大，所以Hive不适合在线数据查询。因为引入了MR，Hive可以进行并行访问数据，所以即使没有索引，在遇到大数据量的访问时，Hive仍然可以体现出优势。数据库中，通常会对常用的列创建一个或者多个索引，因此可以有较高的效率以及较小的延迟。执行： Hive中对的大多数查询的执行都是通过MR来实现的。而数据库通常是存在自己的执行引擎的。执行延迟： Hive由于没有索引，需要扫描整个表，因此延迟较高。并且由于MR自身的延迟就较高，所以在使用MR执行Hive查询的时候，也会有较高的延迟。所以在数据量小的时候数据库的执行能力更好，但是当数据规模比较大，达到超过了数据库的数据处理能力的时候，Hive并行查询能力的优势显然能体现出来了。可扩展性： 由Hive是建立在Hadoop之上的，因此Hive的可扩展性适合Hadoop的可扩展性一致的。数据库由于ACID的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。数据规模： Hive是建立在集群上的，可以利用MR进行并行计算，因此可以执行很大规模的数据。对应的数据库可以支持的数据规模较小。 四、 Hive常见操作在操作Hive之前，我们需要部署好Hive，这部分可以参考Hive初识中的介绍。并且由于Hive默认的Metastore为derby的性能限制，建议将Metastore改成MYSQL，详细更改方式同样参考Hive初识。启动Hive 进行Hive文件夹下对的bin目录，我们可以执行./hive启动hive客户端 查看数据库1hive&gt; show databases; 打开默认数据库1hive&gt; use default; 显示 default 数据库中的表1hive&gt; show tables; 创建一张表1hive&gt; create table student(id int, name string); 显示数据库中有几张表1hive&gt; show tables; 查看表的结构1hive&gt; desc student; 向表中插入数据1hive&gt; insert into student values(1000,&quot;ss&quot;); 查询表中数据1hive&gt; select * from student; 退出1hive hive&gt; quit;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MR学习总结]]></title>
    <url>%2F2020%2F08%2F07%2FMR%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文是在尚硅谷MapReduce课程后的课后总结 一、 MapReduce介绍MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。 二、 MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程：| 进程类型 | 作用 || — | — || MrAppMaster | 负责整个程序的过程调度及状态协调 || MapTask | 负责Map阶段的整个数据处理流程 || ReduceTask | 负责Reduce阶段的整个数据处理流程 | 三、 常见数据序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable 四、 maven包准备1. 在项目中添加以下pom配置1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2. 在resource目录下添加日志相关配置文件 log4j.properties12345678log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%nlog4j.appender.logfile=org.apache.log4j.FileAppenderlog4j.appender.logfile.File=target/spring.loglog4j.appender.logfile.layout=org.apache.log4j.PatternLayoutlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n 五、 实现WordCount1. 统计数据如下12345hello hadoophello hdfshello scalamapreduceword count 2. 期望输出数据：1234567count 1hadoop 1hdfs 1hello 3mapreduce 1scala 1word 1 3. 编写Mapper方法1234567891011121314public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private Text k = new Text(); private IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] lineArr = line.split(" "); for (String s : lineArr) &#123; k.set(s); context.write(k, v); &#125; &#125;&#125; 4. 编写Reducer方法12345678910public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; 5. 编写Driver方法12345678910111213141516171819202122232425public class WordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/input_path", "/output/path"&#125;; Configuration conf = new Configuration(); // 获取Job对象 Job job = Job.getInstance(conf); // 设置jar存储位置 job.setJarByClass(WordCountDriver.class); // 关联map和reduce job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 设置mapper阶段输出数据 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置最终数据输出类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 设置程序输入路径和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交job boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 6. 执行Driver方法可在输出路径下看到结果与预期一致 六、 自定义序列化对象1. 什么是序列化序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁盘（持久化）和网络传输。反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换成内存中的对象。 2. 自定义bean对象实现序列化接口（Writable）实现一个bean对象的序列化需要以下7个步骤 类必须实现Writable接口 反序列化的时候，需要反射调用无参构造函数，所以类必须有无参构造函数 123public WritableBean()&#123; super();&#125; 重写序列化方法 1234@Overridepublic void write(DataOutput dataOutput) throws IOException &#123; // todo&#125; 重写反序列化方法 1234@Overridepublic void readFields(DataInput dataInput) throws IOException &#123; // todo&#125; 反序列化顺序和序列化顺序必须完全一致。否则会导致反序列化失败 要想把结果显示在文件中，需要重写toString()，可用”\t”分开，方便后续用 如果自定义bean需要在key中传输，需要实现Comparable接口，因为MapReduce的Shuffle阶段要求key是可排序的1234@Overridepublic int compareTo(WritableBean o) &#123; // todo&#125; 3. 案例实操1. 需求如下：需要统计下列数据中每个手机号的总上行流量，总下行流量以及总流量123456789101112131415161718192021221 13736230513 192.196.100.1 www.atguigu.com 2481 24681 2002 13846544121 192.196.100.2 264 0 2003 13956435636 192.196.100.3 132 1512 2004 13966251146 192.168.100.1 240 0 4045 18271575951 192.168.100.2 www.atguigu.com 1527 2106 2006 84188413 192.168.100.3 www.atguigu.com 4116 1432 2007 13590439668 192.168.100.4 1116 954 2008 15910133277 192.168.100.5 www.hao123.com 3156 2936 2009 13729199489 192.168.100.6 240 0 20010 13630577991 192.168.100.7 www.shouhu.com 6960 690 20011 15043685818 192.168.100.8 www.baidu.com 3659 3538 20012 15959002129 192.168.100.9 www.atguigu.com 1938 180 50013 13560439638 192.168.100.10 918 4938 20014 13470253144 192.168.100.11 180 180 20015 13682846555 192.168.100.12 www.qq.com 1938 2910 20016 13992314666 192.168.100.13 www.gaga.com 3008 3720 20017 13509468723 192.168.100.14 www.qinghua.com 7335 110349 40418 18390173782 192.168.100.15 www.sogou.com 9531 2412 20019 13975057813 192.168.100.16 www.baidu.com 11058 48243 20020 13768778790 192.168.100.17 120 120 20021 13568436656 192.168.100.18 www.alibaba.com 2481 24681 20022 13568436656 192.168.100.19 1116 954 200 2. 期望输出数据：12345678910111213141516171819202113470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 3. 创建流量统计Bean对象1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class FlowBean implements Writable &#123; // 上行流量 private long upFlow; // 下行流量 private long downFlow; // 总流量 private long sumFlow; public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; upFlow = dataInput.readLong(); downFlow = dataInput.readLong(); sumFlow = dataInput.readLong(); &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125;&#125; 4. 编写Mapper类123456789101112131415161718192021222324public class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; private Text k = new Text(); private FlowBean flowBean = new FlowBean(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一行 String line = value.toString(); // 切割 String[] lineArr = line.split("\t"); // 封装对象 k.set(lineArr[1]); long upFLow = Long.parseLong(lineArr[lineArr.length - 3]); long downFLow = Long.parseLong(lineArr[lineArr.length - 2]); flowBean.setUpFlow(upFLow); flowBean.setDownFlow(downFLow); flowBean.setSumFlow(upFLow + downFLow); // 写出 context.write(k, flowBean); &#125;&#125; 5. 编写Reducer类1234567891011121314151617181920public class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; private FlowBean flowBean = new FlowBean(); @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sumUpFlow = 0; long sumDownFlow = 0; // 累计求和 for (FlowBean value : values) &#123; sumDownFlow += value.getDownFlow(); sumUpFlow += value.getUpFlow(); &#125; flowBean.setUpFlow(sumUpFlow); flowBean.setDownFlow(sumDownFlow); flowBean.setSumFlow(sumDownFlow + sumUpFlow); // 写出 context.write(key, flowBean); &#125;&#125; 6. 编写Driver类123456789101112131415161718public class FlowCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/input_path", "/output_path"&#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowCountDriver.class); job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 7. 执行Driver类可得统计结果与预期一致 七、 FileInputFormat实现类介绍FileInputFormat常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、NLineInputFormat、CombineTextInputFormat和自定义InputFormat等。 1. TextInputFormatTextInputFormat是默认的FileInputFormat实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text类型。以下是一个示例，比如，一个分片包含了如下4条文本记录。1234Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 每条记录表示为以下键/值对：1234(0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 2. KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中添加以下配置来设定分隔符。默认分隔符是tab（\t）。1conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, "\t"); 以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符。1234line1 ——&gt;Rich learning formline2 ——&gt;Intelligent learning engineline3 ——&gt;Learning more convenientline4 ——&gt;From the real demand for more close to the enterprise 每条记录表示为以下键/值对：1234(line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) 此时的键是每行排在制表符之前的Text序列。 3. NLineInputFormat如果使用NlineInputFormat，代表每个map进程处理的InputSplit不再按Block块去划分，而是按NlineInputFormat指定的行数N来划分。即输入文件的总行数/N=切片数，如果不整除，切片数=商+1。以下是一个示例，仍然以上面的4行输入为例。1234Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 例如，如果N是2，则每个输入分片包含两行。开启2个MapTask。12(0,Rich learning form)(19,Intelligent learning engine) 另一个 mapper 则收到后两行：12(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 这里的键和值与TextInputFormat生成的一样。 4. CombineTextInputFormat框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下。设置虚拟存储切片最大值方法12//注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m （1）虚拟存储过程：将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。（2）切片过程：（a）判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。（b）如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。（c）测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为：1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M）最终会形成3个切片，大小分别为：（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M 八、 KeyValueTextInputFormat实操1. 需求如下：统计输入文件中每一行的第一个单词相同的行数。1234banzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhang 2. 期望输出数据：12banzhang 2xihuan 2 3. 编写Mapper类1234567public class KVTextMapper extends Mapper&lt;Text, Text, Text, IntWritable&gt; &#123; IntWritable intWritable = new IntWritable(1); @Override protected void map(Text key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(key, intWritable); &#125;&#125; 4. 编写Reducer类123456789101112public class KVTextReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; IntWritable intWritable = new IntWritable(); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; intWritable.set(sum); context.write(key, intWritable); &#125;&#125; 5. 编写Driver类123456789101112131415161718192021public class KVTextDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/input_path", "/output_path"&#125;; Configuration conf = new Configuration(); // 设置切割符 conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " "); Job job = Job.getInstance(conf); job.setJarByClass(KVTextDriver.class); job.setMapperClass(KVTextMapper.class); job.setReducerClass(KVTextReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setInputFormatClass(KeyValueTextInputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 6. 执行Driver方法发现输出结果与期望数据一致 九、 NLineInputFormat实操1. 需求如下：对每个单词进行个数统计，要求根据每个输入文件的行数来规定输出多少个切片。此案例要求每三行放入一个切片中。1234567891011banzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhangbanzhang ni haoxihuan hadoop banzhang banzhang ni haoxihuan hadoop banzhang 2. 期待输出数据：在日志中打印的分区数为41Number of splits:4 3. 编写Mapper类12345678910111213public class NLineMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] strArr = line.split(" "); for (String s : strArr) &#123; k.set(s); context.write(k, v); &#125; &#125;&#125; 4. 编写Reducer类123456789101112public class NLineReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; IntWritable v = new IntWritable(1); @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; v.set(sum); context.write(key, v); &#125;&#125; 5. 编写Driver类12345678910111213141516171819202122public class NLineDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/input_path", "/output_path"&#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); // 设置每个切片InputSplit中划分三条记录 NLineInputFormat.setNumLinesPerSplit(job, 3); // 使用NLineInputFormat处理记录数 job.setInputFormatClass(NLineInputFormat.class); job.setJarByClass(NLineDriver.class); job.setMapperClass(NLineMapper.class); job.setReducerClass(NLineReducer.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 自定义InputFormat无论HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。 1．需求将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的key-value对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为key，文件内容为value。输入数据为三个文件，所存储的数据分别如下1.txt12yongpeng weidong weinansanfeng luozong xiaoming 2.txt1234longlong fanfanmazong kailun yuhang yixinlonglong fanfanmazong kailun yuhang yixin 3.txt12shuaige changmo zhenqiang dongli lingu xuanxuan 2. 自定义InputFormat流程 自定义一个类继承FileInputFormat（1）重写isSplitable()方法，返回false不可切割（2）重写createRecordReader()，创建自定义的RecordReader对象，并初始化 改写RecordReader，实现一次读取一个完整文件封装为KV（1）采用IO流一次读取一个文件输出到value中，因为设置了不可切片，最终把所有文件都封装到了value中（2）获取文件路径信息+名称，并设置key 设置Driver1234// （1）设置输入的自定义inputFormatjob.setInputFormatClass(XXXFileInputformat.class);// （2）设置输出的outputFormatjob.setOutputFormatClass(SequenceFileOutputFormat.class); 3. 实现WholeFileInputFormat123456789public class WholeFileInputFormat extends FileInputFormat&lt;Text, BytesWritable&gt; &#123; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; WholeRecordReader recordReader = new WholeRecordReader(); recordReader.initialize(inputSplit, taskAttemptContext); return recordReader; &#125;&#125; 4. 实现WholeRecordReader类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class WholeRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; FileSplit split; Configuration configuration; Text k = new Text(); BytesWritable v = new BytesWritable(); boolean isProgress = true; @Override public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; // 初始化 this.split = (FileSplit) inputSplit; this.configuration = taskAttemptContext.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; // 核心业务 if (isProgress)&#123; // 1. 获取fileSystem对象 Path path = split.getPath(); FileSystem fileSystem = path.getFileSystem(configuration); // 2. 获取输入流 FSDataInputStream fsDataInputStream = fileSystem.open(path); // 3. 拷贝 byte[] buf = new byte[(int) split.getLength()]; IOUtils.readFully(fsDataInputStream, buf, 0, buf.length); // 4. 封装kv k.set(path.toString()); v.set(buf, 0, buf.length); // 5. 关闭资源 IOUtils.closeStream(fsDataInputStream); isProgress = false; return true; &#125; return false; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return k; &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return v; &#125; @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123; &#125;&#125; 5. 编写Mapper方法123456public class SequenceFileMapper extends Mapper&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void map(Text key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; context.write(key, value); &#125;&#125; 6. 编写Reducer方法12345678public class SequenceFileReducer extends Reducer&lt;Text, BytesWritable, Text, BytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;BytesWritable&gt; values, Context context) throws IOException, InterruptedException &#123; for (BytesWritable value : values) &#123; context.write(key, value); &#125; &#125;&#125; 7. 编写Driver方法1234567891011121314151617181920public class SequenceFileDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/input_path", "/output_path"&#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SequenceFileDriver.class); job.setMapperClass(SequenceFileMapper.class); job.setReducerClass(SequenceFileReducer.class); // 设置输入的inputFormat job.setInputFormatClass(WholeFileInputFormat.class); // 设置输出的outputFormat job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 十、 实现自定义Partition分区1. 需求：在之前统计流量的基础上，将统计结果按照手机归属地不同省份输出到不同文件中 2. 在之前的案例中添加一个自定义分区类123456789101112131415161718public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text text, FlowBean flowBean, int i) &#123; int partition = 4; String prePhone = text.toString().substring(0, 3); if ("136".equals(prePhone)) &#123; partition = 0; &#125;else if ("137".equals(prePhone)) &#123; partition = 1; &#125;else if ("138".equals(prePhone)) &#123; partition = 2; &#125;else if ("139".equals(prePhone)) &#123; partition = 3; &#125; return partition; &#125;&#125; 3. 在原案例的Driver方法中添加分区配置12345678910111213141516171819202122public class FlowCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; args = new String[]&#123;"/Users/zhangjia/Desktop/phone.txt", "/Users/zhangjia/Desktop/result_phone"&#125;; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowCountDriver.class); job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); job.setMapOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 设置自定义分区类 job.setPartitionerClass(ProvincePartitioner.class); // 设定Reducer任务数量 job.setNumReduceTasks(5); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4. 执行发现输出结果已经按照手机号前三位进行分区 5. 注意对于Driver中设置的Reducer Task任务数量（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；（3）如果ReduceTask的数量=1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个ReduceTask，最终也就只会产生一个结果文件 part-r-00000；（4）分区号必须从零开始，逐一累加。举例：例如：假设自定义分区数为5，则 DriverTask数 结果 1 会正常运行，只不过会产生一个输出文件 2 会报错 6 大于5，程序会正常运行，会产生空文件 十一、 自定义WritableComparable排序1. 原理分析bean对象做为key传输，需要实现WritableComparable接口重写compareTo方法，就可以实现排序。1234@Overridepublic int compareTo(FlowBean o) &#123; // todo&#125; 2. WritableComparable排序案例实操（全排序）1. 输入数据为之前流量统计的输出数据12345678910111213141516171819202113470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 2. 期望输出数据12345678910111213141516171819202113509468723 7335 110349 11768413975057813 11058 48243 5930113568436656 3597 25635 2923213736230513 2481 24681 2716218390173782 9531 2412 1194313630577991 6960 690 765015043685818 3659 3538 719713992314666 3008 3720 672815910133277 3156 2936 609213560439638 918 4938 585684188413 4116 1432 554813682846555 1938 2910 484818271575951 1527 2106 363315959002129 1938 180 211813590439668 1116 954 207013956435636 132 1512 164413470253144 180 180 36013846544121 264 0 26413966251146 240 0 24013768778790 120 120 24013729199489 240 0 240 3. 编写自定义FlowBean类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; // 上行流量 private long upFlow; // 下行流量 private long downFlow; // 总流量 private long sumFlow; public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; @Override public int compareTo(FlowBean o) &#123; return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125; @Override public void write(DataOutput dataOutput) throws IOException &#123; dataOutput.writeLong(upFlow); dataOutput.writeLong(downFlow); dataOutput.writeLong(sumFlow); &#125; @Override public void readFields(DataInput dataInput) throws IOException &#123; upFlow = dataInput.readLong(); downFlow = dataInput.readLong(); sumFlow = dataInput.readLong(); &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125;&#125; 4. 编写Mapper类1234567891011121314151617181920public class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; FlowBean flowBean = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] strArr = line.split("\t"); String phoneNum = strArr[0]; long upFlow = Long.parseLong(strArr[1]); long downFlow = Long.parseLong(strArr[2]); long sumFlow = Long.parseLong(strArr[3]); flowBean.setUpFlow(upFlow); flowBean.setDownFlow(downFlow); flowBean.setSumFlow(sumFlow); v.set(phoneNum); context.write(flowBean, v); &#125;&#125; 5. 编写Reducer类12345678public class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text value : values) &#123; context.write(value, key); &#125; &#125;&#125; 6. 编写Driver方法123456789101112131415161718192021222324252627282930313233public class FlowCountSortDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; // 输入输出路径需要根据自己电脑上实际的输入输出路径设置 args = new String[]&#123;"/input_path","/output_path"&#125;; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCountSortDriver.class); // 3 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); // 4 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 5 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 6 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 7. 执行执行后发现结果与期望数据一致 注：如果希望实现区内排序，只需要在之前需求的基础上添加一个自定义Partition类即可。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume自定义Sink]]></title>
    <url>%2F2020%2F06%2F02%2FFlume%E8%87%AA%E5%AE%9A%E4%B9%89Sink%2F</url>
    <content type="text"><![CDATA[本文为尚硅谷Flume课程随堂笔记 一、 项目介绍使用 flume 接收数据，并在 Sink 端给每条数据添加前缀和后缀，输出到控制台。前后 缀可在flume任务配置文件中配置。 https://flume.apache.org/FlumeDeveloperGuide.html#sink 根据官方说明自定义 MySink 需要继承 AbstractSink 类并实现 Configurable 接口。 二、 Sink1.创建maven项目12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 2. 构建自定义类根据官方说明自定义 MySink 需要继承 AbstractSink 类并实现 Configurable 接口。具体代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.sink.AbstractSink;/** * @author suiwo * @title MySink * @date 2020/6/2 14:29 * @description //todo */public class MySink extends AbstractSink implements Configurable &#123; private String prefix; private String subfix; /** * 1. 获取Channel * 2. 从Channel获取事务以及数据 * 3. 发送数据 */ @Override public Status process() throws EventDeliveryException &#123; // 1. 定义返回值 Status status; // 2. 获取Channel Channel channel = getChannel(); // 3. 从Channel中获取事务 Transaction transaction = channel.getTransaction(); // 4. 启动事务 transaction.begin(); Event event; do &#123; // 5. 从Channel中获取数据 event = channel.take(); &#125; while (event == null); try &#123; // 6. 处理事件 String body = new String(event.getBody()); System.out.println(prefix + "--" + body + "--" + subfix); // 7. 提交事务 transaction.commit(); status = Status.READY; &#125; catch (ChannelException e) &#123; transaction.rollback(); // 10. 修改状态 status = Status.BACKOFF; &#125; finally &#123; // 11. 关闭事务 transaction.close(); &#125; return status; &#125; @Override public void configure(Context context) &#123; prefix = context.getString("prefix"); subfix = context.getString("subfix"); &#125;&#125; 3. maven打包将项目打包，并将项目放至flume的lib目录下 三、编写Flume相关配置1. 编写my-sink.conf1234567891011121314151617181920212223# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = xyz.suiwo.flume.sink.MySinka1.sinks.k1.prefix = suiwoa1.sinks.k1.subfix = haha# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 四、 启动测试执行下面指令启动服务1bin/flume-ng agent -c conf -f job/my-sink.conf -n a1 -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume自定义Source]]></title>
    <url>%2F2020%2F06%2F01%2FFlume%E8%87%AA%E5%AE%9A%E4%B9%89Source%2F</url>
    <content type="text"><![CDATA[本文为尚硅谷Flume课程随堂笔记 一、 项目介绍在本案例中使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。整个架构大致如下图： 官方也提供了自定义source的接口:https://flume.apache.org/FlumeDeveloperGuide.html#source 根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。 二、 构建自定义Source代码1.创建maven项目12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 2. 构建自定义类首先创建自定义类继承AbstractSource类并实现Configurable和PollableSource接口。具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.EventDeliveryException;import org.apache.flume.PollableSource;import org.apache.flume.conf.Configurable;import org.apache.flume.event.SimpleEvent;import org.apache.flume.source.AbstractSource;/** * @author suiwo * @title MySource * @date 2020/6/2 13:19 * @description //todo */public class MySource extends AbstractSource implements Configurable, PollableSource &#123; private String prefix; private String subfix; @Override public void configure(Context context) &#123; prefix = context.getString("prefix"); subfix = context.getString("sub","noSubfix"); &#125; /** * 1. 接收数据（for循环造数据） * 2. 封装为时间 * 3. 将时间传给Channel */ @Override public Status process() throws EventDeliveryException &#123; Status status = null; // 1. 接受数据 try &#123; for (int i = 0; i &lt; 5; i++) &#123; // 2. 构建事件对象 SimpleEvent event = new SimpleEvent(); // 3. 给事件设置值 event.setBody((prefix + "--" + i + "--" + subfix).getBytes()); // 4. 将时间传递给Channel getChannelProcessor().processEvent(event); status = Status.READY; &#125; &#125; catch (Exception e) &#123; status = Status.BACKOFF; e.printStackTrace(); &#125; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return status; &#125; @Override public long getBackOffSleepIncrement() &#123; return 0; &#125; @Override public long getMaxBackOffSleepInterval() &#123; return 0; &#125;&#125; 3. maven打包将项目打包，并将项目放至flume的lib目录下 三、编写Flume相关配置1. 编写my-source.conf123456789101112131415161718192021# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = xyz.suiwo.flume.source.MySourcea1.sources.r1.prefix = suiwoa1.sources.r1.subfix = zaixian# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 四、 启动测试执行下面指令启动服务1bin/flume-ng agent -c conf -f job/my-source.conf -n a1 -Dflume.root.logger=INFO,console 此时可以发现prefix参数获取成功，subfix因为配置文件中未配置，所以使用默认值]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume自定义Interceptor]]></title>
    <url>%2F2020%2F05%2F31%2FFlume%E8%87%AA%E5%AE%9A%E4%B9%89Interceptor%2F</url>
    <content type="text"><![CDATA[本文为尚硅谷Flume课程随堂笔记 一、 项目介绍在该案例中，我们以端口数据模拟日志，以是否存在“Hello”模拟不同类型的日志，我们需要自定义interceptor区分是否存在，将其分别发往不同的分析系统 (Channel)。整个架构大致如下图： 二、 构建拦截器代码1.创建maven项目12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt;&lt;/dependency&gt; 2.创建自定义拦截器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import org.apache.flume.Context;import org.apache.flume.Event;import org.apache.flume.interceptor.Interceptor;import java.util.ArrayList;import java.util.List;import java.util.Map;/** * @author suiwo * @title TypeInterceptor * @date 2020/5/31 14:28 * @description //todo */public class TypeInterceptor implements Interceptor &#123; private List&lt;Event&gt; result; public void initialize() &#123; result = new ArrayList&lt;Event&gt;(); &#125; // 单个事件拦截 public Event intercept(Event event) &#123; // 获取事件中的头信息 Map&lt;String, String&gt; headers = event.getHeaders(); // 获取事件中的body信息 String body = new String(event.getBody()); // 根据body中是否有Hello来决定添加怎样的头信息 if (body.contains("Hello")) &#123; // channel选择器通过type字段中的值向指定的avro发送信息 headers.put("type", "suiwo"); &#125; else &#123; headers.put("type", "inc"); &#125; return event; &#125; // 批量事件拦截 public List&lt;Event&gt; intercept(List&lt;Event&gt; list) &#123; result.clear(); for (Event event : list) &#123; result.add(intercept(event)); &#125; return result; &#125; public void close() &#123; &#125; public static class Builder implements Interceptor.Builder &#123; public Interceptor build() &#123; return new TypeInterceptor(); &#125; public void configure(Context context) &#123; &#125; &#125;&#125; 3. maven打包将项目打包，并将项目放至flume的lib目录下 三、编写Flume相关配置1. 编写flume1编写flume1的配置文件flume1.conf1234567891011121314151617181920212223242526272829303132333435363738394041# Namea1.sources = r1a1.channels = c1 c2a1.sinks = k1 k2# Sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Channela1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100# Sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = localhosta1.sinks.k1.port = 4141a1.sinks.k2.type= avroa1.sinks.k2.hostname = localhosta1.sinks.k2.port = 4142# Binda1.sources.r1.channels = c1 c2a1.sinks.k1.channel = c1a1.sinks.k2.channel = c2# Channel Selectora1.sources.r1.selector.type = multiplexinga1.sources.r1.selector.header = typea1.sources.r1.selector.mapping.suiwo = c1a1.sources.r1.selector.mapping.inc = c2# Interceptora1.sources.r1.interceptors = i1# 使用拦截器全类名，$符号后的为Builder内部类名a1.sources.r1.interceptors.i1.type = xyz.suiwo.flume.interceptor.TypeInterceptor$Builder 2. 编写flume2书写flume2的配置文件flume2.conf123456789101112a2.sources = r2a2.sinks = k2a2.channels = c2a2.sources.r2.type = avroa2.sources.r2.bind = localhosta2.sources.r2.port = 4141a2.sinks.k2.type = loggera2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100a2.sinks.k2.channel = c2a2.sources.r2.channels = c2 3. 编写flume3书写flume3的配置文件flume3.conf123456789101112a3.sources = r3a3.sinks = k3a3.channels = c3a3.sources.r3.type = avroa3.sources.r3.bind = localhosta3.sources.r3.port = 4142a3.sinks.k3.type = loggera3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100a3.sinks.k3.channel = c3a3.sources.r3.channels = c3 四、 启动测试 先启动flume2以及flume3最后启动flume1。 使用netcat工具向44444端口发送消息。 经过测试发现，拦截功能已实现。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume初识]]></title>
    <url>%2F2020%2F05%2F26%2FFlume%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文是尚硅谷Flume课程随手笔记，记录课程的一些实战的操作步骤 一、安装Flume并实现一个简易端口监控 实现监控端口案例，通过Flume来监听端口数据，并将数据打印到控制台 1. 首先进入官网下载Flume源码包 2. 修改flume-env.sh文件1export JAVA_HOME=/opt/module/jdk1.8.0_144 3. 添加配置文件创建Flume Agent配置文件flume-netcat-logger.conf123456789101112131415161718192021222324252627282930313233# Name the components on this agent a1:表示agent的名称# r1:表示a1的Source的名称a1.sources = r1# k1:表示a1的Sink的名称a1.sinks = k1# c1:表示a1的Channel的名称a1.channels = c1# Describe/configure the source# 表示a1的输入源类型为netcat端口类型a1.sources.r1.type = netcat# 表示a1的监听的主机a1.sources.r1.bind = localhost# 表示a1的监听的端口号a1.sources.r1.port = 44444# Describe the sink# 表示a1的输出目的地是控制台logger类型a1.sinks.k1.type = logger# Use a channel which buffers events in memory# 表示a1的channel类型是memory内存型a1.channels.c1.type = memory# 表示a1的channel总容量1000个eventa1.channels.c1.capacity = 1000# 表示a1的channel传输时收集到了100条event以后再去提交事务a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# 表示将r1和c1连接起来a1.sources.r1.channels = c1# 表示将k1和c1连接起来a1.sinks.k1.channel = c1 4. 启动Flume启动终端，在终端中属于下面的指令1bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console 使用下面的指令1nc localhost 44444 通过netcat发送数据，发现此时flume日志中显示接收到数据 二、 实时监控单个追加文件 实时监控 Hive 日志，并上传到 HDFS 中 1. 添加配置文件创建本次程序的配置文件，创建 flume-file-logger.conf 文件，根据官网添加一下配置12345678910111213141516171819202122232425262728293031# Name the components on this agent a1:表示agent的名称# r1:表示a1的Source的名称a1.sources = r1# k1:表示a1的Sink的名称a1.sinks = k1# c1:表示a1的Channel的名称a1.channels = c1# Describe/configure the source# 表示a1的输入源类型为exec source类型a1.sources.r1.type = exec# 监控文件a1.sources.r1.command = tail -F /Users/user/Library/Hive/logs/hive.log# Describe the sink# 表示a1的输出目的地是控制台logger类型a1.sinks.k1.type = logger# Use a channel which buffers events in memory# 表示a1的channel类型是memory内存型a1.channels.c1.type = memory# 表示a1的channel总容量1000个eventa1.channels.c1.capacity = 1000# 表示a1的channel传输时收集到了100条event以后再去提交事务a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# 表示将r1和c1连接起来a1.sources.r1.channels = c1# 表示将k1和c1连接起来a1.sinks.k1.channel = c1 2. 启动Flume在终端中输入下面的指令1bin/flume-ng agent --conf conf --conf-file job/file-flume-logger.conf --name a1 -Dflume.root.logger=INFO,console 之后启动该任务，并且可以监听hive的日志文件 三、 实时监控单个追加文件到HDFS Flume监控Hive实时更新日志然后上传到HDFS 1. 添加相关依赖包首先将需要使用的相关Jar包放入Flume的lib目录下123456commons-configuration-1.6.jar、hadoop-auth-2.7.2.jar、hadoop-common-2.7.2.jar、hadoop-hdfs-2.7.2.jar、commons-io-2.4.jar、htrace-core-3.1.0-incubating.jar 2.添加配置文件创建本次程序的配置文件，创建 flume-file-hdfs.conf 文件，根据官网添加一下配置，因为这里与上一节区别主要在sink部分，所以只需要在上一节配置的基础上对sink相关的部分进行修改即可1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# Name the components on this agent a1:表示agent的名称# r1:表示a1的Source的名称a1.sources = r1# k1:表示a1的Sink的名称a1.sinks = k1# c1:表示a1的Channel的名称a1.channels = c1# Describe/configure the source# 表示a1的输入源类型为exec source类型a1.sources.r1.type = exec# 监控文件a1.sources.r1.command = tail -F /Users/user/Library/Hive/logs/hive.log# Describe the sink# 表示a1的输出目的地是控制台logger类型a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://localhost:9000/flume/%Y%m%d/%H# 上传文件的前缀a1.sinks.k1.hdfs.filePrefix = logs- # 是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true # 多少时间单位创建一个新的文件夹a1.sinks.k1.hdfs.roundValue = 1# 重新定义时间单位a1.sinks.k1.hdfs.roundUnit = hour# 是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true# 积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 1000# 设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream# 多久生成一个新的文件a1.sinks.k1.hdfs.rollInterval = 30# 设置每个文件的滚动大小a1.sinks.k1.hdfs.rollSize = 134217700# 文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0# Use a channel which buffers events in memory# 表示a1的channel类型是memory内存型a1.channels.c1.type = memory# 表示a1的channel总容量1000个eventa1.channels.c1.capacity = 1000# 表示a1的channel传输时收集到了100条event以后再去提交事务a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# 表示将r1和c1连接起来a1.sources.r1.channels = c1# 表示将k1和c1连接起来a1.sinks.k1.channel = c1 3. 启动Flume在终端中输入下面的指令1bin/flume-ng agent --conf conf --conf-file job/file-file-hdfs.conf --name a1 -Dflume.root.logger=INFO,console 之后启动该任务，并且可以监听文件的追加 四、 实时监控目录下的多个新文件至HDFS 当监控的目录下有新文件时，进行上传 1. 添加配置文件写下如下配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# Name the components on this agent a1:表示agent的名称# r1:表示a1的Source的名称a1.sources = r1# k1:表示a1的Sink的名称a1.sinks = k1# c1:表示a1的Channel的名称a1.channels = c1# Describe/configure the source# 表示a1的输入源类型为spooldir类型a1.sources.r1.type = spooldir# 监控文件的路径a1.sources.r1.spoolDir = /Users/user/Library/Flume/upload# 上传完成文件后缀a1.sources.r1.fileSuffix = .COMPLETED# 忽略所有以.tmp 结尾的文件，不上传a1.sources.r1.ignorePattern = ([^ ]*\.tmp)# Describe the sink# 表示a1的输出目的地是控制台hdfs类型a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://localhost:9000/flume/%Y%m%d/%H# 上传文件的前缀a1.sinks.k1.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true # 多少时间单位创建一个新的文件夹a1.sinks.k1.hdfs.roundValue = 1# 重新定义时间单位a1.sinks.k1.hdfs.roundUnit = hour# 是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true# 积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 1000# 设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream# 多久生成一个新的文件a1.sinks.k1.hdfs.rollInterval = 30# 设置每个文件的滚动大小a1.sinks.k1.hdfs.rollSize = 134217700# 文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0# Use a channel which buffers events in memory# 表示a1的channel类型是memory内存型a1.channels.c1.type = memory# 表示a1的channel总容量1000个eventa1.channels.c1.capacity = 1000# 表示a1的channel传输时收集到了100条event以后再去提交事务a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# 表示将r1和c1连接起来a1.sources.r1.channels = c1# 表示将k1和c1连接起来a1.sinks.k1.channel = c1 2. 启动Flume在终端中输入下面的指令1bin/flume-ng agent --conf conf --conf-file job/file-dir-hdfs.conf --name a1 -Dflume.root.logger=INFO,console 之后我们向upload文件夹添加文件，发现成功上传至HDFS。 注： 但是它并不能监控动态变化的数据，在使用 Spooling Directory Source 时，不要在监控目录中创建并持续修改文件，上传完成的文件会以.COMPLETED 结尾，被监控文件夹每 500 毫秒扫描一次文件变动。 五、 实时监控目录下的多个追加文件 Exec source 适用于监控一个实时追加的文件，但不能保证数据不丢失;Spooldir Source 能够保证数据不丢失，且能够实现断点续传，但延迟较高，不能实时监控;而 Taildir Source 既能够实现断点续传，又可以保证数据不丢失，还能够进行实时监控。 1. 添加配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# Name the components on this agent a1:表示agent的名称# r1:表示a1的Source的名称a1.sources = r1# k1:表示a1的Sink的名称a1.sinks = k1# c1:表示a1的Channel的名称a1.channels = c1# Describe/configure the source#表示a1的输入源类型为TAILDIR类型a1.sources.r1.type = TAILDIR# 指定position_file位置a1.sources.r1.positionFile = /Users/user/Library/Flume/upload/tail_dir.json# 文件组a1.sources.r1.filegroups = f1 f2# f1文件组a1.sources.r1.filegroups.f1 = /Users/user/Library/Flume/upload/dict1/a.log# f2文件组a1.sources.r1.filegroups.f2 = /Users/user/Library/Flume/upload/dict2/.*.txt# Describe the sink# 表示a1的输出目的地是控制台logger类型a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = hdfs://localhost:9000/flume/%Y%m%d/%H# 上传文件的前缀a1.sinks.k1.hdfs.filePrefix = upload- # 是否按照时间滚动文件夹a1.sinks.k1.hdfs.round = true # 多少时间单位创建一个新的文件夹a1.sinks.k1.hdfs.roundValue = 1# 重新定义时间单位a1.sinks.k1.hdfs.roundUnit = hour# 是否使用本地时间戳a1.sinks.k1.hdfs.useLocalTimeStamp = true# 积攒多少个 Event 才 flush 到 HDFS 一次a1.sinks.k1.hdfs.batchSize = 1000# 设置文件类型，可支持压缩a1.sinks.k1.hdfs.fileType = DataStream# 多久生成一个新的文件a1.sinks.k1.hdfs.rollInterval = 30# 设置每个文件的滚动大小a1.sinks.k1.hdfs.rollSize = 134217700# 文件的滚动与 Event 数量无关a1.sinks.k1.hdfs.rollCount = 0# Use a channel which buffers events in memory# 表示a1的channel类型是memory内存型a1.channels.c1.type = memory# 表示a1的channel总容量1000个eventa1.channels.c1.capacity = 1000# 表示a1的channel传输时收集到了100条event以后再去提交事务a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channel# 表示将r1和c1连接起来a1.sources.r1.channels = c1# 表示将k1和c1连接起来a1.sinks.k1.channel = c1 2. 启动Flume在终端中输入下面的指令1bin/flume-ng agent --conf conf --conf-file job/file-taildir-hdfs.conf --name a1 -Dflume.root.logger=INFO,console 之后我们对监控的文件组中的文件进行追加，发现成功监听。 注： 在查看tail_dir.json中可以发现监听记录，同时每条记录都存在inode字段和pos字段，inode字段为文件唯一标识符，不随文件名称的变化而变化，pos则是上传文件的偏移量，正是通过这个来实现断点续传。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>Flume</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive初识]]></title>
    <url>%2F2020%2F05%2F24%2FHive%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一、 安装Hive并启动 1. 下载源码包解压源码包后，配置hive-env.sh(a)配置 HADOOP_HOME 路径1export HADOOP_HOME=$&#123;Hadoop的路径&#125; (b)配置 HIVE_CONF_DIR 路径1export HIVE_CONF_DIR=$&#123;Hive配置文件路径&#125; 2. 启动hadoop 3. 启动Hive客户端调用bin/hive进入Hive的客户端 启动Hive后会发现Hive源码包中多了derby.log和metastore_db这个就是Hive默认的元数据 4. 创建数据表在Hive客户端中创建一个数据表，并数据表中插入数据12hive&gt; create table student(id int, name string);hive&gt; insert into student values(1000,"ss"); 此时我们可以发现在hdfs中出现了user/hive/warehouse文件夹下存在一个与表名同名的文件夹，这个文件夹中存储的就是数据表的数据 二、 尝试在文件系统中加载数据 1. 创建测试数据创建一个student.txt文件（id与姓名之间用tab隔开）1231001 zhangsan1002 lisi1003 wangwu 2. 使用load导入数据在hive中执行以下指令1hive&gt; load data local inpath '文件路径' into table student; 此时我们可以发现user/hive/warehouse/student文件夹下多了一个student.txt文件，所以可以理解这其实就是一个上传操作，但是使用select语句查询时发现查询结果为123NULL NULLNULL NULLNULL NULL 这是因为我们在创建表时并未声明文件分隔符，所以我们重新创建数据表，并声明分隔符为’\t’1hive&gt; create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'; 之后重新导入后数据发现可以正常查询了。 3. 通过上传文件导入数据现在我们再创建一个student1.txt1231004 zhaoliu1005 chenba1006 sunjiu 然后直接上传到hdfs的user/hive/warehouse/student路径下之后在查询，可以发现我们上传的student1.txt文件中的数据也可以查询到了。 在实际开发中，可以选择使用load指令或者直接上传两种方式，如果我们想使用load指令加载hdfs下的数据，同样可以使用load指令，此时可以理解成将该文件移动到数据表所对应的文件之下hive&gt; load data inpath ‘文件路径’ into table student; 三、 配置Hive元数据至MySQL 我们会发现一个问题同一时间只能启动一个Hive客户端。这就是数据derby数据库的原因，所以我们接下来要将我们的元数据转移到MySQL中，MySQL安装在此不做赘述。 1. 添加MySQL连接驱动安装好MySQL后，首先我们需要下载mysql连接驱动也就是mysql-connector-java-5.1.47.jar（这个jar包可以在maven的本地仓库中找到，如果之前在maven中使用过这个依赖）文件放入到hive的lib目录中。 2. 配置hive-site.xml按照官方配置，将下列数据拷贝到文件中123456789101112131415161718192021222324&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop102:3306/metastore?createDatabaseI fNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;000000&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 配置完毕后，如果启动 hive 异常，可以重启hadoop，此时我们可以看到mysql中创建了相关的数据库。 3. 创建数据表然后我们创建一个数据表1hive&gt; create table student1(id int, name string); 这时我们在数据库中的TBLS数据表中会发现我们创建者条数据的记录，在DBS表中可以看到我们的数据库路径。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN集群搭建]]></title>
    <url>%2F2020%2F04%2F20%2FYARN%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[本文主要是记录集群搭建流程，具体细节较为简略 一、YARN简介Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。 二、集群规划 主机名 作用 hadoop11 NameNode &amp; DataNode &amp; NodeManager hadoop12 DataNode &amp; ResourceManager &amp; NodeManager hadoop13 NameNode &amp; DataNode &amp; NodeManager 三、搭建集群 修改主机名以及host映射并重启机器 配置ssh免密登录 配置JAVA环境变量 安装Hadoop(1). 修改hadoop-env.sh(2). 修改core-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop11:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/zhangjia/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (3). 修改hdfs-site.xml不需要修改，默认配置即可(4). 修改mapred-site.xml123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (5). 修改yarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop12&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (6). 配置slaves文件123hadoop11hadoop12hadoop13 (7). 格式化NameNode1hdfs namenode -format (8). 启动HDFS集群1start-dfs.sh (9). 启动YARN集群(只可以在ResourceManager节点启动)1start-yarn.sh (10). 可以在 http://hadoop12:8088查看当前YARN启动情况]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS高可用集群搭建]]></title>
    <url>%2F2020%2F04%2F20%2FHDFS%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[环境 CentOS 7 JAVA 1.8 Zookeeper 2.9.2 Hadoop 3.4.14 一、简单HDFS集群中存在的问题及解决办法 如何解决NameNode的单节点问题 多个NameNode备份原NameNode数据 如何解决多个NameNode是集群脑裂问题 使用QJM，QJM（Quorum Journal Manager）是Hadoop专门为Namenode共享存储开发的组件。其集群运行一组Journal Node，每个Journal 节点暴露一个简单的RPC接口，允许Namenode读取和写入数据，数据存放在Journal节点的本地磁盘。当Namenode写入edit log时，它向集群的所有Journal Node发送写入请求，当多数节点回复确认成功写入之后，edit log就认为是成功写入。例如有3个Journal Node，Namenode如果收到来自2个节点的确认消息，则认为写入成功。 而在故障自动转移的处理上，引入了监控Namenode状态的ZookeeperFailController（ZKFC）。ZKFC一般运行在Namenode的宿主机器上，与Zookeeper集群协作完成故障的自动转移。整个集群架构图如下： 在HA集群中如何ZK与NameNode active出现网络延迟问题这种情况，ZK会自动将NameNode standby切换为活跃节点，这个时候就出现了多个active节点，也就意味着现有集群面临脑裂问题 使用JournalNode 负责NameNode的edit log同步 JournalNode隔离机制，保证在一个时刻只有一个NameNode active，使用ssh登录到NameNode节点使用kill命令杀死NameNode。 二、集群规划1. 相关要求： 节点个数最好是奇数个 3个节点 每个zookeeper服务会启动至少三个端口 1.client处理 2.内部数据原子广播 3.内部选举投票端口 2. 服务器相关信息 hostname 所运行服务 IP地址 zk1 zkNode1 按照实际IP地址 zk2 zkNode2 按照实际IP地址 zk3 zkNode3 按照实际IP地址 hadoop1 NameNode(active) &amp; DataNode &amp; JournalNode &amp; ZKFC 按照实际IP地址 hadoop2 NameNode(standby) &amp; DataNode &amp; JournalNode &amp; ZKFC 按照实际IP地址 hadoop3 DataNode &amp; JournalNode &amp; ZKFC 按照实际IP地址 3. 共同配置(1) 修改所有机器hostname1vi /etc/hostname (2) 配置hosts文件，将ip地址与主机名进行映射1vi /etc/hosts (3) 重启机器 (4) 配置ssh免密登录，实现start-dfs.sh执行的机器可以免密登录其他的NameNode和DataNode节点12341. hadoop1: ssh-keygen -t -rsa2. hadoop1: ssh-copy-id hadoop13. hadoop1: ssh-copy-id hadoop24. hadoop1: ssh-copy-id hadoop3 (5) 配置JAVA环境 (6) 修改环境变量12345678910#进入配置文件vi /etc/profile# 添加如下配置export JAVA_HOME=/usr/java/jdk1.8.0_251-amd64export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/libexport PATH=$JAVA_HOME/bin:$PATH# 更新配置source /etc/profile 三、zk集群搭建1. 解压zookeeper文件1tar -zxvf zookeeper文件 2. 在每一个zk节点上创建zk的数据目录1mkdir /home/zkdata 3. 在每一个节点存放zk数据的目录中必须创建一个myid文件123zk1: echo "1" &gt;&gt; /home/zkdata/myidzk2: echo "2" &gt;&gt; /home/zkdata/myidzk3: echo "3" &gt;&gt; /home/zkdata/myid 4. 创建zookeeper的基础配置文件zoo.cfg1vi /home/zkdata/zoo.cfg 配置内容如下1234567891011# 3001为client端口# 3002为原子广播端口# 3003为选举投票端口tickTime=2000initLimit=10syncLimit=5dataDir=/home/zkdataclientPort=3001server.1=zk1:3002:3003server.2=zk2:3002:3003server.3=zk3:3002:3003 5. 启动zk节点，进入zk文件的bin目录下执行以下命令1./zkServer.sh start /home/zkdata/zoo.cfg 6. 执行jps命令发现已成功启动QuorumPeerMain进程7. 查询各个zk节点的集群状态，发现其中一个节点为leader其余节点为follower1./zkServer.sh status /home/zkdata/zoo.cfg 8. zk集群搭建完毕 四、HDFS集群搭建1. 在所有hadoop节点添加Cent OS依赖1yum install psmisc -y 2. 安装hadoop(配置Hadoop环境变量 非必须)123456# 添加如下配置export HADOOP_HOME=/home/hadoop-2.9.2export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin# 更新配置source /etc/profile (1) 配置hadoop-env.sh修改JAVA相关配置 (2) 配置core-site.xml123456789101112131415&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop-2.9.2/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置zk集群节点数 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;zk1:3001,zk2:3001,zk3:3001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (3) 配置hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;configuration&gt; &lt;!-- 指定hdfs的nameservices为ns，需要与core-site.xml中保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- ns下面有两个nameNode分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的rpc通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;hadoop1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的rpc通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;hadoop2:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;hadoop2:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定nameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定journalNode在本地磁盘中存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/zhangjia/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启nameNode故障时自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制。如果ssh默认是22端口，value直接写sshfence即可 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; (4) 配置slaves123hadoop1hadoop2hadoop3 (5) 启动集群 (6) 在任意NameNode节点格式化Zk1hdfs zkfc -formatZK (7) 出现下面这句话则说明格式化成功1ha.ActiveStandbyElector: Successfully created /hadoop-ha/ns in ZK. (8) 启动JournalNode节点 因为Journal在HA中需要同步edit log，所以他需要在edit log没有生成之前启动123hadoop1: hadoop-daemon.sh start journalnodehadoop2: hadoop-daemon.sh start journalnodehadoop3: hadoop-daemon.sh start journalnode (9) 使用jps发现JournalNode进程已启动并且在根文件夹出现journal文件夹 (10) 格式化NameNode,在选中的active的节点上执行1hdfs namenode -format ns (11) 启动hdfs集群1start-dfs.sh (12) 在standby 的 NameNode节点上执行如下命令进行同步active节点的edit log1hdfs namenode -bootstrapStandby (13) 启动standby节点的NameNode1hadoop-daemon.sh start namenode 此时进入两个NameNode节点的图形化界面 http://hadoop1:50070 可以发现一个为active一个为standby (14) 此时HA集群搭建完毕]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汉诺塔问题总结]]></title>
    <url>%2F2020%2F01%2F04%2F%E6%B1%89%E8%AF%BA%E5%A1%94%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、基本递归实现下面是普遍的汉诺塔问题的递归解法代码 1234567891011121314public class Hanoi&#123; public static void hanoi(int n, String x, String y, String z)&#123; if (n == 1)&#123; System.out.println(x + " -&gt; " + z); &#125;else &#123; hanoi(n - 1, x, z, y); System.out.println(x + " -&gt; " + z); hanoi(n - 1, y, x, z); &#125; &#125; public static void main(String[] args) &#123; hanoi(3, "x", "y", "z"); &#125;&#125; 输出如下： 1234567x -&gt; zx -&gt; yz -&gt; yx -&gt; zy -&gt; xy -&gt; zx -&gt; z 二、基本非递归实现非递归实现的方式本质就是尝试使用栈来模拟递归 1.创建一个保存状态的类12345678910111213public class State &#123; public int n; // 当前层数 public String x; // 起始柱 public String y; // 辅助柱 public String z; // 目标柱 public State(int n, String x, String y, String z) &#123; this.n = n; this.x = x; this.y = y; this.z = z; &#125;&#125; 2.实现主程序12345678910111213141516171819202122232425public class Hanoi &#123; public void hanoi(int n, String x, String y, String z) &#123; Stack&lt;State&gt; s = new Stack&lt;&gt;(); s.push(new State(n, x, y, z)); State state; while (!s.empty() &amp;&amp; (state = s.pop()) != null) &#123; if (state.n == 1) &#123; System.out.println(state.x + " -&gt; " + state.y); &#125; else &#123; // 栈结构先进后出，所以需要逆序进栈，这部分是理解重点，用栈模拟递归 s.push(new State(state.n - 1, state.y, state.x, state.z)); s.push(new State(1, state.x, state.y, state.z)); s.push(new State(state.n - 1, state.x, state.z, state.y)); &#125; &#125; &#125; public static void main(String[] args) &#123; Hanoi hanoi = new Hanoi(); System.out.println(); System.out.println("非递归方式："); Hanoi.hanoi(3, "x", "y", "z"); &#125;&#125; 3.输出结果12345678非递归方式：x -&gt; zx -&gt; yz -&gt; yx -&gt; zy -&gt; xy -&gt; zx -&gt; z 三、汉诺塔问题扩展我们尝试更改一下题目要求，不只是需要输出交换步骤，我们还需要记录交换过程中的三个柱子的圆盘存在情况 1.创建柱子类该类用于表示汉诺塔的每一个柱子，并且这个类将记录每个柱子上的圆盘情况1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class HanoiPillar &#123; public int n; // 记录传递hanoi的圆盘数量 public String name; // 柱子名称 public ArrayList&lt;Integer&gt; arr = new ArrayList&lt;&gt;(); //用于记录当前柱子上所存在的圆盘 // 初始化A柱 public HanoiPillar(int n, String name) &#123; this.n = n; this.name = name; for (int i = n; i &gt; 0; i--) &#123; this.arr.add(i); &#125; &#125; // 初始化B柱和C柱 public HanoiPillar(String name) &#123; this.name = name; &#125; // 判断该柱子上方是否为顶部盘子 public boolean top() &#123; boolean result = false; if (!arr.isEmpty() &amp;&amp; arr.size() != 0 &amp;&amp; arr.get(arr.size() - 1) == 1) &#123; result = true; &#125; return result; &#125; public void moveTo(HanoiPillar hanoiPillar) &#123; hanoiPillar.arr.add(this.getDiskSize()); this.arr.remove(this.arr.size() - 1); System.out.println(this.name + " -&gt; " + hanoiPillar.name); &#125; // 得到当前柱子的圆盘的列表，转化为String public String getStore() &#123; StringBuilder result = new StringBuilder(); if (this.arr.size() &gt; 0) &#123; for (int i = this.arr.size() - 1; i &gt;= 0; i--) &#123; result.append(this.arr.get(i)).append(","); &#125; &#125; return result.length() == 0 ? "null" : result.toString().trim(); &#125; // 得到该柱子中最小的圆盘的数值。以1、2、3、4、......、n来表示各个圆盘的大小。并且方便比较 public Integer getDiskSize() &#123; return this.arr.get(this.arr.size() - 1); &#125;&#125; 2.实现主程序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class Hanoi &#123; private int n; private int step; private HanoiPillar a; private HanoiPillar b; private HanoiPillar c; public void hanoi(int n, String a, String b, String c) &#123; this.step = (int) (Math.pow(2, n) - 1); this.a = new HanoiPillar(n, a); this.b = new HanoiPillar(b); this.c = new HanoiPillar(c); this.n = n; if (n % 2 != 0) &#123; HanoiPillar tmp = this.b; this.b = this.c; this.c = tmp; &#125; while (this.step &gt; 0) &#123; // 进行top的移动 if (this.a.top()) &#123; list(); this.a.moveTo(this.b); this.step--; &#125; else if (this.b.top()) &#123; list(); this.b.moveTo(this.c); this.step--; &#125; else if (this.c.top()) &#123; list(); this.c.moveTo(this.a); this.step--; &#125; // 因为step为奇数，而最后完成后step=0； // 同时可以理解为最后一次移动一定是top移动，所以需要进行一次循环判断 if (this.step == 0) &#123; break; &#125; // 执行第二步移动 if (this.a.top()) &#123; move2(this.b, this.c); &#125; else if (this.b.top()) &#123; move2(this.a, this.c); &#125; else if (this.c.top()) &#123; move2(this.a, this.b); &#125; &#125; list(); &#125; private void list() &#123; if (n % 2 == 0) &#123; System.out.print(this.a.name + "柱：" + this.a.getStore() + " " + this.b.name + "柱：" + this.b.getStore() + " " + this.c.name + "柱：" + this.c.getStore() + " "); &#125; else &#123; System.out.print(this.a.name + "柱：" + this.a.getStore() + " " + this.c.name + "柱：" + this.c.getStore() + " " + this.b.name + "柱：" + this.b.getStore() + " "); &#125; &#125; // 执行第二部移动 private void move2(HanoiPillar a, HanoiPillar b) &#123; if (a.arr.size() == 0) &#123; // a柱为空，则将b上层的盘子移到a list(); b.moveTo(a); this.step--; &#125; else if (b.arr.size() == 0) &#123; // b柱为空，则将a上层的盘子移到b list(); a.moveTo(b); this.step--; &#125; else if (a.getDiskSize() &gt; b.getDiskSize()) &#123; // 由于b盘子小于a盘子，所以将b的top盘子移动到a的top盘子 list(); b.moveTo(a); this.step--; &#125; else &#123; // 由于a盘子小于b盘子，所以将a的top盘子移动到b的top盘子 list(); a.moveTo(b); this.step--; &#125; &#125; public static void main(String[] args) &#123; Hanoi hanoi = new Hanoi(); Hanoi.hanoi(3, "A", "B", "C"); &#125;&#125; 3.输出结果12345678A柱：1,2,3, B柱：null C柱：null A -&gt; CA柱：2,3, B柱：null C柱：1, A -&gt; BA柱：3, B柱：2, C柱：1, C -&gt; BA柱：3, B柱：1,2, C柱：null A -&gt; CA柱：null B柱：1,2, C柱：3, B -&gt; AA柱：1, B柱：2, C柱：3, B -&gt; CA柱：1, B柱：null C柱：2,3, A -&gt; CA柱：null B柱：null C柱：1,2,3, 四、汉诺塔问题改编（递归实现）今天在做《程序员代码面试指南：IT名企算法与数据结构题目最优解（第二版）》时，遇到了一个稍微复杂一些的汉诺塔问题，但是理解之后发现本体只是在上面简易递归的基础上进行优化。 1.题目要求123456789101112131415161718192021【题目】 * 汉诺塔问题比较经典，这里修改一下游戏规则： * 现在限制不能从最左侧的塔直接移动到最右侧，也不能从最右侧直接移动到最左侧，而是必须经过中间。 * 求当塔有N层的时候，打印最优移动过程和最优移动总步数。 * 1.如果希望从“左”移到“中”，打印“Move 1 from left to mid”。 * 2.如果希望从“中”移到“左”，打印“Move 1 from mid to left”。 * 3.如果希望从“中”移到“右”，打印“Move 1 from mid to right”。 * 4.如果希望从“右”移到“中”，打印“Move 1 from right to mid”。 * 5.如果希望从“左”移到“右”，打印“Move 1 from left to mid”和“Move 1 from mid to right”。 * 6.如果希望从“右”移到“左”，打印“Move 1 from right to mid”和“Move 1 from mid to left”。 * * 例如，当塔数为两层时，最上层的塔记为1，最下层的塔记为2，则打印： * Move 1 from left to mid * Move 1 from mid to right * Move 2 from left to mid * Move 1 from right to mid * Move 1 from mid to left * Move 2 from mid to right * Move 1 from left to mid * Move 1 from mid to right * It wi11 move 8 steps. 2.情况分析首先我们尝试使用递归方式实现，然后进行常见情况分析 假设剩余N层塔都在左，希望全都移到中，则有下面三个步骤 将1~N-1层从左移到右，该过程为递归 将N层从左移到中 将1~N-1层从右移到中，该过程为递归 假设剩余N层塔都是从中移到右，或者从中移到左，或者从有右到中，其实原理与情况1相同，所以不做赘述 假设剩余N层塔都在左，希望都移到右，则有下面五个步骤 将1~N-1层从左移到右，该过程为递归 将N层从左移到中 将1~N-1层从右移到左，此过程为递归 将N层从中移到右 将1~N-1层从左移到右，此过程为递归 3.实现主程序123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Hanoi &#123; public int hanoiProblem (int num, String left, String mid, String right) &#123; if (num &lt; 1) return 0; return process(num, left, mid, right, left, right); &#125; public int process(int num, String left, String mid, String right, String from, String to) &#123; if (num == 1) &#123; if (from.equals(mid) || to.equals(mid)) &#123; System.out.println("Move 1 from " + from + " to " + to); return 1; &#125; else &#123; System.out.println("Move 1 from " + from + " to " + mid); System.out.println("Move 1 from " + mid + " to " + to); return 2; &#125; &#125; if (from.equals(mid) || to.equals(mid)) &#123; String another = (from.equals(left) || to.equals(left)) ? right : left; int part1 = process(num - 1, left, mid, right, from, another); int part2 = 1; System.out.println("Move " + num + " from " + from + " to " + to); int part3 = process(num - 1, left, mid, right, another, to); return part1 + part2 + part3; &#125; else &#123; int part1 = process(num - 1, left, mid, right, from, to); int part2 = 1; System.out.println("Move " + num + " from " + from + " to " + mid); int part3 = process(num - 1, left, mid, right, to, from); int part4 = 1; System.out.println("Move " + num + " from " + mid + " to " + to); int part5 = process(num - 1, left, mid, right, from, to); return part1 + part2 + part3 + part4 + part5; &#125; &#125; public static void main(String[] args) &#123; Hanoi hanoi = new Hanoi(); hanoi.hanoiProblem(3, "x", "y", "z"); &#125;&#125; 4.输出结果1234567891011121314151617181920212223242526Move 1 from x to yMove 1 from y to zMove 2 from x to yMove 1 from z to yMove 1 from y to xMove 2 from y to zMove 1 from x to yMove 1 from y to zMove 3 from x to yMove 1 from z to yMove 1 from y to xMove 2 from z to yMove 1 from x to yMove 1 from y to zMove 2 from y to xMove 1 from z to yMove 1 from y to xMove 3 from y to zMove 1 from x to yMove 1 from y to zMove 2 from x to yMove 1 from z to yMove 1 from y to xMove 2 from y to zMove 1 from x to yMove 1 from y to z 五、汉诺塔问题改编（非递归实现）1.题目分析 我们把左、中、右三个地点抽象成栈，依次记为LS、MS和RS。最初所有的塔都在LS上。那么如上4个动作就可以看作是：某一个栈（from）把栈顶元素弹出，然后压入到另一个栈里（to），作为这一个栈（to）的栈顶。 例如，如果是7层塔，在最初时所有的塔都在LS上，LS从栈顶到栈底就依次是1～7，如果现在发生了“左”到“中”的动作，这个动作对应的操作是LS栈将栈顶元素1弹出，然后1压入到MS栈中，成为MS的栈顶。其他操作同理。 一个动作能发生的先决条件是不违反小压大的原则。from栈弹出的元素num如果想压入到to栈中，那么num的值必须小于当前to栈的栈顶。还有一个原则不是很明显，但也是非常重要的，叫相邻不可逆原则，解释如下： 我们把4个动作依次定义为：L-＞M、M-＞L、M-＞R和R-＞M。 很明显，L-＞M和M-＞L过程互为逆过程，M-＞R和R-＞M互为逆过程。 在修改后的汉诺塔游戏中，如果想走出最少步数，那么任何两个相邻的动作都不是互为逆过程的。举个例子：如果上一步的动作是 L-＞M，那么这一步绝不可能是 M-＞L，直观地解释为：你在上一步把一个栈顶数从“左”移动到“中”，这一步为什么又要移回去呢？这必然不是取得最小步数的走法。同理，M-＞R动作和R-＞M动作也不可能相邻发生。 有了小压大和相邻不可逆原则后，可以推导出两个十分有用的结论–非递归的方法核心结论： 游戏的第一个动作一定是L-＞M，这是显而易见的。 在走出最少步数过程中的任何时刻，4个动作中只有一个动作不违反小压大和相邻不可逆原则，另外三个动作一定都会违反。 对于结论2，现在进行简单的证明。因为游戏的第一个动作已经确定是L-＞M，则以后的每一步都会有前一步的动作。 假设前一步的动作是L-＞M： 根据小压大原则，L-＞M的动作不会重复发生。 根据相邻不可逆原则，M-＞L的动作也不该发生。 根据小压大原则，M-＞R和R-＞M只会有一个达标。 假设前一步的动作是M-＞L： 根据小压大原则，M-＞L的动作不会重复发生。 根据相邻不可逆原则，L-＞M的动作也不该发生。 根据小压大原则，M-＞R和R-＞M只会有一个达标。 假设前一步的动作是M-＞R： 根据小压大原则，M-＞R的动作不会重复发生。 根据相邻不可逆原则，R-＞M的动作也不该发生。 根据小压大原则，L-＞M和M-＞L只会有一个达标。 假设前一步的动作是R-＞M： 根据小压大原则，R-＞M的动作不会重复发生。 根据相邻不可逆原则，M-＞R的动作也不该发生。 根据小压大原则，L-＞M和M-＞L只会有一个达标。 综上所述，每一步只会有一个动作达标。那么只要每走一步都根据这两个原则考查所有的动作就可以，哪个动作达标就走哪个动作，反正每次都只有一个动作满足要求，按顺序走下来即可 2.实现主程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class Hanoi &#123; public int hanoiProblem(int num, String left, String mid, String right) &#123; Stack&lt;Integer&gt; lS = new Stack&lt;&gt;(); Stack&lt;Integer&gt; mS = new Stack&lt;&gt;(); Stack&lt;Integer&gt; rS = new Stack&lt;&gt;(); // 初始化这四个栈，可以避免栈溢出问题，同时可以使while循环中的四个函数无论谁在前，都必定会限制性 l -&gt; m lS.push(Integer.MAX_VALUE);//最大值：2147483647(2的7次方-1) mS.push(Integer.MAX_VALUE); rS.push(Integer.MAX_VALUE); for (int i = num; i &gt; 0; i--) &#123;//将数字(最小数字在栈顶)压入左栈[1,2,3] lS.push(i); &#125; //调用枚举，记录上一步操作 创建一个数组而不是直接创建一个Action对象是为了使用引用传递而不是值传递 Action[] record = &#123;Action.No&#125;; int step = 0; //size();stack类从vector继承的方法；返回此向量中的组件数 while (rS.size() != num + 1) &#123;//当右栈未将数字全部存入时 //按顺序移动,下面这四个函数顺序并不影响因为每次必定只会有一个函数是满足条件的 step += fStackToStack(record, Action.MToL, Action.LToM, lS, mS, left, mid); step += fStackToStack(record, Action.LToM, Action.MToL, mS, lS, mid, left); step += fStackToStack(record, Action.RToM, Action.MToR, mS, rS, mid, right); step += fStackToStack(record, Action.MToR, Action.RToM, rS, mS, left, mid); &#125; return step; &#125; public static int fStackToStack(Action[] record, Action preNoAet, Action nowAct, Stack&lt;Integer&gt; fStack, Stack&lt;Integer&gt; tStack, String from, String to) &#123; // fStack.peek() &lt; tStack.peek() 必然可以保证 record[0] != nowAct 两条件互斥 if (record[0] != preNoAet &amp;&amp; fStack.peek() &lt; tStack.peek()) &#123;//发生移动且必须小的数字往大的数字上移动 tStack.push(fStack.pop());//fStack 移动到 tStack 且删掉from的栈顶元素 System.out.println("Move " + tStack.peek() + " from " + from + " to " + to); record[0] = nowAct; return 1; &#125; return 0; &#125; public static void main(String[] args) &#123; Hanoi Hanoi = new Hanoi(); int step = hanoi.hanoiProblem(3, "左", "中", "右"); System.out.println("总共需要" + step + "步"); &#125; enum Action &#123; No, // 无操作 LToM, // 从左移到中 MToL, // 从中移到左 MToR, // 从中移到右 RToM // 从右移到中 &#125;&#125; 3.输出结果123456789101112131415161718192021222324252627Move 1 from 左 to 中Move 1 from 中 to 右Move 2 from 左 to 中Move 1 from 左 to 中Move 1 from 中 to 左Move 2 from 中 to 右Move 1 from 左 to 中Move 1 from 中 to 右Move 3 from 左 to 中Move 1 from 左 to 中Move 1 from 中 to 左Move 2 from 左 to 中Move 1 from 左 to 中Move 1 from 中 to 右Move 2 from 中 to 左Move 1 from 左 to 中Move 1 from 中 to 左Move 3 from 中 to 右Move 1 from 左 to 中Move 1 from 中 to 右Move 2 from 左 to 中Move 1 from 左 to 中Move 1 from 中 to 左Move 2 from 中 to 右Move 1 from 左 to 中Move 1 from 中 to 右总共需要26步 参考视频https://www.bilibili.com/video/av31023017?from=search&amp;seid=15595573244367663980 参考文章https://blog.csdn.net/weixin_42636076/article/details/81031580https://www.jb51.net/article/128701.htm]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>汉诺塔问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase中使用过滤器筛选数据]]></title>
    <url>%2F2019%2F12%2F17%2FHBase%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%BF%87%E6%BB%A4%E5%99%A8%E7%AD%9B%E9%80%89%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[一、过滤器能干什么 HBase为筛选数据提供了一组过滤器，通过过滤器可以在HBase中的数据的多个维度(行，列，数据版本)上进行对数据的筛选操作。 通常来说，通过行键、列来筛选数据的应用场景较多。 二、常见的过滤器 基于行的过滤器 PrefixFilter: 行的前缀匹配 PageFilter: 基于行的分页 基于列的过滤器 ColumnPrefixFilter: 列前缀匹配 FirstKeyOnlyFilter: 只返回每一行的第一列 基于单元值的过滤器 KeyOnlyFilter: 返回的数据不包括单元值，只包含行键与列 TimestampsFilter: 根据数据的时间戳版本进行过滤 基于列和单元值的过滤器 SingleColumnValueFilter: 对该列的单元值进行比较过滤 SingleColumnValueExcludeFilter: 对该列的单元值进行比较过滤 比较过滤器 比较过滤器通常需要一个比较运算符以及一个比较器来实现过滤 RowFilter、 FamilyFilter、 QualifierFilter、 ValueFilter 常见过滤器总结 过滤器(Filter) 功能 RowFilter 筛选出匹配的所有的行 PrefixFilter 筛选出具有特定前缀的行键的数据 KeyOnlyFilter 只返回每行的行键，值全部为空 ColumnPrefixFilter 按照列名的前缀来筛选单元格 ValueFilter 按照具体的值来筛选单元格的过滤器 TimestampsFilter 根据数据的时间戳版本进行过滤 FilterList 用于综合使用多个过滤器 三、开发演示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/** * @title HBaseFilterTest * @date 2019/12/9 15:01 * @description 尝试使用过滤器 */public class HBaseFilterTest &#123; @Test public void createTable()&#123; HBaseUtil.createTable("FileTable", new String[]&#123;"fileInfo", "saveInfo"&#125;); &#125; @Test public void addFileDetails()&#123; HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "name", "file1.txt"); HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "type", "txt"); HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "size", "1024"); HBaseUtil.putRow("FileTable", "rowkey1", "saveInfo", "creator", "suiwo1"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "name", "file2.jpg"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "type", "jpg"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "size", "2048"); HBaseUtil.putRow("FileTable", "rowkey2", "saveInfo", "creator", "suiwo3"); HBaseUtil.putRow("FileTable", "rowkey3", "fileInfo", "name", "file3.jpg"); HBaseUtil.putRow("FileTable", "rowkey3", "fileInfo", "type", "jpg"); HBaseUtil.putRow("FileTable", "rowkey3", "fileInfo", "size", "2048"); HBaseUtil.putRow("FileTable", "rowkey3", "saveInfo", "creator", "suiwo3"); &#125; /** * rowkey = rowkey1 * fileName = file1.txt */ @Test public void rowFilterTest()&#123; Filter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes("rowkey1"))); // MUST_PASS_ALL指必须通过所有的Filter FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL, Arrays.asList(filter)); ResultScanner scanner = HBaseUtil.getScanner("FileTable","rowkey1","rowkey3", filterList); if(scanner != null)&#123; scanner.forEach(result -&gt; &#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); &#125;); scanner.close(); &#125; &#125; /** * rowkey = rowkey2 * fileName = file2.jpg */ @Test public void prefixFilterTest()&#123; Filter filter = new PrefixFilter(Bytes.toBytes("rowkey2")); FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL, Arrays.asList(filter)); ResultScanner scanner = HBaseUtil.getScanner("FileTable","rowkey1","rowkey3", filterList); if(scanner != null)&#123; scanner.forEach(result -&gt; &#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); &#125;); scanner.close(); &#125; &#125; /** * rowkey = rowkey1 * fileName =     * rowkey = rowkey2 * fileName =     */ @Test public void keyOnlyFilterTest()&#123; Filter filter = new KeyOnlyFilter(true); FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL, Arrays.asList(filter)); ResultScanner scanner = HBaseUtil.getScanner("FileTable","rowkey1","rowkey3", filterList); if(scanner != null)&#123; scanner.forEach(result -&gt; &#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); &#125;); scanner.close(); &#125; &#125; /** * rowkey = rowkey1 * fileName = file1.txt * fileType = null * rowkey = rowkey2 * fileName = file2.jpg * fileType = null */ @Test public void columnPrefixFilterTest()&#123; Filter filter = new ColumnPrefixFilter(Bytes.toBytes("nam"));// 前缀为nam FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL, Arrays.asList(filter)); ResultScanner scanner = HBaseUtil.getScanner("FileTable","rowkey1","rowkey3", filterList); if(scanner != null)&#123; scanner.forEach(result -&gt; &#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); System.out.println("fileType = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("type")))); &#125;); scanner.close(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构网课总结]]></title>
    <url>%2F2019%2F12%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%BD%91%E8%AF%BE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[网课推荐主要推荐下面的课有几个以及我个人觉得的优缺点 郝斌老师的数据结构 链接：https://www.bilibili.com/video/av6159200 优缺点：讲的非常的通俗易懂，但是课程只讲到了咱们课本的前面几章的知识，后面的图之类的知识没有涉及（排序讲了部分）。 小甲鱼老师的数据结构 链接：https://study.163.com/course/courseMain.htm?courseId=468002 优缺点：基本他的课涉及到了我们考试的所有知识点，但是我当时学习的时候可能是由于老师的口音原因，所以有的时候听的时候会有些分心，还有我记得好像当时我听有一两个章节当时听得有点迷糊。总体来说虽然没有郝斌老师将的通俗易懂，但是也可以说是生动形象了。 浙江大学的数据结构 链接：https://www.bilibili.com/video/av18586085?from=search&amp;seid=16468445350579187548 优缺点：这个课我认为优缺点是非常明显的，优点是每一节课比较短，可以让你对知识点有个简单地认识。缺点也是这个，就是讲的太短，缺少前期的知识由浅入深的代入。并且缺乏代码实战。不是很简易系统的看这个。我的建议是，如果你其他的网课对于某个知识点看不懂，可以尝试着看看这个课能否对你的理解有帮助，但是不要想着通过这个课学会。 严蔚敏老师的数据结构 链接：https://www.bilibili.com/video/av6239731 优缺点：严蔚敏老师的数据结构我并没有系统的去看，只是在一些比较难的知识点部分尝试去听了严蔚敏老师的课。总体来说给我的感觉就是讲的真的非常的好，但是可能对于新学习一个知识没有小甲鱼老师将的生动。适合当你听了郝斌老师或者小甲鱼老师的课之后对这个知识已经一知半解但是还没有彻底理解的时候看。当然了严蔚敏老师的课也有一个很明显的缺点就是他每节课都讲的很久，如果想从头到尾的看肯定是来不及的。 慕课上的数据结构从入门到精通 百度网盘链接：https://pan.baidu.com/s/1o36E3xrZ0pFx4-vjc89HsA 密码:0nbm 优缺点：这个课，其实我觉得对于考试来说并不是一个比较好的课，我知识觉得他在第12章AVL树部分讲的还行，其他部分就不建议看了先。这个等你以后找工作的时候在系统的看一下吧。 总结： 我之前学习的时候是以郝斌老师以及小甲鱼老师的课为主，严蔚敏老师以及浙大的课为辅。慕课的那个avl树部分我觉得考试也不是重点，看不看都行。 数据结构这部分有的课不是一遍就能听懂的，可能需要两遍甚至三遍才能看懂，所以必要的时候可以多看一两遍。 数据结构，可能你们学的时候觉得很难，但是考试是真的很简单，所以不要把过多的备考精力放在数据结构这一门课上。 其他建议 下面是我对部分个章节自己的一些想法（因为这个都是我凭借自己的记忆然后根据我记忆中老师当时给的重点写的建议，可能有些部分不是很准确，所以仅供参考。。。。。。） 线性表堆栈以及队列 这部分整体内容比较简单，所以基本上大家都讲的不错，不过我觉得这部分直接看书就足够了，如果觉得看书还是不太懂，可以去我给你的课中针对的学习一下。 数组和字符串 这部分我个人认为难的可能就是在KMP算法，不过期末考试也不知道会不会考KMP算法，下面是我总结的一些比较好的KMP算法的课 https://www.bilibili.com/video/av6239731/?p=11 这个是严蔚敏老师的课，因为这个每个视频没有写讲的什么，然后我看了一下我以前的总结，P11讲的是KMP算法，这样你就不用一个一个找对应了。 https://www.bilibili.com/video/av3246487?from=search&amp;seid=8682896714663607035 这个是一个印度程序员讲的kmp我觉得也是讲的很好地。 https://study.163.com/course/courseLearn.htm?courseId=468002#/learn/video?lessonId=1023415&amp;courseId=468002 这个就是小甲鱼讲的了。 树 这部分，我觉得郝斌的树讲的还是挺好的，不过他的课有一个缺点就是没有讲到哈弗曼树，哈弗曼树这部分，这部分可以补充着小甲鱼的课进行学习 郝斌：https://www.bilibili.com/video/av6159200?p=51 小甲鱼：https://study.163.com/course/courseLearn.htm?courseId=468002&amp;from=study#/learn/video?lessonId=1482172&amp;courseId=468002 集合搜索、搜索树、跳表以及散列表 这部分我觉得不是特别难，也没有太多其他的多余的建议了。这部分，我记得是没有太多的考试占比好像，不知道现在变了没，但是我觉得看了小甲鱼的课以及浙大的那个课基本上对这个有个概念的认识就差不多了。 图 这部分和上两章也是一样的，就是学的挺多的，但是考的不多，所以先把听网课把所有概念都搞懂，然后等老师给了考点，再把考点部分再认真的学就好了。 排序 这部分是很重要的一部分，但是我也觉得这个相较而言并不是特别难。这个没有太多技巧，尝试自己多写几遍代码理解透彻就好了。]]></content>
      <categories>
        <category>wqf</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Java操作HBase数据库]]></title>
    <url>%2F2019%2F12%2F09%2F%E4%BD%BF%E7%94%A8Java%E6%93%8D%E4%BD%9CHBase%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[一、添加依赖首先我们在maven项目中添加下面两个依赖12345678910111213&lt;!-- hbase依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.4.10&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 单元测试依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; &lt;version&gt;5.3.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 二、连接HBase数据库1. 编写连接数据库的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @title HBaseConnection * @date 2019/12/8 20:46 * @description 连接HBase数据库 */public class HBaseConnection &#123; private static final HBaseConnection INSTANCE = new HBaseConnection(); private static Configuration configuration; private static Connection connection; private HBaseConnection()&#123; try &#123; if(configuration == null)&#123; configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.quorum", "localhost:2181"); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; private Connection getConnection()&#123; if (connection == null || connection.isClosed())&#123; try&#123; connection = ConnectionFactory.createConnection(configuration); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; return connection; &#125; public static Connection getHBaseConnection()&#123; return INSTANCE.getConnection(); &#125; public static Table getTable(String tableName) throws IOException &#123; return INSTANCE.getConnection().getTable(TableName.valueOf(tableName)); &#125; public static void closeConnection()&#123; if(connection != null)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 2. 编写数据库连接测试类1234567891011121314151617181920212223242526/** * @title HBaseTest * @date 2019/12/8 21:35 * @description //todo */public class HBaseConnectionTest &#123; @Test public void getConnectionTest()&#123; Connection hBaseConnection = HBaseConnection.getHBaseConnection(); System.out.println(hBaseConnection.isClosed()); HBaseConnection.closeConnection(); System.out.println(hBaseConnection.isClosed()); &#125; @Test public void getTableTest()&#123; try&#123; Table table = HBaseConnection.getTable("US_POPULATION"); System.out.println(table.getName().getNameAsString()); table.close(); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 三、使用Java实现HBase常见操作1. 编写操作数据库的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223/** * @title HBaseUtil * @date 2019/12/9 11:14 * @description 操作HBase工具类 */public class HBaseUtil &#123; /** * 创建HBase表 * @param tableName 表名 * @param cfs 列族的数据 * @return 是否创建成功 */ public static boolean createTable(String tableName, String[] cfs)&#123; try (HBaseAdmin admin = (HBaseAdmin)HBaseConnection.getHBaseConnection().getAdmin())&#123; if(admin.tableExists(tableName))&#123; return false; &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName)); Arrays.stream(cfs).forEach(cf -&gt; &#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(cf); columnDescriptor.setMaxVersions(1); tableDescriptor.addFamily(columnDescriptor); &#125;); admin.createTable(tableDescriptor); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除HBase表 * @param tableName 表名 * @return */ public static boolean deleteTable(String tableName)&#123; try (HBaseAdmin admin = (HBaseAdmin)HBaseConnection.getHBaseConnection().getAdmin())&#123; admin.disableTable(tableName); admin.deleteTable(tableName); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * HBase表中插入一条数据 * @param tableName 表名 * @param roeKey 唯一标识 * @param cfName 列族名 * @param qualifier 列标识 * @param data 数据 * @return 是否插入成功 */ public static boolean putRow(String tableName, String roeKey, String cfName, String qualifier, String data)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Put put = new Put(Bytes.toBytes(roeKey)); put.addColumn(Bytes.toBytes(cfName), Bytes.toBytes(qualifier), Bytes.toBytes(data)); table.put(put); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * HBase表中批量插入数据 * @param tableName * @param puts * @return */ public static boolean putRows(String tableName, List&lt;Put&gt; puts)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; table.put(puts); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * 获取单条数据 * @param tableName 表名 * @param rowKey 唯一表标识 * @return 查询结果 */ public static Result getRow(String tableName, String rowKey)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * 根据过滤器来获取单条数据 * @param tableName 表名 * @param rowKey 唯一标识 * @param filterList 过滤器 * @return 查询结果 */ public static Result getRow(String tableName, String rowKey, FilterList filterList)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Get get = new Get(Bytes.toBytes(rowKey)); get.setFilter(filterList); return table.get(get); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * 通过Scan来检索数据 * @param tableName 表名 * @return 查询结果 */ public static ResultScanner getScanner(String tableName)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Scan scan = new Scan(); scan.setCaching(1000); return table.getScanner(scan); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * 批量检索数据 * @param tableName 表名 * @param startRowKey 起始rowKey * @param endRowKey 终止rowKey * @return 查询结果 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes(startRowKey)); scan.withStopRow(Bytes.toBytes(endRowKey)); scan.setCaching(1000); return table.getScanner(scan); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * 使用过滤器批量检索数据 * @param tableName 表名 * @param startRowKey 起始rowKey * @param endRowKey 终止rowKey * @param filterList 过滤器 * @return 查询结果 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes(startRowKey)); scan.withStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); scan.setCaching(1000); return table.getScanner(scan); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * HBase删除一行结果 * @param tableName 表名 * @param rowKey 唯一标识 * @return 是否删除 */ public static boolean deleteRow(String tableName, String rowKey)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除一个列族 * @param tableName 表名 * @param cfName 列族名 * @return 是否删除 */ public static boolean deleteColumnFamily(String tableName, String cfName)&#123; try (HBaseAdmin admin = (HBaseAdmin)HBaseConnection.getHBaseConnection().getAdmin())&#123; admin.deleteColumn(tableName, cfName); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除某一列的qualifier * @param tableName 表名 * @param rowKey 唯一标识 * @param cfName 列族名 * @param qualifier * @return */ public static boolean deleteQualifier(String tableName, String rowKey, String cfName, String qualifier)&#123; try (Table table = HBaseConnection.getTable(tableName))&#123; Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(cfName), Bytes.toBytes(qualifier)); table.delete(delete); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; return true; &#125;&#125; 2. 编写相关测试类进行测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * @title HBaseUtilTest * @date 2019/12/9 11:55 * @description //todo */public class HBaseUtilTest &#123; @Test public void createTable()&#123; HBaseUtil.createTable("FileTable", new String[]&#123;"fileInfo", "saveInfo"&#125;); &#125; @Test public void addFileDetails()&#123; HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "name", "file1.txt"); HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "type", "txt"); HBaseUtil.putRow("FileTable", "rowkey1", "fileInfo", "size", "1024"); HBaseUtil.putRow("FileTable", "rowkey1", "saveInfo", "creator", "suiwo1"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "name", "file2.jpg"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "type", "jpg"); HBaseUtil.putRow("FileTable", "rowkey2", "fileInfo", "size", "2048"); HBaseUtil.putRow("FileTable", "rowkey2", "saveInfo", "creator", "suiwo2"); &#125; @Test public void getFileDetails()&#123; Result result = HBaseUtil.getRow("FileTable", "rowkey1"); if(result != null)&#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); &#125; &#125; @Test public void scanFileDetail()&#123; ResultScanner scanner = HBaseUtil.getScanner("FileTable", "rowkey2", "rowkey2"); if(scanner != null)&#123; scanner.forEach(result -&gt; &#123; System.out.println("rowkey = " + Bytes.toString(result.getRow())); System.out.println("fileName = " + Bytes.toString(result.getValue(Bytes.toBytes("fileInfo"), Bytes.toBytes("name")))); &#125;); scanner.close(); &#125; &#125; @Test public void deleteRow()&#123; HBaseUtil.deleteRow("FileTable", "rowkey1"); &#125; @Test public void deleteTable()&#123; HBaseUtil.deleteTable("FileTable"); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>HDFS</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase伪分布式集群安装]]></title>
    <url>%2F2019%2F12%2F08%2FHBase%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、下载HBase安装包为了防止HBase和Hadoop版本间冲突，可以在CDH上下载和Hadoop兼容的HBase版本，本次搭建选择的是hbase-1.2.0下载路径：http://archive.cloudera.com/cdh5/ 二、配置伪分布式环境 HBase相关配置文件在/conf文件之下,我们需要修改的配置文件有hbase-env.sh和hbase-site.xml 我们先将hadoop中的hdfs-site.xml和core-site.xml文件复制HBase的/conf目录之下之后我们需要配置HBase相关 去配置hbase-env.sh，修改java环境变量，因为我们使用的是JDK8所以我们将文件中的下面这两行注释掉 12export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m"export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m" 之后在最后面有下面这个注释，他表示使用HBase自带的Zookeeper进行运行 1# export HBASE_MANAGES_ZK=true 配置hbase-site.xml 1234567891011121314151617&lt;configuration&gt;&lt;!-- 这个表示HBase在HDFS中的路径，因为我们的HDFS启动端口为9000，所以我们这里配置如下，这样启动后我们就可以在hdfs的根路径下看到/hbase文件夹 --&gt;&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;&lt;/property&gt;&lt;!-- 这个指HBase自带的zookeeper的datdaDir的路径 --&gt;&lt;property&gt; &lt;name&gt;nbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;file:/XXX/HBase/zookeeper&lt;/value&gt;&lt;/property&gt;&lt;!-- 这个是指是不是以集群的方式运行 --&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 三、 启动 我们进入/bin目录下执行./start-hbase.sh指令 我们执行jps可以查看到下面三个和HBase相关的进程 12391593 HQuorumPeer91642 HMaster91738 HRegionServer 我们进入/bin执行./hbase shell进入HBase命令行 在HBase命令行中执行status指令查看当前状态 11 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load 进入HDFS中查看有没有创建成功，进入到Hadoop的/bin目录下执行./hdfs -dfs -ls /,这个时候我们可以看到根目录下存在/hbase文件夹 至此HBase的伪分布式集群安装完毕 配置web管理界面1.0.0之后的版本的hbase的master web 默认是不运行的，所以需要自己配置默认端口。在hbase-site.xml中加入一下内容即可123456&lt;!-- 新增的配置 --&gt;&lt;property&gt;&lt;name&gt;hbase.master.info.port&lt;/name&gt;&lt;value&gt;60010&lt;/value&gt;&lt;/property&gt;&lt;!-- 新增的配置 --&gt; 之后访问localhost:60010即可访问]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop伪分布式集群安装]]></title>
    <url>%2F2019%2F12%2F07%2Fhadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[一、下载hadoop安装包hadoop安装包分为apache官方以及CDH，具体区别在此不做介绍，本次搭建选择的是hadoop-2.6.0-cdh5.7.0版本下载路径http://archive.cloudera.com/cdh5/ 二、配置hadoophadoop相关配置文件在/etc目录下我们需要修改的文件有core-site.xml，hadoop-env.sh，hdfs-site.xml 对hadoop-env.sh文件中的JAVA_HOME进行一下配置 在hdfs-site.xml文件中添加配置 1234567891011121314151617&lt;configuration&gt; &lt;property&gt; &lt;!-- 存储副本的数量，因为我们是伪分布式，所以副本数量为1 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode相关信息存储路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/XXX/Hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- datanode相关信息存储路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/XXX/Hadoop/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置好之后我们需要创建/XXX/Hadoop/dfs/name和/XXX/Hadoop/dfs/data这两个文件 配置core-site.xml 123456789101112&lt;configuration&gt; &lt;!-- 表明我们hdfs的默认ip --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 表明我们tmp文件的路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/XXX/Hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 之后我们需要进到/bin目录下面执行./hdfs namenode -format对namenode进行格式化 如果看到下面这段话则说明格式化成功 119/12/07 15:04:31 INFO util.ExitUtil: Exiting with status 0 三、启动 /sbin目录下面使用./start-dfs.sh启动hdfs集群 使用./hdfs dfs -ls /查看hdfs根目录为空 然后我们在执行./hdfs dfs -mkdir /test之后再查询会发现根目录已经存在/test文件]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用iTerm2和OhMyZsh实现一个强大终端]]></title>
    <url>%2F2019%2F11%2F16%2F%E4%BD%BF%E7%94%A8iTerm2%E5%92%8COhMyZsh%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%BC%BA%E5%A4%A7%E7%BB%88%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[参考文章：https://blog.csdn.net/qianghaohao/article/details/79440961https://www.jianshu.com/p/9c3439cc3bdbhttps://www.jianshu.com/p/d194d29e488c?open_source=weibo_searchhttps://www.jianshu.com/p/a78845c3f476 首先我们看一下最终效果 一、下载iTerm2官网下载地址：https://www.iterm2.com/ 二、安装zshzsh一般Mac已经自带了，无需额外安装。可以用cat /etc/shells查看zsh是否安装，如果列出了/bin/zsh则表明zsh已经安装了。接下来修改iTerm2终端的默认Shell，可以用echo $SHELL查看当前Shell是什么，如果不是/bin/zsh则用如下命令修改iTerm2的默认Shell为zsh1chsh -s /bin/bash 这个是默认的样子 三、使用Oh my zsh zsh的功能极其强大，只是配置过于复杂，起初只有极客才在用。后来，有个穷极无聊的程序员可能是实在看不下去广大猿友一直只能使用单调的bash, 于是他创建了一个名为oh-my-zsh的开源项目 1.安装Oh my zsh12# curl 安装方式sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; 12# wget 安装方式sh -c &quot;$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)&quot; 在安装过程中我们发现总是出现无法下载instal.sh文件的情况，所以下面是install.sh文件的源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115main() &#123; # Use colors, but only if connected to a terminal, and that terminal # supports them. if which tput &gt;/dev/null 2&gt;&amp;1; then ncolors=$(tput colors) fi if [ -t 1 ] &amp;&amp; [ -n &quot;$ncolors&quot; ] &amp;&amp; [ &quot;$ncolors&quot; -ge 8 ]; then RED=&quot;$(tput setaf 1)&quot; GREEN=&quot;$(tput setaf 2)&quot; YELLOW=&quot;$(tput setaf 3)&quot; BLUE=&quot;$(tput setaf 4)&quot; BOLD=&quot;$(tput bold)&quot; NORMAL=&quot;$(tput sgr0)&quot; else RED=&quot;&quot; GREEN=&quot;&quot; YELLOW=&quot;&quot; BLUE=&quot;&quot; BOLD=&quot;&quot; NORMAL=&quot;&quot; fi # Only enable exit-on-error after the non-critical colorization stuff, # which may fail on systems lacking tput or terminfo set -e CHECK_ZSH_INSTALLED=$(grep /zsh$ /etc/shells | wc -l) if [ ! $CHECK_ZSH_INSTALLED -ge 1 ]; then printf &quot;$&#123;YELLOW&#125;Zsh is not installed!$&#123;NORMAL&#125; Please install zsh first!\n&quot; exit fi unset CHECK_ZSH_INSTALLED if [ ! -n &quot;$ZSH&quot; ]; then ZSH=~/.oh-my-zsh fi if [ -d &quot;$ZSH&quot; ]; then printf &quot;$&#123;YELLOW&#125;You already have Oh My Zsh installed.$&#123;NORMAL&#125;\n&quot; printf &quot;You&apos;ll need to remove $ZSH if you want to re-install.\n&quot; exit fi # Prevent the cloned repository from having insecure permissions. Failing to do # so causes compinit() calls to fail with &quot;command not found: compdef&quot; errors # for users with insecure umasks (e.g., &quot;002&quot;, allowing group writability). Note # that this will be ignored under Cygwin by default, as Windows ACLs take # precedence over umasks except for filesystems mounted with option &quot;noacl&quot;. umask g-w,o-w printf &quot;$&#123;BLUE&#125;Cloning Oh My Zsh...$&#123;NORMAL&#125;\n&quot; hash git &gt;/dev/null 2&gt;&amp;1 || &#123; echo &quot;Error: git is not installed&quot; exit 1 &#125; # The Windows (MSYS) Git is not compatible with normal use on cygwin if [ &quot;$OSTYPE&quot; = cygwin ]; then if git --version | grep msysgit &gt; /dev/null; then echo &quot;Error: Windows/MSYS Git is not supported on Cygwin&quot; echo &quot;Error: Make sure the Cygwin git package is installed and is first on the path&quot; exit 1 fi fi env git clone --depth=1 https://github.com/robbyrussell/oh-my-zsh.git $ZSH || &#123; printf &quot;Error: git clone of oh-my-zsh repo failed\n&quot; exit 1 &#125; printf &quot;$&#123;BLUE&#125;Looking for an existing zsh config...$&#123;NORMAL&#125;\n&quot; if [ -f ~/.zshrc ] || [ -h ~/.zshrc ]; then printf &quot;$&#123;YELLOW&#125;Found ~/.zshrc.$&#123;NORMAL&#125; $&#123;GREEN&#125;Backing up to ~/.zshrc.pre-oh-my-zsh$&#123;NORMAL&#125;\n&quot;; mv ~/.zshrc ~/.zshrc.pre-oh-my-zsh; fi printf &quot;$&#123;BLUE&#125;Using the Oh My Zsh template file and adding it to ~/.zshrc$&#123;NORMAL&#125;\n&quot; cp $ZSH/templates/zshrc.zsh-template ~/.zshrc sed &quot;/^export ZSH=/ c\\ export ZSH=$ZSH &quot; ~/.zshrc &gt; ~/.zshrc-omztemp mv -f ~/.zshrc-omztemp ~/.zshrc # If this user&apos;s login shell is not already &quot;zsh&quot;, attempt to switch. TEST_CURRENT_SHELL=$(expr &quot;$SHELL&quot; : &apos;.*/\(.*\)&apos;) if [ &quot;$TEST_CURRENT_SHELL&quot; != &quot;zsh&quot; ]; then # If this platform provides a &quot;chsh&quot; command (not Cygwin), do it, man! if hash chsh &gt;/dev/null 2&gt;&amp;1; then printf &quot;$&#123;BLUE&#125;Time to change your default shell to zsh!$&#123;NORMAL&#125;\n&quot; chsh -s $(grep /zsh$ /etc/shells | tail -1) # Else, suggest the user do so manually. else printf &quot;I can&apos;t change your shell automatically because this system does not have chsh.\n&quot; printf &quot;$&#123;BLUE&#125;Please manually change your default shell to zsh!$&#123;NORMAL&#125;\n&quot; fi fi printf &quot;$&#123;GREEN&#125;&quot; echo &apos; __ __ &apos; echo &apos; ____ / /_ ____ ___ __ __ ____ _____/ /_ &apos; echo &apos; / __ \/ __ \ / __ `__ \/ / / / /_ / / ___/ __ \ &apos; echo &apos;/ /_/ / / / / / / / / / / /_/ / / /_(__ ) / / / &apos; echo &apos;\____/_/ /_/ /_/ /_/ /_/\__, / /___/____/_/ /_/ &apos; echo &apos; /____/ ....is now installed!&apos; echo &apos;&apos; echo &apos;&apos; echo &apos;Please look over the ~/.zshrc file to select plugins, themes, and options.&apos; echo &apos;&apos; echo &apos;p.s. Follow us at https://twitter.com/ohmyzsh.&apos; echo &apos;&apos; echo &apos;p.p.s. Get stickers and t-shirts at https://shop.planetargon.com.&apos; echo &apos;&apos; printf &quot;$&#123;NORMAL&#125;&quot; env zsh&#125;main 之后执行1sh -c &quot;$(curl -fsSL install.sh)&quot; 2.修改主题 下面我们进行主题修改，主题简介链接：https://github.com/robbyrussell/oh-my-zsh/wiki/themes （1）打开配置文件1vi ~/.zshrc （2）主题换成自己喜爱的主题1ZSH_THEME=&quot;agnoster&quot; （3）更新配置1source ~/.zshrc 四、安装PowerFonts字体有的同学会发现，执行完上一步后，使用可能会出现乱码，这是因为我们缺少PowerFonts字体安装字体库需要首先将项目clone至本地，然后执行源码中的install.sh。123git clone git@github.com:powerline/fonts.gitcd fonts./install.sh 安装好字体库之后，我们来设置iTerm2的字体，具体的操作是iTerm2 -&gt; Preferences -&gt; Profiles -&gt; Text，在Font区域选中Change Font，然后找到Roboto Mono for Powerline字体。 五、安装配色方案配色链接：https://github.com/mbadolato/iTerm2-Color-Schemes 配色方案在使用VIM或Colorful Log时会变得非常有用，同时界面也不会一片黑绿一样死板。 1.git clone的方式下载源码进行安装：1234cd ~/Desktop/OpenSourcegit clone https://github.com/altercation/solarizedcd solarized/iterm2-colors-solarized/open . 在打开的finder窗口中，双击Solarized Dark.itermcolors和Solarized Light.itermcolors安装明暗两种配色 进入iTerm2 -&gt; Preferences -&gt; Profiles -&gt; Colors -&gt; Color Presets 根据个人喜好选择solarized dark和solarized light两种配色中的一种即可。 2.手动安装但是下面这个是我最喜欢的配色 https://github.com/mbadolato/iTerm2-Color-Schemes/blob/master/schemes/Solarized%20Dark%20Higher%20Contrast.itermcolors 将该配色方案文件（Solarized Dark Higher Contrast.itermcolors）复制出来，保存到本地，文件命名为 SolarizedDarkHigherContrast.itermcolors，然后双击即可安装。 六、增加高亮这是oh my zsh的一个插件，安装方式与theme大同小异：123cd ~/.oh-my-zsh/custom/plugins/git clone https://github.com/zsh-users/zsh-syntax-highlighting.gitvi ~/.zshrc 1.添加plugins这时我们再次打开zshrc文件进行编辑。找到plugins，此时plugins中应该已经有了git，我们需要把高亮插件也加上：1plugins=(git) 12345plugins=(gitzsh-syntax-highlightingzsh-autosuggestions) 请务必保证插件顺序，zsh-syntax-highlighting必须在最后一个。 2.文件的最后一行添加1source ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh 3.修改生效：1source ~/.zshrc 七、安装命令补全跟代码高亮的安装方式一样，这也是一个zsh的插件，叫做zsh-autosuggestion，用于命令建议和补全。123cd ~/.oh-my-zsh/custom/plugins/git clone https://github.com/zsh-users/zsh-autosuggestionsvi ~/.zshrc 之后将插件加入zsh配置与上一个一致 八、使用技巧参考文章：https://www.jianshu.com/p/a78845c3f476]]></content>
      <categories>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>iTerm2</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka初试]]></title>
    <url>%2F2019%2F11%2F07%2FKafka%E5%88%9D%E8%AF%95%2F</url>
    <content type="text"><![CDATA[一、简介Apache Kafka起源于LinkedIn，后来于2011年成为开源Apache项目，然后于2012年成为First-class Apache项目。Kafka是用Scala和Java编写的。 Apache Kafka是基于发布订阅的容错消息系统。 它是快速，可扩展和设计分布。 二、安装Kafka 安装Java 安装Zookeeper 启动命令： zkServer.sh start 停止命令： zkServer.sh stop 安装Kafka 启动命令： kafka-server-start.sh config/server.properties 启动命令： kafka-server-stop.sh config/server.properties 三、代码实践添加Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 生产者客户端代码123456789101112131415161718public class ProducerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); properties.put("bootstrap.servers", brokerList); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, "hello, Kafka!"); try&#123; producer.send(record); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; producer.close(); &#125;&#125; 消费者客户端代码1234567891011121314151617181920public class ConsumerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static final String groupId = "group.demo"; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); properties.put("bootstrap.servers", brokerList); properties.put("group.id", groupId); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(properties); consumer.subscribe(Collections.singletonList(topic)); while (true)&#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(1000)); for(ConsumerRecord&lt;String, String&gt; record : records)&#123; System.out.println(record.value()); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建一个基于redis的id生成器]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%9F%BA%E4%BA%8Eredis%E7%9A%84id%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[参考文章： https://blog.csdn.net/hengyunabc/article/details/44244951 https://www.jianshu.com/p/955909e1bd71 https://tech.meituan.com/2017/04/21/mt-leaf.html 参考项目：https://github.com/hengyunabc/redis-id-generator值。evalsha教程：https://www.runoob.com/redis/scripting-evalsha.htmleval教程：https://www.runoob.com/redis/scripting-eval.html 一、分布式id生成器需要满足的要求1.全局唯一2.尽可能保证id的递增 因为在查询的时候经常会有例如分页以及排序之类的需求，这个时候如果主键的id本身能够体现出时许效率会更加好。而对于常见的排序还有分页，我们解决办法有两种： 在数据表中添加一个时间字段，对其创建一个普通索引。 id本生就是按照时间大致有序的。 因为常见的普通索引的访问效率是比聚集索引要慢的，所以我们尽可能使用第二种解决方案 3.其他的一些要求 id要尽可能的短，这样可以减少存储的空间以及增加查询的效率。 要有足够数量的id可以使用，不然当数据量非常大时，id耗尽就不行了 要考虑不同机器之间的时间不一致问题 QPS尽量要高，这样就可以，否则例如像类SNOWFLAKE算法会在64位ID中利用部分位数（如12）表示单位时间内生成的ID序号，这部分序号用完了，这个单位时间就不能再生成序号了 二、常见的id生成器方案：1.利用mysql数据库的自增主键特性优点： 简单，代码方便，性能还行 数字的id是递增的，方便进行分页和排序 缺点： 不同的数据库语法和实现不同，实现数据迁移或者多数据库版本的时候可能会出现一些问题 我们常见的是一主多从数据库，这会产生单点故障，以及性能瓶颈 数据量大时需要考虑分库分表 优化方案： 使用多个master，对每个master设置的初始id不同，步长不同，例如有四个master，我们可以让master1生成（1，5，9），master2生成（2，6，10），master3生成（3，7，11），master4生成（4，8，12），这样可以降低单个数据库的压力 2.UUID 常见的一种分布式id生成器，可以利用数据库也可以利用代码。 优点： 简单，方便 生成id的性能好，基本上不会有性能问题 全球唯一，对于数据库合并，迁移等问题不会存在冲突 缺点 不是有序的 UUID的字符串长度较长，查询效率不高，且消耗存储空间比较大，如果是海量数据库就需要考虑存储量的问题了 可读性差 3.redis生成id redis的大致原理和普通数据库的生成原理是大致相同的，只不过redis不是使用自增组件，而是使用原子操作 INCR和INCRBY来实现。 优点： 不依赖于数据库，灵活方便，且性能优于数据库。数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点： 如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。 需要编码和配置的工作量比较大。 4.snowflake算法 一个ID由64位生成 41bit作为时间戳，记录当前时间到标记的起始时间（如到2018.1.1）差，精确到毫秒，那么服务可用时长为(1&lt;&lt;41)/(1000 60 60 24 365) = 69.73年 10bit作为机器ID，也就是可以有1024台机器 12bit作为序列号，代表单位时间（这里是毫秒）内允许生成的ID总数，也就是1ms内允许生成4096个ID 优点： 不依赖于数据库，灵活方便，且性能优于数据库。 ID按照时间在单机上是递增的。 缺点： 在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。 5.类SNOWFLAKE算法 SNOWFLAKE给出的主要是一个思想，把ID划分为多个段，有不同的含义，可以结合自己的要求进行重新划分。按照个人理解，时间戳位数少了，机器位数多了，序列号位数多了。借鉴snowflake的思想，结合各公司的业务逻辑和并发量，可以实现自己的分布式ID生成算法。 举例，假设某公司ID生成器服务的需求如下： 单机高峰并发量小于1W，预计未来5年单机高峰并发量小于10W 有2个机房，预计未来5年机房数量小于4个 每个机房机器数小于100台 目前有5个业务线有ID生成需求，预计未来业务线数量小于10个 分析过程如下： 高位取从2017年1月1日到现在的毫秒数（假设系统ID生成器服务在这个时间之后上线），假设系统至少运行10年，那至少需要10年 365天 24小时 3600秒 1000毫秒 = 320 * 10 ^ 9，差不多预留39bit给毫秒数 每秒的单机高峰并发量小于10W，即平均每毫秒的单机高峰并发量小于100，差不多预留7bit给每毫秒内序列号 5年内机房数小于4个，预留2bit给机房标识 每个机房小于100台机器，预留7bit给每个机房内的服务器标识 业务线小于10个，预留4bit给业务线标识 这样设计的64bit标识，可以保证： 每个业务线、每个机房、每个机器生成的ID都是不同的 同一个机器，每个毫秒内生成的ID都是不同的 同一个机器，同一个毫秒内，以序列号区区分保证生成的ID是不同的 将毫秒数放在最高位，保证生成的ID是趋势递增的 缺点： 由于“没有一个全局时钟”，每台服务器分配的ID是绝对递增的，但从全局看，生成的ID只是趋势递增的（有些服务器的时间早，有些服务器的时间晚） 三、实现一个简易的redis的id生成器 利用redis的lua脚本执行功能，在每个节点上通过lua脚本生成唯一id，其中使用的是雪花算法。生成的ID是64位的： 使用41 bit来存放时间，精确到毫秒，可以使用41年。 使用12 bit来存放逻辑分片ID，最大分片ID是4095 使用10 bit来存放自增长ID，意味着每个节点，每毫秒最多可以生成1024个ID 比如GTM时间 Fri Mar 13 10:00:00 CST 2015 ，它的距1970年的毫秒数是 1426212000000，假定分片ID是53，自增长序列是4，则生成的ID是：12// 左移22位，指代最前面14bit的存储信息，再左移10位表示中间存储分片信息的12bit5981966696448054276 = 1426212000000 &lt;&lt; 22 + 53 &lt;&lt; 10 + 4 redis提供了TIME命令，可以取得redis服务器上的秒数和微秒数。因些lua脚本返回的是一个四元组。1second, microSecond, partition, seq 客户端要自己处理，生成最终ID。1((second * 1000 + microSecond / 1000) &lt;&lt; (12 + 10)) + (shardId &lt;&lt; 10) + seq; seq对应的是集群中的节点值如集群里有3个节点，则节点1返回的seq是：10, 3, 6, 9, 12 ... 节点2返回的seq是11, 4, 7, 10, 13 ... 节点3返回的seq是12, 5, 8, 11, 14 ... 我们可以将lua脚本转换成sha1值，然后通过EVALSHA指令传递这个 下面我们直接看代码项目主程序123456789101112131415161718192021public class Main &#123; public static void main(String[] args) &#123; String tab = "order"; long userId = 123456789; IdGenerator idGenerator = IdGenerator.builder() .addHost("127.0.0.1", 6379, "c5809078fa6d652e0b0232d552a9d06d37fe819c")// .addHost("127.0.0.1", 7379, "accb7a987d4fb0fd85c57dc5a609529f80ec3722")// .addHost("127.0.0.1", 8379, "f55f781ca4a00a133728488e15a554c070b17255") .build(); long id = idGenerator.next(tab, userId); System.out.println("id:" + id); List&lt;Long&gt; result = IdGenerator.parseId(id); System.out.println("miliSeconds:" + result.get(0) + ", partition:" + result.get(1) + ", seq:" + result.get(2)); &#125;&#125; id生成器相关代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public class IdGenerator &#123; static final Logger logger = LoggerFactory.getLogger(IdGenerator.class); /** * JedisPool, luaSha */ List&lt;Pair&lt;JedisPool, String&gt;&gt; jedisPoolList; int retryTimes; int index = 0; private IdGenerator() &#123; &#125; private IdGenerator(List&lt;Pair&lt;JedisPool, String&gt;&gt; jedisPoolList, int retryTimes) &#123; this.jedisPoolList = jedisPoolList; this.retryTimes = retryTimes; &#125; static public IdGeneratorBuilder builder() &#123; return new IdGeneratorBuilder(); &#125; static class IdGeneratorBuilder &#123; List&lt;Pair&lt;JedisPool, String&gt;&gt; jedisPoolList = new ArrayList(); int retryTimes = 5; public IdGeneratorBuilder addHost(String host, int port, String luaSha) &#123; jedisPoolList.add(Pair.of(new JedisPool(host, port), luaSha)); return this; &#125; public IdGeneratorBuilder retryTimes(int retryTimes) &#123; this.retryTimes = retryTimes; return this; &#125; public IdGenerator build() &#123; return new IdGenerator(jedisPoolList, retryTimes); &#125; &#125; public long next(String tab) &#123; return next(tab, 0); &#125; public long next(String tab, long shardId) &#123; for (int i = 0; i &lt; retryTimes; ++i) &#123; Long id = innerNext(tab, shardId); if (id != null) &#123; return id; &#125; &#125; throw new RuntimeException("Can not generate id!"); &#125; Long innerNext(String tab, long shardId) &#123; index++; Pair&lt;JedisPool, String&gt; pair = jedisPoolList.get(index % jedisPoolList.size()); JedisPool jedisPool = pair.getLeft(); String luaSha = pair.getRight(); Jedis jedis = null; try &#123; jedis = jedisPool.getResource(); List&lt;Long&gt; result = (List&lt;Long&gt;) jedis.evalsha(luaSha, 2, tab, "" + shardId); long id = buildId(result.get(0), result.get(1), result.get(2), result.get(3)); return id; &#125; catch (JedisConnectionException e) &#123; if (jedis != null) &#123; jedisPool.returnBrokenResource(jedis); &#125; logger.error("generate id error!", e); &#125; finally &#123; if (jedis != null) &#123; jedisPool.returnResource(jedis); &#125; &#125; return null; &#125; public static long buildId(long second, long microSecond, long shardId, long seq) &#123; long miliSecond = (second * 1000 + microSecond / 1000); return (miliSecond &lt;&lt; (12 + 10)) + (shardId &lt;&lt; 10) + seq; &#125; public static List&lt;Long&gt; parseId(long id) &#123; long miliSecond = id &gt;&gt;&gt; 22; // 2 ^ 12 = 0xFFF long shardId = (id &amp; (0xFFF &lt;&lt; 10)) &gt;&gt; 10; long seq = id &amp; 0x3FF; List&lt;Long&gt; re = new ArrayList&lt;Long&gt;(4); re.add(miliSecond); re.add(shardId); re.add(seq); return re; &#125;&#125; 至此我们的基于redis的id生成器就完成了]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>分布式</tag>
        <tag>SNOWFLAKE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写SpringMVC，剑指优秀开源框架灵魂]]></title>
    <url>%2F2019%2F08%2F20%2F%E6%89%8B%E5%86%99SpringMVC%EF%BC%8C%E5%89%91%E6%8C%87%E4%BC%98%E7%A7%80%E5%BC%80%E6%BA%90%E6%A1%86%E6%9E%B6%E7%81%B5%E9%AD%82%2F</url>
    <content type="text"><![CDATA[由于Spring官方就是选择gradle作为自动化构建工具，所以我们在本次尝试中就按照spring的选择也是用gradle在整个项目中，我们一共包含两个模块framework模块用于首先实现我们springmvc的常见功能，test模块则是用来测试我们手写的模块是否正确项目链接：https://github.com/ZhangJia97/Mini-Spring 下面是项目结构，只保留了我们用到的文件结构123456789101112131415161718192021222324252627282930313233343536373839404142├── build.gradle├── framework│ ├── build.gradle│ └── src│ ├── main│ ├── java│ └── xyz│ └── suiwo│ └── imooc│ ├── beans│ │ ├── Autowired.java│ │ ├── Bean.java│ │ └── BeanFactory.java│ ├── core│ │ └── ClassScanner.java│ ├── starter│ │ └── MiniApplication.java│ └── web│ ├── handler│ │ ├── HandlerManager.java│ │ └── MappingHandler.java│ ├── mvc│ │ ├── Controller.java│ │ ├── RequestMapping.java│ │ └── RequestParam.java│ ├── server│ │ └── TomcatServer.java│ └── servlet│ └── DispatcherServlet.java└── test ├── build.gradle └── src ├── main ├── java └── xyz └── suiwo └── imooc ├── Application.java ├── controller │ └── SalaryController.java └── service └── SalaryService.java 首先我们需要在framework的依赖中添加tomcat的依赖，因为springboot就是通过加入tomcat依赖来实现的12345dependencies &#123; testCompile group: 'junit', name: 'junit', version: '4.12' // https://mvnrepository.com/artifact/org.apache.tomcat.embed/tomcat-embed-core compile group: 'org.apache.tomcat.embed', name: 'tomcat-embed-core', version: '8.5.23'&#125; 接下来让我们看看如何去创建一个tomcat服务123456789101112131415161718192021222324252627282930313233343536373839public class TomcatServer &#123; private Tomcat tomcat; private String[] args; public TomcatServer(String[] args) &#123; this.args = args; &#125; public void startServer() throws LifecycleException &#123; tomcat = new Tomcat(); tomcat.setPort(8080); Context context = new StandardContext(); context.setPath(""); context.addLifecycleListener(new Tomcat.FixContextListener()); DispatcherServlet dispatcherServlet = new DispatcherServlet(); // servlet注册到tomcat容器内并开启异步支持 Tomcat.addServlet(context, "dispatcherServlet", dispatcherServlet).setAsyncSupported(true); context.addServletMappingDecoded("/", "dispatcherServlet"); // 注册到默认host容器 tomcat.getHost().addChild(context); tomcat.start(); Thread awaitThread = new Thread("tomcat_await_thread")&#123; @Override public void run() &#123; TomcatServer.this.tomcat.getServer().await(); &#125; &#125;; //设置成非守护线程 awaitThread.setDaemon(false); awaitThread.start(); &#125;&#125; 然后我们可以看到上述代码向tomcat中set了一个dispatchServlet用于处理请求，我们看看DispatchServlet如何去处理请求 1234567891011121314151617181920212223242526272829303132333435public class DispatcherServlet implements Servlet &#123; @Override public void init(ServletConfig config) throws ServletException &#123; &#125; @Override public ServletConfig getServletConfig() &#123; return null; &#125; @Override public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; for(MappingHandler mappingHandler : HandlerManager.mappingHandlerList)&#123; try &#123; if(mappingHandler.handle(req, res))&#123; return; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public String getServletInfo() &#123; return null; &#125; @Override public void destroy() &#123; &#125;&#125; 我们现在已经成功创建了一个Tomcat的服务类，下面我们就可以在主类中启动tomcat服务了 然后我们看一些framework的主类12345678910111213141516171819202122public class MiniApplication &#123; public static void run(Class&lt;?&gt; cls, String[] args)&#123; System.out.println("Hello Mini-Spring!"); // 创建一个Tomcat服务 TomcatServer tomcatServer = new TomcatServer(args); try &#123; // 启动tomcat tomcatServer.startServer(); // 扫描项目中当前cls目录下的所有包 List&lt;Class&lt;?&gt;&gt; classList = ClassScanner.scannerClass(cls.getPackage().getName()); // 初始化所有bean BeanFactory.init(classList); // 初始化所有的MappingHandler HandlerManager.resolveMappingHandler(classList); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 我们再创建三个mvc相关的注解12345@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface Controller &#123;&#125; 123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)public @interface RequestMapping &#123; String value() default "";&#125; 123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.PARAMETER)public @interface RequestParam &#123; String value() default "";&#125; 然后我们看一下ClassScanner类，这个类主要用于扫描包1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ClassScanner &#123; public static List&lt;Class&lt;?&gt;&gt; scannerClass(String packageName) throws IOException, ClassNotFoundException &#123; List&lt;Class&lt;?&gt;&gt; classList= new ArrayList&lt;&gt;(); String path = packageName.replaceAll("\\.", "/"); // 获取默认类加载器 ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // 获取资源文件的路径 Enumeration&lt;URL&gt; resources = classLoader.getResources(path); while(resources.hasMoreElements())&#123; URL resource = resources.nextElement(); // 判断资源类型 if(resource.getProtocol().contains("jar"))&#123; // 如果资源类型是jar包，则我们先获取jar包的绝对路径 JarURLConnection jarURLConnection = (JarURLConnection) resource.openConnection(); String jarFilePath = jarURLConnection.getJarFile().getName(); // 获取这个jar包下所有的类 classList.addAll(getClassesFromJar(jarFilePath, path)); &#125;else &#123; // todo 处理非jar包的情况 &#125; &#125; return classList; &#125; private static List&lt;Class&lt;?&gt;&gt; getClassesFromJar(String jarFilePath, String path) throws IOException, ClassNotFoundException &#123; //初始化一个容器用于存储类 List&lt;Class&lt;?&gt;&gt; classes = new ArrayList&lt;&gt;(); // 通过路径获取JarFile实例 JarFile jarFile = new JarFile(jarFilePath); // 遍历jar包，每个jarEntry都是jar包里的一个文件 Enumeration&lt;JarEntry&gt; jarEntryEnumeration = jarFile.entries(); while(jarEntryEnumeration.hasMoreElements())&#123; JarEntry jarEntry = jarEntryEnumeration.nextElement(); String entryName = jarEntry.getName(); // xyz/suiwo/imooc/test/Test.class if(entryName.startsWith(path) &amp;&amp; entryName.endsWith(".class"))&#123; // 把分隔符换成点，并去除.class后缀 String classFullName = entryName.replace("/", ".").substring(0, entryName.length() - 6); classes.add(Class.forName(classFullName)); &#125; &#125; return classes; &#125;&#125; 作为spring的经典ioc思想，初始化创建bean是重中之重，下面让我们看看如何实现吧对于常见与Bean相关的注解就是@Bean还有@Autowired所以我们首先创建两个注解123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)public @interface Autowired &#123; String value() default "";&#125; 123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface Bean &#123; String value() default "";&#125; 下面我们看看如何去初始化bean吧12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class BeanFactory &#123; private static Map&lt;Class&lt;?&gt;, Object&gt; classToBean = new ConcurrentHashMap&lt;&gt;(); public static Object getBean(Class&lt;?&gt; cls)&#123; return classToBean.get(cls); &#125; public static void init(List&lt;Class&lt;?&gt;&gt; classList) throws Exception &#123; List&lt;Class&lt;?&gt;&gt; toCreate = new ArrayList&lt;&gt;(classList); while (toCreate.size() &gt; 0)&#123; int remainSize = toCreate.size(); for(int i = 0; i &lt; toCreate.size(); i++)&#123; // 返回true则说明创建成功或者说当前类不是一个bean // 返回false则此时可能存存在当前需要创建的bean的依赖还没有创建所以暂时先跳过 if(finishCreate(toCreate.get(i)))&#123; toCreate.remove(i); &#125; &#125; // 如果数量没有改变则说明出现了死循环，抛出异常 if(toCreate.size() == remainSize)&#123; throw new Exception("死循环"); &#125; &#125; &#125; private static boolean finishCreate(Class&lt;?&gt; cls) throws IllegalAccessException, InstantiationException &#123; // 如果没有满足的注解，则直接返回true if(!cls.isAnnotationPresent(Bean.class) &amp;&amp; !cls.isAnnotationPresent(Controller.class))&#123; return true; &#125; Object bean = cls.newInstance(); for(Field field : cls.getDeclaredFields())&#123; if(field.isAnnotationPresent(Autowired.class))&#123; Class&lt;?&gt; fieldType = field.getType(); Object reliantBean = BeanFactory.getBean(fieldType); // 如果为空，则说明当前类中的字段所依赖的类还没有注入，所以返回false，先跳过，等到所需要依赖注入之后再创建 if(reliantBean == null)&#123; return false; &#125; field.setAccessible(true); field.set(bean, reliantBean); &#125; &#125; // 将创建好的bean放入容器中 classToBean.put(cls, bean); return true; &#125;&#125; 然后我们来看一下控制器，每一个MappingHandler都是一个请求映射器12345678910111213141516171819202122232425262728293031323334353637public class MappingHandler &#123; // 需要处理的uri private String uri; // 所对应的方法 private Method method; // 所对应的方法 private Class&lt;?&gt; controller; // 所需要的参数 private String[] args; public MappingHandler(String uri, Method method, Class&lt;?&gt; controller, String[] args) &#123; this.uri = uri; this.method = method; this.controller = controller; this.args = args; &#125; // 若与MappingHandler匹配成功，执行方法 public boolean handle(ServletRequest req, ServletResponse res) throws IllegalAccessException, InstantiationException, InvocationTargetException, IOException &#123; String requestUri = ((HttpServletRequest)req).getRequestURI(); if(!uri.equals(requestUri))&#123; return false; &#125; Object[] parameters = new Object[args.length]; for(int i = 0; i &lt; args.length; i++)&#123; parameters[i] = req.getParameter(args[i]); &#125; Object ctl = BeanFactory.getBean(controller); Object response = method.invoke(ctl, parameters); res.getWriter().println(response.toString()); return true; &#125;&#125; 我们在创建一个管理器去管理这些MappingHandler1234567891011121314151617181920212223242526272829303132public class HandlerManager &#123; public static List&lt;MappingHandler&gt; mappingHandlerList = new ArrayList&lt;&gt;(); // 把Controller类挑选出来，并将类中的带有@RequestMapping方法初始化成MappingHandler public static void resolveMappingHandler(List&lt;Class&lt;?&gt;&gt; classList)&#123; for(Class&lt;?&gt; cls : classList)&#123; if(cls.isAnnotationPresent(Controller.class))&#123; parseHandlerFromController(cls); &#125; &#125; &#125; // 解析controller类 private static void parseHandlerFromController(Class&lt;?&gt; cls) &#123; Method[] methods = cls.getDeclaredMethods(); for(Method method : methods)&#123; if(!method.isAnnotationPresent(RequestMapping.class))&#123; continue; &#125; String uri = method.getDeclaredAnnotation(RequestMapping.class).value(); List&lt;String&gt; paramNameList = new ArrayList&lt;&gt;(); for(Parameter parameter : method.getParameters())&#123; if(parameter.isAnnotationPresent(RequestParam.class))&#123; paramNameList.add(parameter.getDeclaredAnnotation(RequestParam.class).value()); &#125; &#125; String[] params = paramNameList.toArray(new String[paramNameList.size()]); MappingHandler mappingHandler = new MappingHandler(uri, method, cls, params); HandlerManager.mappingHandlerList.add(mappingHandler); &#125; &#125;&#125; 至此，我们就已经成功的将整个框架大致完成了，对于test模块中的代码，我就不在这里在书写了，因为和我们日常写springboot业务相同只是为了测试我们手写框架的几个功能。]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[aop链式调用]]></title>
    <url>%2F2019%2F08%2F19%2Faop%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[多个aop如果叠加采用了责任链的模式使用ConcerteHandler实现类实现HandleProcess接口同时这个实现类又组合了这个接口，通过successor判断来进行链式调用 首先我们新建一个Handler123456789101112131415161718192021public abstract class Handler &#123; private Handler successor; public Handler getSuccessor() &#123; return successor; &#125; public void setSuccessor(Handler successor) &#123; this.successor = successor; &#125; public void execute()&#123; handleProcess(); if(successor != null)&#123; successor.execute(); &#125; &#125; protected abstract void handleProcess();&#125; 下面我们看看client端实现12345678910111213141516171819202122232425262728293031323334public class ChainClient &#123; static class HandlerA extends Handler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by a"); &#125; &#125; static class HandlerB extends Handler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by b"); &#125; &#125; static class HandlerC extends Handler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by c"); &#125; &#125; public static void main(String[] args) &#123; Handler handlerA = new HandlerA(); Handler handlerB = new HandlerB(); Handler handlerC = new HandlerC(); handlerA.setSuccessor(handlerB); handlerB.setSuccessor(handlerC); handlerA.execute(); &#125;&#125; 由此我们实现了链式调用，但是我们也发现了每次都需要设置从属关系。 下面我们对上面方法改造，使用另外一个Chain对再下面的代码进行一次封装12345678public static void main(String[] args) &#123; Handler handlerA = new HandlerA(); Handler handlerB = new HandlerB(); Handler handlerC = new HandlerC(); handlerA.setSuccessor(handlerB); handlerB.setSuccessor(handlerC); handlerA.execute();&#125; 下面我们新建一个ChainHandler12345678public abstract class ChainHandler &#123; public void execute(Chain chain)&#123; handleProcess(); chain.proceed(); &#125; protected abstract void handleProcess();&#125; Chain对链式关系封装起来123456789101112131415public class Chain &#123; private List&lt;ChainHandler&gt; handlers; private int index = 0; public Chain(List&lt;ChainHandler&gt; handlers) &#123; this.handlers = handlers; &#125; public void proceed()&#123; if(index &gt;= handlers.size())&#123; return; &#125; handlers.get(index++).execute(this); &#125;&#125; 下面我们看一下如何实现调用类1234567891011121314151617181920212223242526272829public class ChainClient &#123; static class ChainHandlerA extends ChainHandler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by chain a"); &#125; &#125; static class ChainHandlerB extends ChainHandler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by chain b"); &#125; &#125; static class ChainHandlerC extends ChainHandler&#123; @Override protected void handleProcess() &#123; System.out.println("handler by chain c"); &#125; &#125; public static void main(String[] args) &#123; List&lt;ChainHandler&gt; handlers = Arrays.asList( new ChainHandlerA(), new ChainHandlerB(), new ChainHandlerC() ); Chain chain = new Chain(handlers); chain.proceed(); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>aop</tag>
        <tag>链式调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cglib动态代理]]></title>
    <url>%2F2019%2F08%2F19%2Fcglib%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[首先我们看一下如何实现cglib的动态代理 cglib通过实现MethodInterceptor接口来实现动态代理12345678910111213141516public class DemoMethodInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("before in cglib"); Object result = null; try&#123; result = methodProxy.invokeSuper(o, objects); &#125;catch (Exception e)&#123; e.printStackTrace(); throw e; &#125;finally &#123; System.out.println("after in cglib"); &#125; return result; &#125;&#125; 下面代码是告诉我们如何实现cglib的调用123456789public class MethodInterceptorClient &#123; public static void main(String[] args) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(RealSubject.class); enhancer.setCallback(new DemoMethodInterceptor()); Subject subject = (Subject) enhancer.create(); subject.hello(); &#125;&#125; JDK与Cglib代理对比JDK只能针对有接口的类的接口方法进行动态代理Cglib基 于继承来实现代理,无法对static、final类进行代理,Cglib基于继承来实现代理,无法对private、static方法进行代理 最后我们看看Spring还如何创建动态代理类的 下面我们看一下DefalutAopProxyFactory12345678910111213141516171819202122public class DefaultAopProxyFactory implements AopProxyFactory, Serializable &#123; public DefaultAopProxyFactory() &#123; &#125; public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (!config.isOptimize() &amp;&amp; !config.isProxyTargetClass() &amp;&amp; !this.hasNoUserSuppliedProxyInterfaces(config)) &#123; return new JdkDynamicAopProxy(config); &#125; else &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException("TargetSource cannot determine target class: Either an interface or a target is required for proxy creation."); &#125; else &#123; return (AopProxy)(!targetClass.isInterface() &amp;&amp; !Proxy.isProxyClass(targetClass) ? new ObjenesisCglibAopProxy(config) : new JdkDynamicAopProxy(config)); &#125; &#125; &#125; private boolean hasNoUserSuppliedProxyInterfaces(AdvisedSupport config) &#123; Class&lt;?&gt;[] ifcs = config.getProxiedInterfaces(); return ifcs.length == 0 || ifcs.length == 1 &amp;&amp; SpringProxy.class.isAssignableFrom(ifcs[0]); &#125;&#125; 在代码中我们可以看到有!config.isOptimize() &amp;&amp; !config.isProxyTargetClass() &amp;&amp; !this.hasNoUserSuppliedProxyInterfaces(config)这三个选项，如果这三个都不满足就会走JDK的实现。或者本身是JDK代理的话也会走JDK代理，其他都是走cglib代理 那我们如何强制使用cglib呢12345678@SpringBootApplication// 强制使用cglib@EnableAspectJAutoProxy(proxyTargetClass = true)public class AopDemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(AopDemoApplication.class, args); &#125;&#125; 总结：如果目标对象实现了接口,则默认采用JDK动态代理如果目标对象没有实现接口,则采用Cglib进行动态代理如果目标对象实现了接口,且强制cglib代理,则使用cglib代理]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>动态代理</tag>
        <tag>cglib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK动态代理解析]]></title>
    <url>%2F2019%2F08%2F19%2FJDK%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[首先我们生成的时候调用了Proxy.newInstance()这个方法，这个方法会调用getProxyCLass0()方法，而getProxyCLass0()是从ProxyClassFactory中生成proxy代码，而ProxyClassFactory又是使用ProxyGenerator来生成的代码，然后生成的字节码，使用反射来new一个实例。整个调用大致就是 newInstance() -&gt; getProxyCLass0() -&gt; ProxyClassFactory -&gt; ProxyGenerator -&gt; generateProxyClass 12345678public class DynamicClient &#123; public static void main(String[] args) &#123; Subject subject = (Subject) Proxy.newProxyInstance(DynamicClient.class.getClassLoader(), new Class[]&#123;Subject.class&#125;, new JDKProxySubject(new RealSubject())); subject.request(); &#125;&#125; 我们使用newProxyInstance新建一个实例，进入源码中查看会发现源码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@CallerSensitivepublic static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException&#123; Objects.requireNonNull(h); final Class&lt;?&gt;[] intfs = interfaces.clone(); final SecurityManager sm = System.getSecurityManager(); if (sm != null) &#123; checkProxyAccess(Reflection.getCallerClass(), loader, intfs); &#125; /* * Look up or generate the designated proxy class. */ Class&lt;?&gt; cl = getProxyClass0(loader, intfs); /* * Invoke its constructor with the designated invocation handler. */ try &#123; if (sm != null) &#123; checkNewProxyPermission(Reflection.getCallerClass(), cl); &#125; final Constructor&lt;?&gt; cons = cl.getConstructor(constructorParams); final InvocationHandler ih = h; if (!Modifier.isPublic(cl.getModifiers())) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; cons.setAccessible(true); return null; &#125; &#125;); &#125; return cons.newInstance(new Object[]&#123;h&#125;); &#125; catch (IllegalAccessException|InstantiationException e) &#123; throw new InternalError(e.toString(), e); &#125; catch (InvocationTargetException e) &#123; Throwable t = e.getCause(); if (t instanceof RuntimeException) &#123; throw (RuntimeException) t; &#125; else &#123; throw new InternalError(t.toString(), t); &#125; &#125; catch (NoSuchMethodException e) &#123; throw new InternalError(e.toString(), e); &#125;&#125; 在源码中有这行代码12//寻找或生成制定的代理类Class&lt;?&gt; cl = getProxyClass0(loader, intfs); 进入这个函数，源码如下：123456789101112131415 /** * Generate a proxy class. Must call the checkProxyAccess method * to perform permission checks before calling this. */private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) &#123; if (interfaces.length &gt; 65535) &#123; throw new IllegalArgumentException("interface limit exceeded"); &#125; // If the proxy class defined by the given loader implementing // the given interfaces exists, this will simply return the cached copy; // otherwise, it will create the proxy class via the ProxyClassFactory return proxyClassCache.get(loader, interfaces);&#125; 发现函数会先从缓存中获取，如果缓存中不存在，则生成一个放入缓存 缓存如下：12345/** * a cache of proxy classes */private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); 由此可见当不存在的时候是由ProxyClassFactory()来生成的 然后我们看以下这个Factory的源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * A factory function that generates, defines and returns the proxy class given * the ClassLoader and array of interfaces. */private static final class ProxyClassFactory implements BiFunction&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt;&#123; // prefix for all proxy class names private static final String proxyClassNamePrefix = "$Proxy"; // next number to use for generation of unique proxy class names private static final AtomicLong nextUniqueNumber = new AtomicLong(); @Override public Class&lt;?&gt; apply(ClassLoader loader, Class&lt;?&gt;[] interfaces) &#123; Map&lt;Class&lt;?&gt;, Boolean&gt; interfaceSet = new IdentityHashMap&lt;&gt;(interfaces.length); for (Class&lt;?&gt; intf : interfaces) &#123; /* * Verify that the class loader resolves the name of this * interface to the same Class object. */ Class&lt;?&gt; interfaceClass = null; try &#123; interfaceClass = Class.forName(intf.getName(), false, loader); &#125; catch (ClassNotFoundException e) &#123; &#125; if (interfaceClass != intf) &#123; throw new IllegalArgumentException( intf + " is not visible from class loader"); &#125; /* * Verify that the Class object actually represents an * interface. */ if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( interfaceClass.getName() + " is not an interface"); &#125; /* * Verify that this interface is not a duplicate. */ if (interfaceSet.put(interfaceClass, Boolean.TRUE) != null) &#123; throw new IllegalArgumentException( "repeated interface: " + interfaceClass.getName()); &#125; &#125; String proxyPkg = null; // package to define proxy class in int accessFlags = Modifier.PUBLIC | Modifier.FINAL; /* * Record the package of a non-public proxy interface so that the * proxy class will be defined in the same package. Verify that * all non-public proxy interfaces are in the same package. */ for (Class&lt;?&gt; intf : interfaces) &#123; int flags = intf.getModifiers(); if (!Modifier.isPublic(flags)) &#123; accessFlags = Modifier.FINAL; String name = intf.getName(); int n = name.lastIndexOf('.'); String pkg = ((n == -1) ? "" : name.substring(0, n + 1)); if (proxyPkg == null) &#123; proxyPkg = pkg; &#125; else if (!pkg.equals(proxyPkg)) &#123; throw new IllegalArgumentException( "non-public interfaces from different packages"); &#125; &#125; &#125; if (proxyPkg == null) &#123; // if no non-public proxy interfaces, use com.sun.proxy package proxyPkg = ReflectUtil.PROXY_PACKAGE + "."; &#125; /* * Choose a name for the proxy class to generate. */ long num = nextUniqueNumber.getAndIncrement(); String proxyName = proxyPkg + proxyClassNamePrefix + num; /* * Generate the specified proxy class. */ byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); try &#123; return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); &#125; catch (ClassFormatError e) &#123; /* * A ClassFormatError here means that (barring bugs in the * proxy class generation code) there was some other * invalid aspect of the arguments supplied to the proxy * class creation (such as virtual machine limitations * exceeded). */ throw new IllegalArgumentException(e.toString()); &#125; &#125;&#125; 在这个类的源码中的apply函数可以看到这几行代码生成字节码12345/* * Generate the specified proxy class. */byte[] proxyClassFile = ProxyGenerator.generateProxyClass( proxyName, interfaces, accessFlags); 下面我们在调用接口的类中添加这个系统配置1System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles", "true"); 123456789public class DynamicClient &#123; public static void main(String[] args) &#123; System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles", "true"); Subject subject = (Subject) Proxy.newProxyInstance(DynamicClient.class.getClassLoader(), new Class[]&#123;Subject.class&#125;, new JDKProxySubject(new RealSubject())); subject.request(); &#125;&#125; 之后在我们运行后会发现项目根路径中出现了下面这个文件1com/sun/proxy/$Proxy0.class 这个就是生成的字节码文件进行反编译之后，代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788//// Source code recreated from a .class file by IntelliJ IDEA// (powered by Fernflower decompiler)//package com.sun.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.lang.reflect.UndeclaredThrowableException;import xyz.suiwo.demo.Subject;public final class $Proxy0 extends Proxy implements Subject &#123; private static Method m1; private static Method m4; private static Method m2; private static Method m3; private static Method m0; public $Proxy0(InvocationHandler var1) throws &#123; super(var1); &#125; public final boolean equals(Object var1) throws &#123; try &#123; return (Boolean)super.h.invoke(this, m1, new Object[]&#123;var1&#125;); &#125; catch (RuntimeException | Error var3) &#123; throw var3; &#125; catch (Throwable var4) &#123; throw new UndeclaredThrowableException(var4); &#125; &#125; public final void hello() throws &#123; try &#123; super.h.invoke(this, m4, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final String toString() throws &#123; try &#123; return (String)super.h.invoke(this, m2, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final void request() throws &#123; try &#123; super.h.invoke(this, m3, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; public final int hashCode() throws &#123; try &#123; return (Integer)super.h.invoke(this, m0, (Object[])null); &#125; catch (RuntimeException | Error var2) &#123; throw var2; &#125; catch (Throwable var3) &#123; throw new UndeclaredThrowableException(var3); &#125; &#125; static &#123; try &#123; m1 = Class.forName("java.lang.Object").getMethod("equals", Class.forName("java.lang.Object")); m4 = Class.forName("xyz.suiwo.demo.Subject").getMethod("hello"); m2 = Class.forName("java.lang.Object").getMethod("toString"); m3 = Class.forName("xyz.suiwo.demo.Subject").getMethod("request"); m0 = Class.forName("java.lang.Object").getMethod("hashCode"); &#125; catch (NoSuchMethodException var2) &#123; throw new NoSuchMethodError(var2.getMessage()); &#125; catch (ClassNotFoundException var3) &#123; throw new NoClassDefFoundError(var3.getMessage()); &#125; &#125;&#125; 然后我们大致阅读以下反编译的代码，就会有大致了解了，所以我们多添加一个接口的时候，动态代理就会在解析成字节码文件的时候动态生成代理。但是通过静态代理，新添加多少个类就需要手动添加多少次]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JDK动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态代理与动态代理]]></title>
    <url>%2F2019%2F08%2F19%2F%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86%E4%B8%8E%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[静态代理静态代理存在一些缺点，如每当要代理的方法越多，那你需要代理的方法也越多，这样可能会有很多的重复下面我们先实现以下静态代理的例子 先创建一个接口123public interface Subject &#123; void request();&#125; 创建一个实现类123456public class RealSubject implements Subject &#123; @Override public void request() &#123; System.out.println("This is request"); &#125;&#125; 创建一个静态代理类123456789101112131415161718192021public class ProxySubject implements Subject &#123; private RealSubject realSubject; public ProxySubject(RealSubject realSubject) &#123; this.realSubject = realSubject; &#125; @Override public void request() &#123; System.out.println("before"); try &#123; realSubject.request(); &#125;catch (Exception e)&#123; e.printStackTrace(); throw e; &#125;finally &#123; System.out.println("after"); &#125; &#125;&#125; 调用静态代理类123456public class Client &#123; public static void main(String[] args) &#123; Subject subject = new ProxySubject(new RealSubject()); subject.request(); &#125;&#125; 假设我们这个时候在Subject接口中新建一个hello()方法，这个时候我们需要重写RealSubject以及ProxySubject这两个类，这是很不方便的，下面让我们看一下动态代理如何实现的 JDK与Cglib代理对比 JDK只能针对有接口的类的接口方法进行动态代理 Cglib基于继承来实现代理,无法对static、final类进行代理 Cglib基于继承来实现代理,无法对private、static方法进行代理 动态代理动态代理有两类实现：基于接口的代理和基于继承的代理，而这两类的代表分别为JDK代理和Cglib代理，下面我们来演示一下基于JDK的动态代理 而对于JDK动态代理的实现要点有以下三个 通过java.lang.reflect.Proxy类动态生成代理类 实现InvocationHandler这个几口 JDK代理只能基于接口的动态代理 首先我们创建一个JDK代理类去实现InvocationHandler这个接口，然后重写里面的invoke方法，然后利用反射动态去反射方法1234567891011121314151617181920212223public class JDKProxySubject implements InvocationHandler &#123; private RealSubject realSubject; public JDKProxySubject(RealSubject realSubject) &#123; this.realSubject = realSubject; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("proxy before"); Object result = null; try &#123; result = method.invoke(realSubject, args); &#125;catch (Exception e)&#123; e.printStackTrace(); throw e; &#125;finally &#123; System.out.println("proxy after"); &#125; return result; &#125;&#125; 然后我们看看动态代理的调用12345678public class DynamicClient &#123; public static void main(String[] args) &#123; Subject subject = (Subject) Proxy.newProxyInstance(DynamicClient.class.getClassLoader(), new Class[]&#123;Subject.class&#125;, new JDKProxySubject(new RealSubject())); subject.request(); &#125;&#125; 这个时候如果我们又要在Subject接口中新添加一个方法，这个时候我们的动态代理实现类并不需要去重写接口，因为我们实现了基于方法的反射机制]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>动态代理</tag>
        <tag>静态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream流编程]]></title>
    <url>%2F2019%2F08%2F18%2FStream%E6%B5%81%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[外部迭代与内部迭代12345678910111213public class StreamDemo &#123; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 3&#125;; int res1 = 0; for(int i = 0; i &lt; nums.length; i++)&#123; res1 += nums[i]; &#125; System.out.println("结果是：" + res1); int res2 = IntStream.of(nums).sum(); System.out.println("结果是：" + res2); &#125;&#125; 中间操作/终止操作和惰性操作12345678910111213141516public class StreamDemo &#123; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 3&#125;; // map就是中间操作（返回stream的操作） // sum就是终止操作 int res = IntStream.of(nums).map(StreamDemo::doubleNum).sum(); System.out.println("结果是：" + res); System.out.println("惰性求值就是在终止操作没有执行的情况下，中间操作不会执行"); IntStream.of(nums).map(StreamDemo::doubleNum); &#125; public static int doubleNum(int i)&#123; System.out.println("进入了doubleNum静态方法"); return 2 * i; &#125;&#125; 然后我们可以看到输出如下： 12345进入了doubleNum静态方法进入了doubleNum静态方法进入了doubleNum静态方法结果是：12惰性求值就是在终止操作没有执行的情况下，中间操作不会执行 因为我们有三个数，所以执行了三次，但是因为第二次的是惰性求值所以没有执行静态方法。 Stream流编程的创建 1234567891011121314151617181920212223public class StreamDemo1 &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); //从集合创建 list.stream(); list.parallelStream(); // 从数组创建 Arrays.stream(new int[]&#123;1,2,3&#125;); // 创建数字流 IntStream.of(1, 2, 3); IntStream.rangeClosed(1, 10); // 从random中创建一个无限流 new Random().ints().limit(10); // 自定义创建 Random random = new Random(); Stream.generate(() -&gt; random.nextInt()).limit(20); &#125;&#125; stream中间操作12345678910111213141516171819202122public class StreamDemo2 &#123; public static void main(String[] args) &#123; String str = "my name is 007"; // 把每个单词中单词长度大于2的长度调用出来 Stream.of(str.split(" ")).filter(s -&gt; s.length() &gt; 2).map(s -&gt; s.length()).forEach(System.out::println); // flatMap A-&gt;B 属性是一个集合，最终得到所有的A元素中的所有B属性 // intStream/longStream并不是Stream的子类，所以要使用装箱 boxed Stream.of(str.split(" ")).flatMap(s -&gt; s.chars().boxed()).forEach( i -&gt; System.out.println((char)i.intValue()) ); System.out.println("-------- start peek --------"); // peek用于debug，是一个中间操作，和forEach是个终止操作 Stream.of(str.split(" ")).peek(System.out::println).forEach(System.out::println); System.out.println("-------- start random --------"); // limit的使用，主要用于无限流 new Random().ints().filter(i -&gt; i &gt; 100 &amp;&amp; i &lt; 10000).limit(10).forEach(System.out::println); &#125;&#125; stream流终止操作123456789101112131415161718192021222324252627282930313233public class StreamDemo3 &#123; public static void main(String[] args) &#123; String str = "my name is 007"; // 使用并行流 str.chars().parallel().forEach(i -&gt; System.out.println((char)i)); // 使用并行流顺序打印 str.chars().parallel().forEachOrdered(i -&gt; System.out.println((char)i)); // 收集到list List&lt;String&gt; list = Stream.of(str.split(" ")).collect(Collectors.toList()); System.out.println(list); // 使用Reduce拼接字符串 Optional&lt;String&gt; letters = Stream.of(str.split(" ")).reduce((s1, s2) -&gt; s1 + "|" + s2); System.out.println(letters.orElse("")); // 使用初始化值的reduce String reduce = Stream.of(str.split(" ")).reduce("", (s1, s2) -&gt; s1 + "|" + s2); System.out.println(reduce); // 使用初始化值的reduce Integer reduce1 = Stream.of(str.split(" ")).map(String::length).reduce(0, Integer::sum); System.out.println(reduce1); Optional&lt;String&gt; max = Stream.of(str.split(" ")).max(Comparator.comparingInt(String::length)); System.out.println(max.get()); OptionalInt first = new Random().ints().findFirst(); System.out.println(first); &#125;&#125; 并行流1234567891011121314151617181920212223242526272829public class StreamDemo4 &#123; public static void main(String[] args) throws InterruptedException &#123; // 串行 IntStream.range(1, 100).peek(StreamDemo4::debug1).count(); // 并行，并行流默认使用线程池ForkJoinPool.commonPool-worker，默认大小为当前机器的cpu个数，可以使用 // System.setProperty"java.util.concurrent.ForkJoinPool.common.parallelism "20"); // 来设置默认线程数 IntStream.range(1, 100).parallel().peek(StreamDemo4::debug2).count(); // 多次调用以最后一次为准 IntStream.range(1, 100).parallel().peek(StreamDemo4::debug1).sequential().peek(StreamDemo4::debug2).count(); // 使用自己的线程池，防止都是用默认线程然后导致的线程阻塞 ForkJoinPool pool = new ForkJoinPool(20); pool.submit(() -&gt; IntStream.range(0, 100).parallel().peek(StreamDemo4::debug1).count()); pool.shutdown(); pool.wait(); &#125; public static void debug1(int i)&#123; System.out.println(Thread.currentThread().getName() + " debug1 " + i); &#125; public static void debug2(int i)&#123; System.err.println(Thread.currentThread().getName() + " debug2 " + i); &#125;&#125; 收集器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class StreamDemo5 &#123; public static void main(String[] args) &#123; List&lt;Student&gt; list = Arrays.asList( new Student("zhangsan1", 11, Sex.MALE, Gender.ONE), new Student("zhangsan2", 12, Sex.FEMAlE, Gender.TWO), new Student("zhangsan3", 13, Sex.MALE, Gender.THREE), new Student("zhangsan4", 14, Sex.FEMAlE, Gender.ONE), new Student("zhangsan5", 15, Sex.MALE, Gender.TWO), new Student("zhangsan6", 16, Sex.FEMAlE, Gender.THREE), new Student("zhangsan7", 17, Sex.MALE, Gender.ONE) ); // 可以使用多种方式创建多种的集合 List&lt;Integer&gt; ages = list.stream().map(Student::getAge).collect(Collectors.toList()); Set&lt;Integer&gt; agesSet1 = list.stream().map(Student::getAge).collect(Collectors.toSet()); Set&lt;Integer&gt; agesSet2 = list.stream().map(Student::getAge).collect(Collectors.toCollection(TreeSet::new)); System.out.println("所有学生的年龄是：" + ages); // 获取学生的年龄汇总信息 IntSummaryStatistics studentList = list.stream().collect(Collectors.summarizingInt(Student::getAge)); System.out.println("学生的汇总信息是：" + studentList); // 分块 Map&lt;Boolean, List&lt;Student&gt;&gt; sex = list.stream().collect(Collectors.partitioningBy(s -&gt; s.getSex() == Sex.MALE)); System.out.println("按照性别分块的结果是：" + sex); // 分组 Map&lt;Gender, List&lt;Student&gt;&gt; groups = list.stream().collect(Collectors.groupingBy(Student::getGender)); System.out.println("按照班级分组的结果是：" + groups); // 获取每个班级的人数 Map&lt;Gender, Long&gt; nums = list.stream().collect(Collectors.groupingBy(Student::getGender, Collectors.counting())); System.out.println("每个班级的人数结果是：" + nums); &#125;&#125;class Student&#123; private String name; private Integer age; private Sex sex; private Gender gender; public Student(String name, Integer age, Sex sex, Gender gender)&#123; this.name = name; this.age = age; this.sex = sex; this.gender = gender; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Sex getSex() &#123; return sex; &#125; public void setSex(Sex sex) &#123; this.sex = sex; &#125; public Gender getGender() &#123; return gender; &#125; public void setGender(Gender gender) &#123; this.gender = gender; &#125; @Override public String toString() &#123; return "Student&#123;" + "name='" + name + '\'' + ", age=" + age + ", sex=" + sex + ", gender=" + gender + '&#125;'; &#125;&#125;enum Sex&#123; MALE, FEMAlE&#125;enum Gender&#123; ONE, TWO, THREE, FOUR, FIVE&#125; Stream运行机制12345678910111213141516171819202122232425262728293031/** * 验证Stream运行机制 * * 1.所有操作都是链式调用，一个元素之迭代一次 * 2.每一个中间操作返回一个新的流，流里面有一个属性sourceStage执行同一个地方，就是Head * 3.Head -&gt; nextStage -&gt; nextStage -&gt; ... -&gt; null * 4.如果多个无状态操作与有状态操作交错存在，，则每个有状态操作会把之前的无状态操作段单独处理 * 5.并行状态下，有状态的中间操作不一定能并行操作 * 6.parallel()操作和sequential()都是中间操作（也是但会stream）但是他们不创建流，他们只修改我们的Head的并行标志 */public class StreamDemo6 &#123; public static void main(String[] args) &#123; Random random = new Random(); Stream.generate(() -&gt; random.nextInt()) //产生五百个数据 .limit(500) // 第一个无状态操作 .peek(s -&gt; print("peek:" + s)) // 第二个无状态操作 .filter(s -&gt; &#123; print("filter:" + s); return s &gt; 10000; &#125;) // 终止操作 .count(); &#125; private static void print(String s) &#123; System.out.println(s); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jdk8</tag>
        <tag>java</tag>
        <tag>Stream流编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lambda表达式]]></title>
    <url>%2F2019%2F08%2F18%2Flambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[什么是lambda表达式“Lambda 表达式”(lambda expression)是一个匿名函数，Lambda表达式基于数学中的λ演算得名，直接对应于其中的lambda抽象(lambda abstraction)，是一个匿名函数，即没有函数名的函数。Lambda表达式可以表示闭包（注意和数学传统意义上的不同）。 下面是我们常见的命令式编程12345678910public class LambdaDemo &#123; public static void main(String[] args) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("start"); &#125; &#125;).start(); &#125;&#125; 下面这个就是函数式编程12345public class LambdaDemo &#123; public static void main(String[] args) &#123; new Thread(() -&gt; System.out.println("start")).start(); &#125;&#125; 由此可见我们可以对一些接口代码进行简化，但是是什么样的接口都可以使用lambda表达式吗？显然不是的。接口必须满足接口里面只有一个要实现的方法。我们可以在想要实现函数式编程的接口添加注解@FunctionalInterface，进行编译期间的校验，当接口不满足我们的条件的时候，将会有错误信息。 默认方法在上面我们说了接口要实现只有一个要实现的方法，并不是说接口只能有一个方法，这个时候我们可以使用比如像默认方法这样的方式来解决1234567@FunctionalInterfaceinterface InterfaceTest&#123; int add(int i, int j); default void print() &#123; System.out.println("This is default method"); &#125;&#125; 函数式编程 静态方法 动态方法的引用下面是对于一些常见方法引用的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class MethodReferenceDemo &#123; public static void main(String[] args) &#123; // 方法引用 Consumer&lt;String&gt; consumer1 = System.out::println; consumer1.accept("接受的"); // 静态方法的引用 Consumer&lt;Dog&gt; consumer2 = Dog::bark; Dog dog = new Dog(); consumer2.accept(dog); // 非静态方法引用，使用对象实例的方法引用// Function&lt;Integer, Integer&gt; function = dog::eat;// UnaryOperator&lt;Integer&gt; function = dog::eat;// System.out.println("还剩下" + function.apply(2) + "斤"); IntUnaryOperator function = dog::eat; System.out.println("还剩下" + function.applyAsInt(2) + "斤"); //用类名来引用非静态方法 BiFunction&lt;Dog, Integer, Integer&gt; eatFunction = Dog::eat; System.out.println("还剩下" + eatFunction.apply(dog, 2) + "斤"); // 构造函数的方法引用 Supplier&lt;Dog&gt; dogSupplier = Dog::new; System.out.println("创建了新对象：" + dogSupplier.get()); // 带参数的构造方法的引用 Function&lt;String, Dog&gt; dogFunction = Dog::new; System.out.println("创建了新对象：" + dogFunction.apply("旺财")); &#125;&#125;class Dog&#123; private String name = "dog"; private Integer food = 10; public Dog()&#123;&#125; public Dog(String name)&#123; this.name = name; &#125; public static void bark(Dog dog)&#123; System.out.println(dog + "叫了"); &#125; public int eat(int i)&#123; System.out.println("吃了" + i + "斤"); this.food -= i; return this.food; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return "Dog&#123;" + "name='" + name + '\'' + '&#125;'; &#125;&#125; 级联和柯 柯里化：把多个参数的函数转换成只有一个参数的函数 柯里化的目的：函数标准化 高阶函数：返回函数的函数 下面是对级联和柯里化的一些代码实现12345678910111213141516171819202122232425public class CurryDemo &#123; public static void main(String[] args) &#123; // 级联表达式 Function&lt;Integer, Function&lt;Integer, Integer&gt;&gt; fun1 = x -&gt; y -&gt; x + y; System.out.println(fun1.apply(1).apply(2)); Function&lt;Integer, Function&lt;Integer, Function&lt;Integer, Integer&gt;&gt;&gt; fun2 = x -&gt; y -&gt; z -&gt; x + y + z; System.out.println(fun2.apply(1).apply(2).apply(3)); int[] nums = &#123;2, 3, 4&#125;; Function f = fun2; for(int i : nums)&#123; if(f instanceof Function)&#123; Object obj = f.apply(i); if(obj instanceof Function)&#123; f = (Function) obj; &#125;else &#123; System.out.println("调用结束：结果为" + obj); &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>jdk8</tag>
        <tag>java</tag>
        <tag>lambda</tag>
        <tag>级联</tag>
        <tag>柯里化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消费端限流、重回队列、TTL以及死信队列]]></title>
    <url>%2F2019%2F08%2F16%2F%E6%B6%88%E8%B4%B9%E7%AB%AF%E9%99%90%E6%B5%81%E3%80%81%E9%87%8D%E5%9B%9E%E9%98%9F%E5%88%97%E3%80%81TTL%E4%BB%A5%E5%8F%8A%E6%AD%BB%E4%BF%A1%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[什么是消费端的限流?假设一个场景，首先，我们Rabbitmq服务器有上万条未处理的消息，我们随便打开一个消费者客户端，会出现下面情况:巨量的消息瞬间全部推送过来，但是我们单个客户端无法同时处理这么多数据! RabbitMQ提供了一种qos (服务质量保证)功能，即在非自动确认消息的前提下，如果一定数目的消息(通过基于consume或者channel设置Qos的值)未被确认前，不进行消费新的消息。 1void BasicQos(uint prefetchSize, ushort prefetchCount, bool global); prefetchSize:0 prefetchCount:会告诉RabbitMQ不要同时给一个消费者推送多于N个消息，即一旦有N个消息还没有ack,则该consumer将block掉，直到有消息ack global: true\false 是否将上面设置应用于channel,简单点说，就是上面限制是channel级别的还是consumer级别 注：prefetchSize和global这两项，rabbitmq没有实现，暂且不研究prefetch count在no ask= false的情况下生效，即在自动应答的情况下这两个值是不生效的。并且千万不要使用AutoACK。一定要使用手动ack 消费端的手工ACK和NACK消费端进行消费的时候，如果由于业务异常我们可以进行日志的记录，然后进行补偿!如果由于服务器宕机等严重问题,那我们就需要手工进行ACK保障，消费端消费成功! 消费端的重回队列消费端重回队列是为了对没有处理成功的消息，把消息重新会递给Broker!一般我们在实际应用中，都会关闭重回队列，也就是设置为False。 TTL TTL是Time To Live的缩写，也就是生存时间 RabbitMQ 支持消息的过期时间，在消息发送时可以进行指定 RabbitMQ 支持队列的过期时间，从消息入队列开始计算，只要超过了队列的超时时间配置，那么消息会自动的清除 死信队列 DLX, Dead-Letter-Exchange利用DLX,当消息在一个队列中变成死信(dead message)之后,它能被重新publish到另一个Exchange, 这个Exchange就是DLX DLX也是一个正常的Exchange,和一-般的Exchange没有区别，它能在任何的队列上被指定，实际上就是设置某个队列的属性。 当这个队列中有死信时，RabbitMQ就会 自动的将这个消息重新发布到设置的Exchange_上去，进而被路由到另一个队列。 可以监听这个队列中消息做相应的处理，这个特性可以弥补RabbitMQ3.0以前支持的immediate参数的功能。 消息变成死信有以下几种情况： 消息被拒绝(basic.reject/basic.nack) 并且requeue=false 消息TTL过期 队列达到最大长度]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ生产端可靠性投递]]></title>
    <url>%2F2019%2F08%2F16%2FRabbitMQ%E7%94%9F%E4%BA%A7%E7%AB%AF%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8A%95%E9%80%92%2F</url>
    <content type="text"><![CDATA[对于消息的生产端的可靠投递，我们常见的解决方案有两种1.消息落库，对消息状态进行打标2.消息的延迟投递，做二次确认，回调检查 1、消息落库，对消息状态进行打标 上面图片为消息落库，对消息状态进行打标的常见步骤（状态0表示已发送，1表示已消费，2表示失败） 首先将将要发送的数据持久化到BIZ数据库中，并且创建一个存储着消息状态的数据持久化到MSG数据库中。 将数据发送至MQ。 消费者接收到数据，对数据进行消费然后将MSG在数据库中的状态修改为1。 在此之外，我们还存在一个分布式定时任务线程，用于定时查看是否有超时失败任务，当发现MSG数据库中存在着状态为0的数据则对数据进行重发，当数据多次重发失败后则将消息状态修改为2。 由于这个方式需要进行两个数据的数据落库容易产生数据库的性能瓶颈，所以我们更多的使用的是下一个可靠性投递的解决方式 消息的延迟投递，做二次确认，回调检查 上面图片时延迟投递的流程（upstream上游服务，downstream下游服务（消费端）） 首先将需要发送的数据持久化到BIZ数据库。 将数据发送至MQ中。 消费者消费数据，并新建一个消费成功的消息进入MQ。 Callback服务获取到消费者消费成功的消息后将消息持久化到MSG数据库。 生产者将延迟投递消息发送至MQ。 Callback获取到延迟投递消息后进入MSG数据库查询是否投递成功，如果投递失败则进行失败补偿，这是向上游服务发送消息，让上游服务的查询BIZ数据库然后进行消息重发。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）如何保证消息的可靠性传输]]></title>
    <url>%2F2019%2F08%2F16%2F%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E4%BC%A0%E8%BE%93%2F</url>
    <content type="text"><![CDATA[原文链接：https://doocs.github.io/advanced-java/#/ 为什么需要保证数据的可靠性传输？如果用 MQ 来传递非常核心的消息，比如说计费、扣费的一些消息，那必须确保这个 MQ 传递过程中绝对不会把计费消息给弄丢。我们常见的数据的丢失问题，可能出现在生产者、MQ、消费者中，本文主要从RabbitMQ的角度进行分析。如果向看Kafka相关的总结，请进入原博文查找 生产者弄丢了数据生产者将数据发送到 RabbitMQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。此时可以选择用 RabbitMQ 提供的事务功能，就是生产者发送数据之前开启 RabbitMQ 事务channel.txSelect，然后发送消息，如果消息没有成功被 RabbitMQ 接收到，那么生产者会收到异常报错，此时就可以回滚事务channel.txRollback，然后重试发送消息；如果收到了消息，那么可以提交事务1234567891011channel.txCommit。// 开启事务channel.txSelecttry &#123; // 这里发送消息&#125; catch (Exception e) &#123; channel.txRollback // 这里再次重发这条消息&#125;// 提交事务channel.txCommit 但是问题是，RabbitMQ 事务机制（同步）一搞，基本上吞吐量会下来，因为太耗性能。所以一般来说，如果你要确保说写 RabbitMQ 的消息别丢，可以开启 confirm 模式，在生产者那里设置开启 confirm 模式之后，你每次写的消息都会分配一个唯一的 id，然后如果写入了 RabbitMQ 中，RabbitMQ 会给你回传一个 ack 消息，告诉你说这个消息 ok 了。如果 RabbitMQ 没能处理这个消息，会回调你的一个 nack 接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息 id 的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和 confirm 机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是 confirm 机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息 RabbitMQ 接收了之后会异步回调你的一个接口通知你这个消息接收到了。所以一般在生产者这块避免数据丢失，都是用 confirm 机制的。 RabbitMQ 弄丢了数据 RabbitMQ 自己弄丢了数据，这个你必须开启 RabbitMQ 的持久化，就是消息写入之后会持久化到磁盘，哪怕是 RabbitMQ 自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，RabbitMQ 还没持久化，自己就挂了，可能导致少量数据丢失，但是这个概率较小。 设置持久化有两个步骤： 创建 queue 的时候将其设置为持久化,这样就可以保证 RabbitMQ 持久化 queue 的元数据，但是它是不会持久化 queue 里的数据的。 第二个是发送消息的时候将消息的 deliveryMode 设置为 2，delivery_mode=2指明message为持久的就是将消息设置为持久化的，此时 RabbitMQ 就会将消息持久化到磁盘上去。 必须要同时设置这两个持久化才行，RabbitMQ 哪怕是挂了，再次重启，也会从磁盘上重启恢复 queue，恢复这个 queue 里的数据。注意，哪怕是你给 RabbitMQ 开启了持久化机制，也有一种可能，就是 这个消息写到了 RabbitMQ 中，但是还没来得及持久化到磁盘上，结果不巧，此时 RabbitMQ 挂了，就会导致内存里的一点点数据丢失。在正确存入RabbitMQ之后，还需要有一段时间（这个时间很短，但不可忽视）才能存入磁盘之中，RabbitMQ并不是为每条消息都做fsync的处理，可能仅仅保存到cache中而不是物理磁盘上，在这段时间内RabbitMQ broker发生crash, 消息保存到cache但是还没来得及落盘，那么这些消息将会丢失。那么这个怎么解决呢？ 首先可以引入RabbitMQ的mirrored-queue即镜像队列，这个相当于配置了副本，当master在此特殊时间内crash掉，可以自动切换到slave，这样有效的保障了HA, 除非整个集群都挂掉。 持久化可以跟生产者那边的 confirm 机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者 ack了，所以哪怕是在持久化到磁盘之前，RabbitMQ 挂了，数据丢了，生产者收不到 ack，你也是可以自己重发的。 消费端弄丢了数据 主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了，比如重启了，那么就尴尬了，RabbitMQ 认为你都消费了，这数据就丢了。这个时候得用 RabbitMQ 提供的 ack 机制，简单来说，就是你必须关闭 RabbitMQ 的自动 ack，可以通过一个 api 来调用就行，然后每次你自己代码里确保处理完的时候，再在程序里 ack 一把。这样的话，如果你还没处理完，不就没有 ack 了？那 RabbitMQ 就认为你还没处理完，这个时候 RabbitMQ 会把这个消费分配给别的 consumer 去处理，消息是不会丢的。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何保证消息消费的幂等性]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E6%B6%88%E8%B4%B9%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7%2F</url>
    <content type="text"><![CDATA[幂等性概念及业界主流解决方案幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单的例子，那就是支付，用户购买商品使用约支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额返发现多扣钱了，流水记录也变成了两条．．． 业界主流的幂等性操作： 唯一ID + 指纹码机制，利用数据库主键去重SELECT COUNT(1) FROM T_ORDER WHERE ID = 唯一ID +指纹码好处:实现简单坏处:高并发下有数据库写入的性能瓶颈解决方案:跟进ID进行分库分表进行算法路由 使用Redis的原子特性去重使用Redis进行幂等，需要考虑的问题第一:我们是否要进行数据落库,如果落库的话，关键解决的问题是数据库和缓存如何做到原子性?第二:如果不进行落库，那么都存储到缓存中，如何设置定时同步的策略? 常见解决思路 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）如何保证消息队列的高可用？]]></title>
    <url>%2F2019%2F08%2F16%2F%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[原文链接：https://doocs.github.io/advanced-java/#/ RabbitMQ 的高可用性RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。 RabbitMQ 有三种模式： 单机模式 普通集群模式 镜像集群模式 单机模式单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的😄，没人生产用单机模式。 普通集群模式（无高可用性）普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。 这种方式确实很麻烦，也不怎么好，没做到所谓的分布式，就是个普通集群。因为这导致你要么消费者每次随机连接一个实例然后拉取数据，要么固定连接那个 queue 所在实例消费数据，前者有数据拉取的开销，后者导致单实例性能瓶颈。而且如果那个放 queue 的实例宕机了，会导致接下来其他实例就无法从那个实例拉取，如果你开启了消息持久化，让 RabbitMQ 落地存储消息的话，消息不一定会丢，得等这个实例恢复了，然后才可以继续从这个 queue 拉取数据。所以这个事儿就比较尴尬了，这就没有什么所谓的高可用性，这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 镜像集群模式（高可用性）这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。 那么如何开启这个镜像集群模式呢？其实很简单，RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。 这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！第二，这么玩儿，不是分布式的，就没有扩展性可言了，如果某个 queue 负载很重，你加机器，新增的机器也包含了这个 queue 的所有数据，并没有办法线性扩展你的 queue。你想，如果这个 queue 的数据量很大，大到这个机器上的容量无法容纳了，此时该怎么办呢？]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[（转载）为什么使用消息队列？]]></title>
    <url>%2F2019%2F08%2F16%2F%EF%BC%88%E8%BD%AC%E8%BD%BD%EF%BC%89%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[原链接：https://doocs.github.io/advanced-java/#/ 为什么使用消息队列当系统中出现“生产“和“消费“的速度或稳定性等因素不一致的时候，就需要消息队列，作为抽象层，弥合双方的差异。“ 消息 ”是在两台计算机间传送的数据单位。消息可以非常简单，例如只包含文本字符串；也可以更复杂，可能包含嵌入对象。消息被发送到队列中，“ 消息队列 ”是在消息的传输过程中保存消息的容器 。 提高系统响应速度使用了消息队列，生产者一方，把消息往队列里一扔，就可以立马返回，响应用户了。无需等待处理结果。处理结果可以让用户稍后自己来取，如医院取化验单。也可以让生产者订阅（如：留下手机号码或让生产者实现listener接口、加入监听队列），有结果了通知。获得约定将结果放在某处，无需通知。 提高系统稳定性考虑电商系统下订单，发送数据给生产系统的情况。电商系统和生产系统之间的网络有可能掉线，生产系统可能会因维护等原因暂停服务。如果不使用消息队列，电商系统数据发布出去，顾客无法下单，影响业务开展。两个系统间不应该如此紧密耦合。应该通过消息队列解耦。同时让系统更健壮、稳定。 简而言之，主要是为了解决下面这个三个问题 异步化 解耦 消除峰值 解耦看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。 异步再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ 削峰每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。 消息队列有什么优缺点优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。缺点有以下几个： 系统可用性降低：系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。-系统复杂度提高：硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题：A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。 所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？ 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 综上，各种对比之后，有如下建议：一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成mapstruct]]></title>
    <url>%2F2019%2F08%2F11%2FSpringboot%E9%9B%86%E6%88%90mapstruct%2F</url>
    <content type="text"><![CDATA[一、什么是mapstructMapStruct是一个代码生成器的工具类，简化了不同的Java Bean之间映射的处理，所以映射指的就是从一个实体变化成一个实体。在实际项目中，我们经常会将PO转DTO、DTO转PO等一些实体间的转换。在转换时大部分属性都是相同的，只有少部分的不同，这时我们可以通过mapStruct的一些注解来匹配不同属性，可以让不同实体之间的转换变的简单。MapStruct官网地址： http://mapstruct.org/ 二、添加依赖12345678910111213&lt;!--MapStruct依赖--&gt;&lt;!-- https://mvnrepository.com/artifact/org.mapstruct/mapstruct-jdk8 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-jdk8&lt;/artifactId&gt; &lt;version&gt;1.0.0.Final&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.mapstruct/mapstruct-processor --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;1.0.0.Final&lt;/version&gt;&lt;/dependency&gt; 三、mapstruct实体间的转换12345678//Info.javapublic class Info&#123; private Integer userId; private String email; private Integer score; private String remark; //constructor, getters, setters etc.&#125; 12345678//InfoDto.javapublic class InfoDto&#123; private Integer id; private Integer value; private String remark; private User user; //constructor, getters, setters etc.&#125; 123456//User.javapublic class User&#123; private Integer userId; private String email; //constructor, getters, setters etc.&#125; 四、mapper接口要生成一个PeopleDTO与PeopleEntity对象相互转换的映射器，我们需要定义一个mapper接口。像这两个实体类有些属性不一样时，我们可以通过@Mapping注解来进行转换. @Mapper注解标记这个接口作为一个映射接口，并且是编译时MapStruct处理器的入口。 @Mapping解决源对象和目标对象中，属性名字不同的情况。 Mappers.getMapper自动生成的接口的实现可以通过Mapper的class对象获取,从而让客户端可以访问Mapper接口的实现。 1234567891011121314151617181920212223242526@Mapperpublic interface InfoMapper &#123; PeopleMapper INSTANCE = Mappers.getMapper(InfoMapper.class); /** * PO转DTO * * @param info PO * @return DTO */ @Mapping(target = "value", source = "score") @Mapping(target = "user.userId", source = "userId") @Mapping(target = "user.email", source = "email") InfoDTO entityToDTO(Info info); /** * DTO转PO * * @param infoDTO DTO * @param entity PO */ @Mapping(target = "score", source = "value") @Mapping(target = "userId", source = "user.userId") @Mapping(target = "email", source = "user.email") void updateInfoFromDto(InfoDTO InfoDTO, @MappingTarget Info info);&#125; 当运行是将会自动编译我们的InfoMapper.java 下面是我在学习时写的Mapper文件以及编译后的一个文件123456789@Mapper(componentModel = "spring")public interface TeamMapper &#123; TeamVo poToTeamVo(TeamPo teamPo); Team poToTeamDo(TeamPo teamPo); List&lt;TeamVo&gt; poListToVoList(List&lt;TeamPo&gt; teamPoList);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Generated( value = "org.mapstruct.ap.MappingProcessor", date = "2019-08-08T13:30:54+0800", comments = "version: 1.0.0.Final, compiler: javac, environment: Java 1.8.0_211 (Oracle Corporation)")@Componentpublic class TeamMapperImpl implements TeamMapper &#123; @Override public TeamVo poToTeamVo(TeamPo teamPo) &#123; if ( teamPo == null ) &#123; return null; &#125; TeamVo teamVo = new TeamVo(); teamVo.setId( teamPo.getId() ); teamVo.setName( teamPo.getName() ); teamVo.setDepartmentId( teamPo.getDepartmentId() ); teamVo.setDepartmentName( teamPo.getDepartmentName() ); return teamVo; &#125; @Override public Team poToTeamDo(TeamPo teamPo) &#123; if ( teamPo == null ) &#123; return null; &#125; Team team = new Team(); team.setId( teamPo.getId() ); team.setName( teamPo.getName() ); team.setDepartmentId( teamPo.getDepartmentId() ); return team; &#125; @Override public List&lt;TeamVo&gt; poListToVoList(List&lt;TeamPo&gt; teamPoList) &#123; if ( teamPoList == null ) &#123; return null; &#125; List&lt;TeamVo&gt; list = new ArrayList&lt;TeamVo&gt;(); for ( TeamPo teamPo : teamPoList ) &#123; list.add( poToTeamVo( teamPo ) ); &#125; return list; &#125;&#125; 五、注意1、 由于编译可能不及时的原因，所以一开始遇到了修改了po或者dto的一个类，但是mapper没有及时的重新编译，所以dto，po，vo有更改的话，建议先clean一下之后重新编译然后运行 2、 在使用mapstruct + lombok时要注意maven-comiler-plugin插件版本一定要在3.6.0以上，若版本低，则会报找不到属性的错误下面是一个依赖模板12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;xyz.suiwo&lt;/groupId&gt; &lt;artifactId&gt;gradingdog&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;gradingdog&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;org.mapstruct.version&gt;1.2.0.Beta2&lt;/org.mapstruct.version&gt; &lt;org.projectlombok.version&gt;1.16.14&lt;/org.projectlombok.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--MapStruct依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;!-- use mapstruct-jdk8 for Java 8 or higher --&gt; &lt;version&gt;$&#123;org.mapstruct.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-jdk8&lt;/artifactId&gt; &lt;version&gt;$&#123;org.mapstruct.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;org.projectlombok.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;!-- or higher, depending on your project --&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;!-- or higher, depending on your project --&gt; &lt;annotationProcessorPaths&gt; &lt;path&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;org.projectlombok.version&#125;&lt;/version&gt; &lt;/path&gt; &lt;path&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;$&#123;org.mapstruct.version&#125;&lt;/version&gt; &lt;/path&gt; &lt;/annotationProcessorPaths&gt; &lt;compilerArgs&gt; &lt;arg&gt;-Amapstruct.suppressGeneratorTimestamp=true&lt;/arg&gt; &lt;arg&gt;-Amapstruct.defaultComponentModel=spring&lt;/arg&gt; &lt;/compilerArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>mapstruct</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手写Spring框架]]></title>
    <url>%2F2019%2F08%2F09%2F%E6%89%8B%E5%86%99Spring%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[本文是对spring常见的如ioc等功能的手写实现 本文的代码的链接在下面项目链接：https://github.com/ZhangJia97/Spring-demo 下面这个项目链接是对上面这个手写的spring框架的部分优化项目链接：https://github.com/ZhangJia97/SpringMVC-demo 项目结构1234567891011121314151617181920212223242526272829├── pom.xml├── spring-demo.iml├── src └── main ├── java │ └── xyz │ └── suiwo │ ├── action │ │ ├── controller │ │ │ └── DemoController.java │ │ └── service │ │ ├── DemoService.java │ │ └── impl │ │ └── DemoServiceImpl.java │ └── framework │ ├── annotation │ │ ├── SWAutowried.java │ │ ├── SWController.java │ │ ├── SWRequestMapping.java │ │ ├── SWRequestParam.java │ │ └── SWService.java │ └── servlet │ └── SWDispatcherServlet.java ├── resources │ └── application.properties └── web ├── WEB-INF │ └── web.xml └── index.jsp 所用依赖123456&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;$&#123;servlet.api.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; web.xml配置如下1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd" version="4.0"&gt; &lt;display-name&gt;Suiwo Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;swmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;xyz.suiwo.framework.servlet.SWDispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;application.properties&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;swmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; application.properties配置如下1scanPackage=xyz.suiwo.action 下面是几个我们自定义的注解类123456@Target(&#123;ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SWAutowried &#123; String value() default "";&#125; 123456@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SWController &#123; String value() default "";&#125; 123456@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SWRequestMapping &#123; String value() default "";&#125; 123456@Target(&#123;ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SWRequestParam &#123; String value() default "";&#125; 123456@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SWService &#123; String value() default "";&#125; 下面我们新建几个controller类和service类并使用我们自定义的注解123456789101112131415161718@SWController@SWRequestMapping("/get")public class DemoController &#123; @SWAutowried DemoService demoService; @SWRequestMapping("/method") public void get(HttpServletRequest request, HttpServletResponse response, @SWRequestParam("name") String name)&#123; demoService.get(); try &#123; response.getWriter().write("my name is " + name); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 123public interface DemoService &#123; void get();&#125; 123456@SWServicepublic class DemoServiceImpl implements DemoService &#123; public void get() &#123; System.out.println("This is method get"); &#125;&#125; 最后让我们看一下我们的Dispatcher类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241public class SWDispatcherServlet extends HttpServlet &#123; private Properties properties = new Properties(); private List&lt;String&gt; classNames = new ArrayList&lt;&gt;(); private Map&lt;String, Object&gt; ioc = new HashMap&lt;&gt;(); private Map&lt;String, Method&gt; handlerMapping = new HashMap&lt;&gt;(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; doDispatch(req,resp); //注：当使用父类的doGet以及doPost可能会导致405错误// super.doGet(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) &#123; doDispatch(req,resp);// doDispatch(req, resp); &#125; @Override public void init(ServletConfig config) &#123; //1.加载配置文件 doLoadConfig(config.getInitParameter("contextConfigLocation")); //2.扫描所有相关的类 doScanner(properties.getProperty("scanPackage")); //3.初始化所有相关Class的实例，并且将其保存到IOC容器中 doInstance(); //4.自动化的依赖注入 doAutowired(); //5.初始化handlerMapping initHandlerMapping(); System.out.println("初始化成功"); &#125; private void doLoadConfig(String location) &#123; InputStream inputStream =null; try &#123; inputStream = this.getClass().getClassLoader().getResourceAsStream(location); properties.load(inputStream); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != inputStream) &#123; inputStream.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; //进行扫描 private void doScanner(String packageName) &#123; URL url = this.getClass().getClassLoader() .getResource("/" + packageName.replaceAll("\\.", "/")); File classDir = new File(Objects.requireNonNull(url).getFile()); for(File file : Objects.requireNonNull(classDir.listFiles()))&#123; //递归获取scan下所有的类，并将类名添加至classNames中用于之后实例化 if(file.isDirectory())&#123; doScanner(packageName + "." + file.getName()); &#125;else &#123; String className = packageName + "." + file.getName().replace(".class","").trim(); classNames.add(className); &#125; &#125; &#125; //进行实例化 private void doInstance() &#123; if(classNames.isEmpty())&#123; return; &#125; for(String className : classNames)&#123; try &#123; Class&lt;?&gt; clazz = Class.forName(className); //进行实例化 //判断不是所有的都需要实例化，只有添加了例如Controller或者Service的注解才初始化 if(clazz.isAnnotationPresent(SWController.class))&#123; //beanName beanId //1.默认采用类名的首字母小写 //2.如果自定义了名字，默认使用自定义名字 //3.根据类型匹配，利用实现类的接口名作为Key String beanName = toLowStr(clazz.getSimpleName()); ioc.put(beanName, clazz.newInstance()); &#125;else if(clazz.isAnnotationPresent(SWService.class))&#123; SWService swService = clazz.getAnnotation(SWService.class); String beanName = swService.value(); if(!"".equals(beanName.trim()))&#123; ioc.put(beanName, clazz.newInstance()); continue; &#125; //获取对象所实现的所有接口 Class&lt;?&gt;[] interfaces = clazz.getInterfaces(); for(Class&lt;?&gt; i : interfaces)&#123; ioc.put(i.getName(), clazz.newInstance()); &#125; &#125; &#125; catch (ClassNotFoundException | IllegalAccessException | InstantiationException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private void doAutowired() &#123; if(ioc.isEmpty())&#123; return; &#125; for (Map.Entry&lt;String, Object&gt; entry : ioc.entrySet())&#123; //在spring中没有隐私 //咱们只认 @Autowried，获取所有属性 Field[] fields = entry.getValue().getClass().getDeclaredFields(); for(Field field : fields)&#123; if(!field.isAnnotationPresent(SWAutowried.class))&#123; continue; &#125; SWAutowried swAutowried = field.getAnnotation(SWAutowried.class); String beanName = swAutowried.value().trim(); //如果为空说明使用默认名字，所以使用getName获取默认名字 if("".equals(beanName))&#123; beanName = field.getType().getName(); &#125; field.setAccessible(true); try &#123; //向属性set进已经实例化的对象 field.set(entry.getValue(), ioc.get(beanName)); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; private void initHandlerMapping() &#123; if(!ioc.isEmpty())&#123; for(Map.Entry&lt;String, Object&gt; entry : ioc.entrySet())&#123; Class&lt;?&gt; clazz = entry.getValue().getClass(); //HandlerMapping只认识SWController if(!clazz.isAnnotationPresent(SWController.class))&#123; continue; &#125; String url = ""; //获取类上的RequestMapping地址 if(clazz.isAnnotationPresent(SWRequestMapping.class))&#123; SWRequestMapping swRequestMapping = clazz.getAnnotation(SWRequestMapping.class); url = swRequestMapping.value(); &#125; Method[] methods = clazz.getMethods(); for(Method method : methods)&#123; if(!method.isAnnotationPresent(SWRequestMapping.class))&#123; continue; &#125; //获取实际方法上的requestMapping SWRequestMapping swRequestMapping = method.getAnnotation(SWRequestMapping.class); String mUrl = url + swRequestMapping.value(); handlerMapping.put(mUrl, method); System.out.println("Mapping : " + mUrl + " " + method); &#125; &#125; &#125; &#125; private void doDispatch(HttpServletRequest request, HttpServletResponse response) &#123; String url = request.getRequestURI(); String contextPath = request.getContextPath(); url = url.replace(contextPath, "").replaceAll("/+", "/"); if(!handlerMapping.containsKey(url))&#123; try &#123; response.getWriter().write("404"); return; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //获取当前路由对应的方法 Method method = handlerMapping.get(url); System.out.println("获得对应的方法" + method); //获取方法列表 Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); Map&lt;String, String[]&gt; parameterMap = request.getParameterMap(); Object[] paramValues = new Object[parameterTypes.length]; for(int i = 0; i &lt; parameterTypes.length; i++)&#123; Class parameterType = parameterTypes[i]; if(parameterType == HttpServletRequest.class)&#123; paramValues[i] = request; continue; &#125;else if(parameterType == HttpServletResponse.class)&#123; paramValues[i] = response; &#125;else if(parameterType == String.class)&#123; for(Map.Entry&lt;String, String[]&gt; entry : parameterMap.entrySet())&#123; String value = Arrays.toString(entry.getValue()).replaceAll("\\[|\\]","") .replaceAll(",\\s", ","); paramValues[i++] = value; if(i == parameterTypes.length)&#123; break; &#125; &#125; &#125; &#125; try&#123; String beanName = toLowStr(method.getDeclaringClass().getSimpleName()); method.invoke(this.ioc.get(beanName), paramValues); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; private String toLowStr(String str)&#123; char[] ch = str.toCharArray(); ch[0] += 32; return String.valueOf(ch); &#125;&#125; 此时我们就可以在浏览器中访问对应的网页了，至此一个简易的Spring框架就完成了]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成POI]]></title>
    <url>%2F2019%2F08%2F08%2FSpringboot%E9%9B%86%E6%88%90POI%2F</url>
    <content type="text"><![CDATA[本文比较简单。。。希望想真正使用的同学。。去看别人整理的吧。。。我怕这次整理比较乱。。影响大家阅读 什么是POIApache POI是Apache软件基金会的开放源码函式库，POI提供API给Java程序对Microsoft Office格式档案读和写的功能。 添加依赖12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt;&lt;/dependency&gt; 代码实现 因为为了方便所以本代码。。。就是基本上没有修改的项目代码。。。所以又看不懂的话。。多担待 -_-!!!1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public String exportExcel(List&lt;Object&gt; data, Class&lt;?&gt; clazz) &#123; OutputStream outputStream = null; File[] roots = File.listRoots(); String path = roots[0].getPath(); try &#123; Field[] fields = data.getClass().getDeclaredFields(); //实例化HSSFWorkbook HSSFWorkbook workbook = new HSSFWorkbook(); //创建一个Excel表单，参数为sheet的名字 HSSFSheet sheet = workbook.createSheet("sheet"); //设置表头 setTitle(workbook, sheet, clazz); //设置单元格并赋值 setData(sheet, data, clazz); &#125; catch (Exception e) &#123; logger.info("ExcelUtil.exportExcel : 导出解析失败!"); e.printStackTrace(); &#125; &#125; private void setTitle(HSSFWorkbook workbook, HSSFSheet sheet, Class&lt;?&gt; clazz) &#123; ClassField classField = new ClassField(); List&lt;String&gt; list = classField.map.get(clazz); logger.info("ExcelUtil.setTitle : 这是map信息：" + classField.map); logger.info("ExcelUtil.setTitle : 这是头信息：" + list); try &#123; HSSFRow row = sheet.createRow(0); //设置列宽，setColumnWidth的第二个参数要乘以256，这个参数的单位是1/256个字符宽度 for (int i = 0; i &lt; list.size(); i++) &#123; sheet.setColumnWidth(i, 15 * 256); &#125; //设置为居中加粗,格式化时间格式 HSSFCellStyle style = workbook.createCellStyle(); HSSFFont font = workbook.createFont(); font.setBold(true); style.setFont(font); style.setDataFormat(HSSFDataFormat.getBuiltinFormat("m/d/yy h:mm")); //创建表头名称 HSSFCell cell; for (int j = 0; j &lt; list.size(); j++) &#123; cell = row.createCell(j); cell.setCellValue(list.get(j)); cell.setCellStyle(style); &#125; &#125; catch (Exception e) &#123; logger.info("ExcelUtil.setTitle : 导出时设置表头失败！"); e.printStackTrace(); &#125; &#125; private static void setData(HSSFSheet sheet, List&lt;Object&gt; data, Class&lt;?&gt; clazz) &#123; try &#123; int rowNum = 1; Field[] fields = clazz.getDeclaredFields(); for (int i = 0; i &lt; data.size(); i++) &#123; HSSFRow row = sheet.createRow(rowNum); for (int j = 0; j &lt; fields.length; j++) &#123; // 对于每个属性，获取属性名 String varName = fields[j].getName(); try &#123; // 获取原来的访问控制权限 boolean accessFlag = fields[j].isAccessible(); // 修改访问控制权限 fields[j].setAccessible(true); // 获取在对象f中属性fields[i]对应的对象中的变量 Object o; try &#123; o = fields[j].get(data.get(i)); row.createCell(j).setCellValue(String.valueOf(o)); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 恢复访问控制权限 fields[i].setAccessible(accessFlag); &#125; catch (IllegalArgumentException ex) &#123; ex.printStackTrace(); &#125; &#125; rowNum++; &#125; logger.info("ExcelUtil.setTitle : 表格赋值成功！"); &#125; catch (Exception e) &#123; logger.info("ExcelUtil.setTitle : 表格赋值失败！"); e.printStackTrace(); &#125; &#125; 上面的代码中ClassField.java代码12345678910111213141516171819202122232425262728public class ClassField &#123; public Map&lt;Class&lt;?&gt;, List&gt; map = new HashMap&lt;&gt;(); public ClassField() &#123; List&lt;String&gt; gradingLoglist = new ArrayList&lt;&gt;(); gradingLoglist.add("ID"); gradingLoglist.add("问卷ID"); gradingLoglist.add("问卷时间"); gradingLoglist.add("问卷简介"); gradingLoglist.add("被评人ID"); gradingLoglist.add("被评人姓名"); gradingLoglist.add("分数"); map.put(GradingLogPo.class, gradingLoglist); List&lt;String&gt; submitInfoList = new ArrayList&lt;&gt;(); submitInfoList.add("ID"); submitInfoList.add("评分人ID"); submitInfoList.add("评分人姓名"); submitInfoList.add("被评人ID"); submitInfoList.add("被评人姓名"); submitInfoList.add("问卷ID"); submitInfoList.add("问卷简介"); submitInfoList.add("评分规则ID"); submitInfoList.add("分数"); submitInfoList.add("备注"); map.put(SubmitInformationPo.class, submitInfoList); &#125;&#125;]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>POI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java遍历一个类的所有属性和值]]></title>
    <url>%2F2019%2F08%2F08%2FJava%E9%81%8D%E5%8E%86%E4%B8%80%E4%B8%AA%E7%B1%BB%E7%9A%84%E6%89%80%E6%9C%89%E5%B1%9E%E6%80%A7%E5%92%8C%E5%80%BC%2F</url>
    <content type="text"><![CDATA[今天在尝试使用Java反射来实现Java导出数据库数据到Excel中，然后在尝试获取类所有的属性的时候，由于对于反射的不熟练，然后找到了一篇博文，所以在此进行记录，以供未来参考，原博文地址:https://blog.csdn.net/ztx114/article/details/78274314 1234567891011121314151617181920212223242526private void bianLi(Object obj)&#123; Field[] fields = obj.getClass().getDeclaredFields(); for(int i = 0 , len = fields.length; i &lt; len; i++) &#123; // 对于每个属性，获取属性名 String varName = fields[i].getName(); try &#123; // 获取原来的访问控制权限 boolean accessFlag = fields[i].isAccessible(); // 修改访问控制权限 fields[i].setAccessible(true); // 获取在对象f中属性fields[i]对应的对象中的变量 Object o; try &#123; o = fields[i].get(obj); System.err.println("传入的对象中包含一个如下的变量：" + varName + " = " + o); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; // 恢复访问控制权限 fields[i].setAccessible(accessFlag); &#125; catch (IllegalArgumentException ex) &#123; ex.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ事务,confirm机制以及return机制]]></title>
    <url>%2F2019%2F08%2F07%2FRabbitMQ%E4%BA%8B%E5%8A%A1%2Cconfirm%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8Areturn%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文是参考下面这篇博客然后进行复现浓缩后的总结https://www.cnblogs.com/vipstone/p/9350075.html 正常情况下，如果消息经过交换器进入队列就可以完成消息的持久化，但如果消息在没有到达broker之前出现意外，那就造成消息丢失，有没有办法可以解决这个问题？RabbitMQ有两种方式来解决这个问题： 通过AMQP提供的事务机制实现； 使用发送者确认模式实现； 事务使用事物的实现主要是对于信道（Channel）的设置，其中主要的方法有三个： channel.txSelect()声明启动事务模式； channel.txComment()提交事务； channel.txRollback()回滚事务； 尝试代码实现：1234567891011121314151617181920212223public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setUsername("guest"); factory.setPassword("guest"); factory.setHost("localhost"); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 队列名， 持久化， 是否排外， 非自动删除 channel.queueDeclare("queueName",true, false, false, null); String message = "这是一个测试消息"; try &#123; channel.txSelect(); // String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish("exchangeName", "queueName", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); channel.txCommit(); &#125;catch (Exception e)&#123; channel.txRollback(); &#125;finally &#123; channel.close(); connection.close(); &#125;&#125; 客户端与rabbitMQ的交互流程如下： 客户端发送给服务器Tx.Select(开启事务模式) 服务器端返回Tx.Select-Ok（开启事务模式ok） 推送消息 客户端发送给事务提交Tx.Commit 服务器端返回Tx.Commit-Ok 消费者模式使用事务假设消费者模式中使用了事务，并且在消息确认之后进行了事务回滚，那么RabbitMQ会产生什么样的变化？结果分为两种情况： autoAck=false手动确认的时候是支持事务的，也就是说即使你已经手动确认了消息，但客户端也会在确认事务返回消息之后，再做决定是确认消息还是重新放回队列，如果你手动确认之后，又回滚了事务，那么以事务回滚为主，此条消息会重新放回队列； autoAck=true如果确认为true的情况是不支持事务的，也就是说你即使在收到消息之后在回滚事务也是于事无补的，队列已经把消息移除了； 二、Confirm发送方确认模式Confirm发送方确认模式使用和事务类似，也是通过设置Channel进行发送方确认的。Confirm的三种实现方式：方式一：channel.waitForConfirms()普通发送方确认模式；方式二：channel.waitForConfirmsOrDie()批量确认模式；方式三：channel.addConfirmListener()异步监听发送方确认模式； 1.普通发送方确认1234567891011121314151617public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setUsername("guest"); factory.setPassword("guest"); factory.setHost("localhost"); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 队列名， 持久化， 是否排外， 非自动删除 channel.queueDeclare("queueName",true, false, false, null); String message = "这是一个测试消息"; // String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish("exchangeName", "queueName", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); if(channel.waitForConfirms())&#123; System.out.println("消息发送成功"); &#125;&#125; 2.批量确认模式123456789101112131415161718public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setUsername("guest"); factory.setPassword("guest"); factory.setHost("localhost"); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 队列名， 持久化， 是否排外， 非自动删除 channel.queueDeclare("queueName",true, false, false, null); channel.confirmSelect(); for(int i = 0; i &lt; 10; i++)&#123; String message = "这是一个测试消息" + i; // String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish("exchangeName", "queueName", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); &#125; channel.waitForConfirmsOrDie();//对所有消息进行等待，只要有一个未发送就会返回异常&#125; 3.异步监听发送方确认模式12345678910111213141516171819202122232425262728public void test2() throws IOException, TimeoutException, InterruptedException &#123; ConnectionFactory factory = new ConnectionFactory(); factory.setUsername("guest"); factory.setPassword("guest"); factory.setHost("localhost"); factory.setPort(5672); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // 队列名， 持久化， 是否排外， 非自动删除 channel.queueDeclare("queueName",true, false, false, null); channel.confirmSelect(); for(int i = 0; i &lt; 10; i++)&#123; String message = "这是一个测试消息" + i; // String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish("exchangeName", "queueName", MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); &#125; channel.addConfirmListener(new ConfirmListener() &#123; @Override public void handleAck(long l, boolean b) throws IOException &#123; System.out.println("消息发送失败，标识：" + l); &#125; @Override public void handleNack(long l, boolean b) throws IOException &#123; System.out.println(String.format("消息发送成功，标识：%d， 是否是多个同时确认：%b", l, b)); &#125; &#125;);&#125; 可以看出，代码是异步执行的，消息确认有可能是批量确认的，是否批量确认在于返回的multiple的参数，此参数为bool值，如果true表示批量执行了deliveryTag这个值以前的所有消息，如果为false的话表示单条确认。 二、Return消息机制Return Listener用于处理一些不可路由的消息!我们的消息生产者，通过指定一个Exchange 和Routingkey,把消息送达到某一个队列中去，然后我们的消费者监听队列，进行消费处理操作! 在基础API中有一个关键的配置项:Mandatory:如果为true，则监听器会接收到路由不可达的消息，然后进行后续处理，如果为false,那么broker端自动删除该消息! 使用方式是在channel中添加一个ReturnListener 1234567channel.addReturnListener (new ReturnListener () &#123; @Override public void handleReturn (int replyCode, string replyText, String exchange, String routingKey, AMQP.BasicProperties properties,byte[] body) throws IOException &#123; // 这里是业务代码 &#125;&#125;); 总结：Confirm批量确定和Confirm异步模式性能相差不大，Confirm模式要比事务快10倍左右。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
        <tag>RabbitMQ</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis与LRU]]></title>
    <url>%2F2019%2F08%2F05%2FRedis%E4%B8%8ELRU%2F</url>
    <content type="text"><![CDATA[参考文章https://zhuanlan.zhihu.com/p/34133067https://blog.csdn.net/azurelaker/article/details/85045245https://www.bilibili.com/video/av45625512 最近在使用redis，然后看到了redis与LRU相关的一些内容，再此对之前所看的一些博文进行总结 一、什么是LRULRU是Least Recently Used的缩写，即最近最少使用，是一种常用的页面置换算法，选择最近最久未使用的页面予以淘汰。该算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间t，当须淘汰一个页面时，选择现有页面中其t值最大的，即最近最少使用的页面予以淘汰。 二、如何实现LRU如果按照访问时间进行了排序，会有大量的内存拷贝操作，所以性能肯定是不能接受的。那么如何设计一个LRU缓存，使得放入和移除都是O(1)的，我们需要把访问次序维护起来，但是不能通过内存中的真实排序来反应，有一种方案就是使用双向链表。 三、基于HashMap和双向链表实现LRU在这个双向链表中存在一个HashMap用于存储实现LRU的双向链表的节点，如图所示 而双向链表则存在一个head和一个tail分别指代双向链表的头与尾。假设我们预设一个大小为3的cache，当我们执行以下操作时，12345678save("key1", 7)save("key2", 0)save("key3", 1)save("key4", 2)get("key2")save("key5", 3)get("key2")save("key6", 4) 双向链表变化如图所示： 四、LRU的Java实现1234567//节点数据结构class DLinkedNode&#123; int key; int value; DLinkedNode pre; DLinkedNode next;&#125; 实际操作代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class LRUache&#123; private HashMap&lt;Integer, DLinkedNode&gt; cache = new HashMap&lt;&gt;(); //目前双向链表中的节点数 private int count; //最大容量 private int capacity; //双向链表头尾节点 private DLinkedNode head, tail; public LRUache(int capacity)&#123; this.count = 0; this.capacity = capacity; head = new DLinkedNode(); head.pre = null; tail = new DLinkedNode(); tail.next = null; head.next = tail; tail.pre = head; &#125; //将节点挪至头部 private void moveToHead(DLinkedNode node)&#123; //清除当前节点，即将当前节点的pre节点和next节点相连 removeNode(node); //根据LRU算法，将操作的节点放至首位置 addNode(node); &#125; //将该节点挪除 private void removeNode(DLinkedNode node)&#123; DLinkedNode pre = node.pre; DLinkedNode next = node.next; pre.next = next; next.pre = pre; &#125; //在头部添加节点 private void addNode(DLinkedNode node)&#123; node.pre = head; node.next = head.next; head.next.pre = node; head.next = node; &#125; //清除超出最大容量后的最后一个节点 private DLinkedNode popTail()&#123; DLinkedNode node = tail.pre; removeNode(node); return node; &#125; //获取节点并更新LRU public int get(int key)&#123; DLinkedNode node = cache.get(key); if(node == null)&#123; return -1; &#125; moveToHead(node); return node.val; &#125; //压入新节点 public void put(int key, int value)&#123; DLinkedNode node = cache.get(key); if(node == null)&#123; DLinkedNode newNode = new DLinkedNode(); newNode.key = key; newNode.value = value; cache.put(key, value); addNode(newNode); count++; if(count &gt; capacity)&#123; DLinkedNode tail = popTail(); cache.remove(tail.key); count--; &#125; &#125;else&#123; node.value = value; moveToHead(node); &#125; &#125;&#125; 五、Redis的LRU实现Redis系统中与LRU功能相关的配置参数有三个: maxmemory : 该参数即为缓存数据占用的内存限制. 当缓存的数据消耗的内存超过这个数值限制时, 将触发数据淘汰. 该数据配置为0时,表示缓存的数据量没有限制, 即LRU功能不生效. maxmemory_policy : 淘汰策略. 定义参与淘汰的数据的类型和属性. maxmemory_samples : 随机采样的精度. 该数值配置越大, 越接近于真实的LRU算法,但是数值越大, 消耗的CPU计算时间越多,执行效率越低. 我们知道在Redis缓存中可以有超时属性所以Redis在每个数据库结构中使用了两个不同的哈希表来管理缓存数据. 数据结构如下:12345678910//redis.htypedef struct redisDb &#123; dict *dict; /* The keyspace for this DB */ dict *expires; /* Timeout of keys with a timeout set */ dict *blocking_keys; /* Keys with clients waiting for data (BLPOP) */ dict *ready_keys; /* Blocked keys that received a PUSH */ dict *watched_keys; /* WATCHED keys for MULTI/EXEC CAS */ int id; long long avg_ttl; /* Average TTL, just for stats */&#125; redisDb; 由助手可知 expires存储含有超时属性的数据，而dict则可以存储所有的数据。 Redis一共提供了六种淘汰策略,即参数maxmemory_policy有六种取值: noeviction: 如果缓存数据超过了maxmemory限定值,并且客户端正在执行的命令会导致内存分配,则向客户端返回错误响应. allkeys-lru: 所有的缓存数据(包括没有超时属性的和具有超时属性的)都参与LRU算法淘汰. volatile-lru: 只有超时属性的缓存数据才参与LRU算法淘汰. allkeys-random: 所有的缓存数据(包括没有超时属性的和具有超时属性的)都参与淘汰, 但是采用随机淘汰,而不是用LRU算法进行淘汰. volatile-random: 只有超时属性的缓存数据才参与淘汰,但是采用随机淘汰,而不是用LRU算法进行淘汰. volatile-ttl: 只有超时属性的缓存数据才参与淘汰. 根据缓存数据的超时TTL进行淘汰,而不是用LRU算法进行淘汰. 注: volatile-lru,volatile-random和volatile-ttl这三种淘汰策略不是使用的全量数据，所以可能会导致无法淘汰出足够的内存空间。而且当设置超时属性时属性会占用更大的内存，所以当内存压力比较大时要慎用超时属性。 redis处理流程1.客户端向redis发送消息，redis对命令进行解析，为命令分配内存。2.判断内存是否超出限定值，即maxmemory，如果超过，则按照所选定的淘汰算法，进行内存释放。3.当指令为读指令时忽略淘汰算法，当为写指令，且超出限定值进行内存释放，若内存释放失败则向客户端返回错误响应，如释放成功则执行写指令。 redis源码解析 Redis处理命令的入口函数processCommand123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158//redis.cint processCommand(redisClient *c) &#123; /* The QUIT command is handled separately. Normal command procs will * go through checking for replication and QUIT will cause trouble * when FORCE_REPLICATION is enabled and would be implemented in * a regular command proc. */ if (!strcasecmp(c-&gt;argv[0]-&gt;ptr,"quit")) &#123; addReply(c,shared.ok); c-&gt;flags |= REDIS_CLOSE_AFTER_REPLY; return REDIS_ERR; &#125; /* Now lookup the command and check ASAP about trivial error conditions * such as wrong arity, bad command name and so forth. */ c-&gt;cmd = c-&gt;lastcmd = lookupCommand(c-&gt;argv[0]-&gt;ptr); if (!c-&gt;cmd) &#123; flagTransaction(c); addReplyErrorFormat(c,"unknown command '%s'", (char*)c-&gt;argv[0]-&gt;ptr); return REDIS_OK; &#125; else if ((c-&gt;cmd-&gt;arity &gt; 0 &amp;&amp; c-&gt;cmd-&gt;arity != c-&gt;argc) || (c-&gt;argc &lt; -c-&gt;cmd-&gt;arity)) &#123; flagTransaction(c); addReplyErrorFormat(c,"wrong number of arguments for '%s' command", c-&gt;cmd-&gt;name); return REDIS_OK; &#125; /* Check if the user is authenticated */ if (server.requirepass &amp;&amp; !c-&gt;authenticated &amp;&amp; c-&gt;cmd-&gt;proc != authCommand) &#123; flagTransaction(c); addReply(c,shared.noautherr); return REDIS_OK; &#125; /* Handle the maxmemory directive. * * First we try to free some memory if possible (if there are volatile * keys in the dataset). If there are not the only thing we can do * is returning an error. */ if (server.maxmemory) &#123; int retval = freeMemoryIfNeeded(); /* freeMemoryIfNeeded may flush slave output buffers. This may result * into a slave, that may be the active client, to be freed. */ if (server.current_client == NULL) return REDIS_ERR; /* It was impossible to free enough memory, and the command the client * is trying to execute is denied during OOM conditions? Error. */ if ((c-&gt;cmd-&gt;flags &amp; REDIS_CMD_DENYOOM) &amp;&amp; retval == REDIS_ERR) &#123; flagTransaction(c); addReply(c, shared.oomerr); return REDIS_OK; &#125; &#125; /* Don't accept write commands if there are problems persisting on disk * and if this is a master instance. */ if (((server.stop_writes_on_bgsave_err &amp;&amp; server.saveparamslen &gt; 0 &amp;&amp; server.lastbgsave_status == REDIS_ERR) || server.aof_last_write_status == REDIS_ERR) &amp;&amp; server.masterhost == NULL &amp;&amp; (c-&gt;cmd-&gt;flags &amp; REDIS_CMD_WRITE || c-&gt;cmd-&gt;proc == pingCommand)) &#123; flagTransaction(c); if (server.aof_last_write_status == REDIS_OK) addReply(c, shared.bgsaveerr); else addReplySds(c, sdscatprintf(sdsempty(), "-MISCONF Errors writing to the AOF file: %s\r\n", strerror(server.aof_last_write_errno))); return REDIS_OK; &#125; /* Don't accept write commands if there are not enough good slaves and * user configured the min-slaves-to-write option. */ if (server.masterhost == NULL &amp;&amp; server.repl_min_slaves_to_write &amp;&amp; server.repl_min_slaves_max_lag &amp;&amp; c-&gt;cmd-&gt;flags &amp; REDIS_CMD_WRITE &amp;&amp; server.repl_good_slaves_count &lt; server.repl_min_slaves_to_write) &#123; flagTransaction(c); addReply(c, shared.noreplicaserr); return REDIS_OK; &#125; /* Don't accept write commands if this is a read only slave. But * accept write commands if this is our master. */ if (server.masterhost &amp;&amp; server.repl_slave_ro &amp;&amp; !(c-&gt;flags &amp; REDIS_MASTER) &amp;&amp; c-&gt;cmd-&gt;flags &amp; REDIS_CMD_WRITE) &#123; addReply(c, shared.roslaveerr); return REDIS_OK; &#125; /* Only allow SUBSCRIBE and UNSUBSCRIBE in the context of Pub/Sub */ if (c-&gt;flags &amp; REDIS_PUBSUB &amp;&amp; c-&gt;cmd-&gt;proc != pingCommand &amp;&amp; c-&gt;cmd-&gt;proc != subscribeCommand &amp;&amp; c-&gt;cmd-&gt;proc != unsubscribeCommand &amp;&amp; c-&gt;cmd-&gt;proc != psubscribeCommand &amp;&amp; c-&gt;cmd-&gt;proc != punsubscribeCommand) &#123; addReplyError(c,"only (P)SUBSCRIBE / (P)UNSUBSCRIBE / QUIT allowed in this context"); return REDIS_OK; &#125; /* Only allow INFO and SLAVEOF when slave-serve-stale-data is no and * we are a slave with a broken link with master. */ if (server.masterhost &amp;&amp; server.repl_state != REDIS_REPL_CONNECTED &amp;&amp; server.repl_serve_stale_data == 0 &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; REDIS_CMD_STALE)) &#123; flagTransaction(c); addReply(c, shared.masterdownerr); return REDIS_OK; &#125; /* Loading DB? Return an error if the command has not the * REDIS_CMD_LOADING flag. */ if (server.loading &amp;&amp; !(c-&gt;cmd-&gt;flags &amp; REDIS_CMD_LOADING)) &#123; addReply(c, shared.loadingerr); return REDIS_OK; &#125; /* Lua script too slow? Only allow a limited number of commands. */ if (server.lua_timedout &amp;&amp; c-&gt;cmd-&gt;proc != authCommand &amp;&amp; c-&gt;cmd-&gt;proc != replconfCommand &amp;&amp; !(c-&gt;cmd-&gt;proc == shutdownCommand &amp;&amp; c-&gt;argc == 2 &amp;&amp; tolower(((char*)c-&gt;argv[1]-&gt;ptr)[0]) == 'n') &amp;&amp; !(c-&gt;cmd-&gt;proc == scriptCommand &amp;&amp; c-&gt;argc == 2 &amp;&amp; tolower(((char*)c-&gt;argv[1]-&gt;ptr)[0]) == 'k')) &#123; flagTransaction(c); addReply(c, shared.slowscripterr); return REDIS_OK; &#125; /* Exec the command */ if (c-&gt;flags &amp; REDIS_MULTI &amp;&amp; c-&gt;cmd-&gt;proc != execCommand &amp;&amp; c-&gt;cmd-&gt;proc != discardCommand &amp;&amp; c-&gt;cmd-&gt;proc != multiCommand &amp;&amp; c-&gt;cmd-&gt;proc != watchCommand) &#123; queueMultiCommand(c); addReply(c,shared.queued); &#125; else &#123; call(c,REDIS_CALL_FULL); if (listLength(server.ready_keys)) handleClientsBlockedOnLists(); &#125; return REDIS_OK;&#125; 当调用该函数时,Redis已经解析完命令以及参数,并分配了内存空间,客户端对象的argv字段指向这些分配的内存空间. LINE 40 - 53调用函数freeMemoryIfNeeded释放缓存的内存空间,如果freeMemoryIfNeeded返回失败,即无法释放足够的内存,并且客户端命令是导致内存增加的命令,则向客户端返回OOM错误消息响应. 函数freeMemoryIfNeeded淘汰缓存的数据,其实现为(redis.c):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150int freeMemoryIfNeeded(void) &#123; size_t mem_used, mem_tofree, mem_freed; int slaves = listLength(server.slaves); mstime_t latency; /* Remove the size of slaves output buffers and AOF buffer from the * count of used memory. */ mem_used = zmalloc_used_memory(); if (slaves) &#123; listIter li; listNode *ln; listRewind(server.slaves,&amp;li); while((ln = listNext(&amp;li))) &#123; redisClient *slave = listNodeValue(ln); unsigned long obuf_bytes = getClientOutputBufferMemoryUsage(slave); if (obuf_bytes &gt; mem_used) mem_used = 0; else mem_used -= obuf_bytes; &#125; &#125; if (server.aof_state != REDIS_AOF_OFF) &#123; mem_used -= sdslen(server.aof_buf); mem_used -= aofRewriteBufferSize(); &#125; /* Check if we are over the memory limit. */ if (mem_used &lt;= server.maxmemory) return REDIS_OK; if (server.maxmemory_policy == REDIS_MAXMEMORY_NO_EVICTION) return REDIS_ERR; /* We need to free memory, but policy forbids. */ /* Compute how much memory we need to free. */ mem_tofree = mem_used - server.maxmemory; mem_freed = 0; latencyStartMonitor(latency); while (mem_freed &lt; mem_tofree) &#123; int j, k, keys_freed = 0; for (j = 0; j &lt; server.dbnum; j++) &#123; long bestval = 0; /* just to prevent warning */ sds bestkey = NULL; struct dictEntry *de; redisDb *db = server.db+j; dict *dict; if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_RANDOM) &#123; dict = server.db[j].dict; &#125; else &#123; dict = server.db[j].expires; &#125; if (dictSize(dict) == 0) continue; /* volatile-random and allkeys-random policy */ if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_RANDOM || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_RANDOM) &#123; de = dictGetRandomKey(dict); bestkey = dictGetKey(de); &#125; /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; robj *o; de = dictGetRandomKey(dict); thiskey = dictGetKey(de); /* When policy is volatile-lru we need an additional lookup * to locate the real key, as dict is set to db-&gt;expires. */ if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) de = dictFind(db-&gt;dict, thiskey); o = dictGetVal(de); thisval = estimateObjectIdleTime(o); /* Higher idle time is better candidate for deletion */ if (bestkey == NULL || thisval &gt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125; &#125; /* volatile-ttl */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_TTL) &#123; for (k = 0; k &lt; server.maxmemory_samples; k++) &#123; sds thiskey; long thisval; de = dictGetRandomKey(dict); thiskey = dictGetKey(de); thisval = (long) dictGetVal(de); /* Expire sooner (minor expire unix timestamp) is better * candidate for deletion */ if (bestkey == NULL || thisval &lt; bestval) &#123; bestkey = thiskey; bestval = thisval; &#125; &#125; &#125; /* Finally remove the selected key. */ if (bestkey) &#123; long long delta; robj *keyobj = createStringObject(bestkey,sdslen(bestkey)); propagateExpire(db,keyobj); /* We compute the amount of memory freed by dbDelete() alone. * It is possible that actually the memory needed to propagate * the DEL in AOF and replication link is greater than the one * we are freeing removing the key, but we can't account for * that otherwise we would never exit the loop. * * AOF and Output buffer memory will be freed eventually so * we only care about memory used by the key space. */ delta = (long long) zmalloc_used_memory(); dbDelete(db,keyobj); delta -= (long long) zmalloc_used_memory(); mem_freed += delta; server.stat_evictedkeys++; notifyKeyspaceEvent(REDIS_NOTIFY_EVICTED, "evicted", keyobj, db-&gt;id); decrRefCount(keyobj); keys_freed++; /* When the memory to free starts to be big enough, we may * start spending so much time here that is impossible to * deliver data to the slaves fast enough, so we force the * transmission here inside the loop. */ if (slaves) flushSlavesOutputBuffers(); &#125; &#125; if (!keys_freed) &#123; latencyEndMonitor(latency); latencyAddSampleIfNeeded("eviction-cycle",latency); return REDIS_ERR; /* nothing to free... */ &#125; &#125; latencyEndMonitor(latency); latencyAddSampleIfNeeded("eviction-cycle",latency); return REDIS_OK;&#125; 执行if (mem_used &lt;= server.maxmemory) return REDIS_OK;如果当前缓存数据占用的总的内存小于配置的maxmemory,则不用淘汰,直接返回. 如果当前缓存的数据使用的内存大于配置的maxmemory,并且淘汰策略不允许释放内存(noeviction),则该函数返回失败. 接下来,局部变量mem_tofree表示需要淘汰的内存,局部变量mem_freed表示已经淘汰的内存.循环执行while (mem_freed &lt; mem_tofree)淘汰缓存数据,该循环中的逻辑可以概括为: 从全局的0号数据库开始(Redis默认有16个全局的数据库),根据淘汰策略,选择该数据库中的哈希表.如果该哈希表为空, 选择下一个全局数据库. 根据淘汰策略,在相应哈希表中找到一个待淘汰的key, 从该数据库对象中删除该key所对应的缓存数据. 如果没有找到待淘汰的key,即无法淘汰所需的缓存数据大小 函数直接返回错误. 如果当前访问的是最后一个全局数据库, 并且已经淘汰了所需的缓存数据,则该函数成功返回.如果没有淘汰所需的缓存数据,则返回步骤1,并且从0号数据库重新淘汰.如果当前访问的不是最后一个全局数据库, 则返回步骤1, 从当前数据库的下一个数据库继续淘汰缓存数据. 如果淘汰策略是allkeys-random或者volatile-random,则从相应数据库中随机选择一个key进行淘汰. 如果淘汰策略是allkeys-lru或者volatile-lru, 则根据配置的采样值maxmemory_samples,随机从数据库中选择maxmemory_samples个key, 淘汰其中热度最低的key对应的缓存数据. 如果淘汰策略是volatile-ttl,则根据配置的采样值maxmemory_samples,随机从数据库中选择maxmemory_samples个key,淘汰其中最先要超时的key对应的缓存数据. 所以采样参数maxmemory_samples配置的数值越大, 就越能精确的查找到待淘汰的缓存数据,但是也消耗更多的CPU计算,执行效率降低. 从数据库的哈希表结构中随机返回一个key的执行函数为dictGetRandomKey, 其实现为(dict.c): 1234567891011121314151617181920212223242526272829303132333435363738/* Return a random entry from the hash table. Useful to * implement randomized algorithms */dictEntry *dictGetRandomKey(dict *d)&#123; dictEntry *he, *orighe; unsigned int h; int listlen, listele; if (dictSize(d) == 0) return NULL; if (dictIsRehashing(d)) _dictRehashStep(d); if (dictIsRehashing(d)) &#123; do &#123; h = random() % (d-&gt;ht[0].size+d-&gt;ht[1].size); he = (h &gt;= d-&gt;ht[0].size) ? d-&gt;ht[1].table[h - d-&gt;ht[0].size] : d-&gt;ht[0].table[h]; &#125; while(he == NULL); &#125; else &#123; do &#123; h = random() &amp; d-&gt;ht[0].sizemask; he = d-&gt;ht[0].table[h]; &#125; while(he == NULL); &#125; /* Now we found a non empty bucket, but it is a linked * list and we need to get a random element from the list. * The only sane way to do so is counting the elements and * select a random index. */ listlen = 0; orighe = he; while(he) &#123; he = he-&gt;next; listlen++; &#125; listele = random() % listlen; he = orighe; while(listele--) he = he-&gt;next; return he;&#125; 上述代码主要执行了两件事情: 首先在哈希表中随机选择一个非空的桶(bucket). 在该桶的冲突链表中随机选择一个节点. 根据LRU淘汰算法的属性,如果缓存的数据被频繁访问, 其热度就高,反之,热度低. 下面说明缓存数据的热度相关的细节.Redis中的对象结构定义为(redis.h):1234567typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr;&#125; robj; 即对象结构中存在一个lru字段, 且使用了unsigned的低24位(REDIS_LRU_BITS定义的值). Redis命令访问缓存的数据时,均会调用函数lookupKey, 其实现为(db.c):123456789101112131415robj *lookupKey(redisDb *db, robj *key) &#123; dictEntry *de = dictFind(db-&gt;dict,key-&gt;ptr); if (de) &#123; robj *val = dictGetVal(de); /* Update the access time for the ageing algorithm. * Don't do it if we have a saving child, as this will trigger * a copy on write madness. */ if (server.rdb_child_pid == -1 &amp;&amp; server.aof_child_pid == -1) val-&gt;lru = server.lruclock; return val; &#125; else &#123; return NULL; &#125;&#125; 该函数会更新对象的lru值, 设置为全局的server.lruclock值.当然,在对象创建的时候也会将该lru字段设置为全局的server.lruclock. 全局的server.lruclock是在函数serverCron中调用函数updateLRUClock更新的(redis.c):1234void updateLRUClock(void) &#123; server.lruclock = (server.unixtime/REDIS_LRU_CLOCK_RESOLUTION) &amp; REDIS_LRU_CLOCK_MAX;&#125; 而全局的server.unixtime是在函数serverCron中调用函数updateCachedTime更新的(redis.c):12345678/* We take a cached value of the unix time in the global state because with * virtual memory and aging there is to store the current time in objects at * every object access, and accuracy is not needed. To access a global var is * a lot faster than calling time(NULL) */void updateCachedTime(void) &#123; server.unixtime = time(NULL); server.mstime = mstime();&#125; 函数serverCron是定时器执行函数, 会周期性执行.Redis系统中全局变量server.hz设置为10, 则serverCron的调度周期为100毫秒.也就是说,全局变量server.lruclock会每隔100毫秒得到更新,该字段也和对象结构的lru字段一样,也是使用了unsigned的低24位. 所以函数lookupKey中更新缓存数据的lru热度值时,不是调用的系统函数获得的当前时间戳,而是该值的一个近似值server.lruclock, 这样不用每次调用系统函数,可以提高执行效率. 函数estimateObjectIdleTime评估指定对象的lru热度,其实现为(object.c): 12345678910/* Given an object returns the min number of seconds the object was never * requested, using an approximated LRU algorithm. */unsigned long estimateObjectIdleTime(robj *o) &#123; if (server.lruclock &gt;= o-&gt;lru) &#123; return (server.lruclock - o-&gt;lru) * REDIS_LRU_CLOCK_RESOLUTION; &#125; else &#123; return ((REDIS_LRU_CLOCK_MAX - o-&gt;lru) + server.lruclock) * REDIS_LRU_CLOCK_RESOLUTION; &#125;&#125; 其思想就是对象的lru热度值和全局的server.lruclock的差值越大, 该对象热度越低.但是,因为全局的server.lruclock数值有可能发生溢出(超过REDIS_LRU_CLOCK_MAX则溢出), 所以对象的lru数值可能大于server.lruclock数值. 所以计算二者的差值时,需考虑二者间的大小关系. Redis系统没有使用一个全局的链表将所有的缓存数据管理起来,而是使用一种近似的算法来模拟LRU淘汰的效果: 首先可以节省内存占用.如果用全局的双向链表管理所有的缓存数据,则每个节点的两个指针字段将增加16字节(64位系统上). Redis系统中不同对象实现的可能是不同的结构,有的是比较复杂的复合结构. 如果再引入一个全局的链表,将增加代码复杂性,可读性也变差. 六、常见缓存算法总结1.LRULRU全称是Least Recently Used，即最近最久未使用的意思。如果一个数据在最近一段时间没有被访问到，那么在将来它被访问的可能性也很小。也就是说，当限定的空间已存满数据时，应当把最久没有被访问到的数据淘汰。而用什么数据结构来实现LRU算法呢？可能大多数人都会想到：用一个数组来存储数据，给每一个数据项标记一个访问时间戳，每次插入新数据项的时候，先把数组中存在的数据项的时间戳自增，并将新数据项的时间戳置为0并插入到数组中。每次访问数组中的数据项的时候，将被访问的数据项的时间戳置为0。当数组空间已满时，将时间戳最大的数据项淘汰。这种实现思路很简单，但是有什么缺陷呢？需要不停地维护数据项的访问时间戳，另外，在插入数据、删除数据以及访问数据时，时间复杂度都是O(n)。那么有没有更好的实现办法呢？那就是利用链表移动访问时间的数据顺序和hashmap查询是否是新数据项。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。 2.LFULFU（Least Frequently Used）最近最少使用算法。它是基于“如果一个数据在最近一段时间内使用次数很少，那么在将来一段时间内被使用的可能性也很小”的思路。注意LFU和LRU算法的不同之处，LRU的淘汰规则是基于访问时间，而LFU是基于访问次数的。举个简单的例子：假设缓存大小为3，数据访问序列为set(2,2),set(1,1),get(2),get(1),get(2),set(3,3),set(4,4)，则在set(4,4)时对于LFU算法应该淘汰(3,3)，而LRU应该淘汰(1,1)。为了能够淘汰最少使用的数据，因此LFU算法最简单的一种设计思路就是 利用一个数组存储 数据项，用hashmap存储每个数据项在数组中对应的位置，然后为每个数据项设计一个访问频次，当数据项被命中时，访问频次自增，在淘汰的时候淘汰访问频次最少的数据。这样一来的话，在插入数据和访问数据的时候都能达到O(1)的时间复杂度，在淘汰数据的时候，通过选择算法得到应该淘汰的数据项在数组中的索引，并将该索引位置的内容替换为新来的数据内容即可，这样的话，淘汰数据的操作时间复杂度为O(n)。 3.FIFOFIFO（First in First out），先进先出。其实在操作系统的设计理念中很多地方都利用到了先进先出的思想，比如作业调度（先来先服务），为什么这个原则在很多地方都会用到呢？因为这个原则简单、且符合人们的惯性思维，具备公平性，并且实现起来简单，直接使用数据结构中的队列即可实现。在FIFO Cache设计中，核心原则就是：如果一个数据最先进入缓存中，则应该最早淘汰掉。那么利用什么数据结构来实现呢？下面提供一种实现思路：利用一个双向链表保存数据，当来了新的数据之后便添加到链表末尾，如果Cache存满数据，则把链表头部数据删除，然后把新的数据添加到链表末尾。在访问数据的时候，如果在Cache中存在该数据的话，则返回对应的value值；否则返回-1。如果想提高访问效率，可以利用hashmap来保存每个key在链表中对应的位置。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成RabbitMQ]]></title>
    <url>%2F2019%2F08%2F03%2FSpringboot%E9%9B%86%E6%88%90RabbitMQ%2F</url>
    <content type="text"><![CDATA[一、什么是RabbitMQRabbitMQ是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ服务器是用Erlang语言编写的，而集群和故障转移是构建在开放电信平台框架上的。所有主要的编程语言均有与代理接口通讯的客户端库。 二、添加依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 三、添加配置文件创建消息队列123456789101112@Configurationpublic class MessageConfig &#123; public final static String EMAIL_QUEUE_NAME = "emailQueue"; @Bean public Queue emailQueue() &#123; //队列名，是否持久化 return new Queue(EMAIL_QUEUE_NAME, true); &#125; &#125; 四、创建消息队列发送者 下面的生产者与消费者是我用来一步发送邮件的123456789101112131415@Servicepublic class MessageSender &#123; public Logger logger = LoggerFactory.getLogger(getClass()); @Autowired AmqpTemplate amqpTemplate; public void sendTopicEmail(Email emailEntity) &#123; String email = ConvertUtil.beanToString(emailEntity); logger.debug(" [RabbitMQ] MessageSender.sendTopicEmail 向 &#123;&#125; 队列发送邮件消息 -&gt; : &#123;&#125;", EMAIL_QUEUE_NAME, emailEntity); amqpTemplate.convertAndSend(EMAIL_QUEUE_NAME, email); &#125;&#125; 五、创建消队列息监听者12345678910111213141516@Servicepublic class MessageReceiver &#123; public Logger logger = LoggerFactory.getLogger(getClass()); @Autowired EmailService emailService; @RabbitListener(queues = MessageConfig.EMAIL_QUEUE_NAME) public void receiveEmail1(String message) &#123; Email email = ConvertUtil.stringToBean(message, Email.class); logger.debug(" [RabbitMQ] MessageReceiver.receiveEmail 获取 &#123;&#125; 队列的邮件消息 -&gt; : &#123;&#125;", MessageConfig.EMAIL_QUEUE_NAME, email); emailService.sendEmail(email); &#125;&#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RabbitMQ</tag>
        <tag>Spring</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot集成Log4j2]]></title>
    <url>%2F2019%2F07%2F31%2Fspringboot%E9%9B%86%E6%88%90Log4j2%2F</url>
    <content type="text"><![CDATA[前言：关于Log4j2，SLF4J，LogBack以及其他的一些日志框架的区别，建议大家可以先了解一下 一、添加依赖12345678910111213141516&lt;!--SpringBoot默认日志框架为LogBack所以我们需要将LogBack依赖移除--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt;&lt;!-- 去掉默认配置 --&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt;&lt;/dependency&gt; 二、application.properties中添加配置12# log4j2 配置文件地址，而且log4j2-spring.xml作为默认的配置文件名，在resources目录下是就算不配置也会默认读logging.config=classpath:log4j2-spring.xml 三、创建相关配置文件 配置文件应该放在resources目录下，关于配置方面的详细内容，可以参考下面几篇博文以及项目demo https://www.jianshu.com/p/087a20d4aba8 https://www.jianshu.com/p/5dcf4ece0de3 https://www.jianshu.com/p/19628db2e7ef https://github.com/bycuimiao/springboot2-log4j2-demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!--设置log4j2的自身log级别为warn--&gt;&lt;!--日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!--Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置， 当设置成trace时，你会看到log4j2内部各种详细输出--&gt;&lt;!--monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数--&gt;&lt;configuration status="info" monitorInterval="30"&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--这个输出控制台的配置--&gt; &lt;console name="Console" target="SYSTEM_OUT"&gt; &lt;ThresholdFilter level="DEBUG" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;!--输出日志的格式--&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;/console&gt; &lt;RollingFile name="RollingFileDebug" fileName="$&#123;sys:user.home&#125;/logs/hpaasvc/debug.log" filePattern="$&#123;sys:user.home&#125;/logs/hpaasvc/$$&#123;date:yyyy-MM&#125;/debug-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;Filters&gt; &lt;!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level="DEBUG" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;ThresholdFilter level="INFO" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- 这个会打印出所有的info及以下级别的信息，每次大小超过size， 则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档--&gt; &lt;RollingFile name="RollingFileInfo" fileName="$&#123;sys:user.home&#125;/logs/hpaasvc/info.log" filePattern="$&#123;sys:user.home&#125;/logs/hpaasvc/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;Filters&gt; &lt;!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level="INFO" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;ThresholdFilter level="WARN" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileWarn" fileName="$&#123;sys:user.home&#125;/logs/hpaasvc/warn.log" filePattern="$&#123;sys:user.home&#125;/logs/hpaasvc/$$&#123;date:yyyy-MM&#125;/warn-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;Filters&gt; &lt;ThresholdFilter level="WARN" onMatch="ACCEPT" onMismatch="DENY"/&gt; &lt;ThresholdFilter level="ERROR" onMatch="DENY" onMismatch="NEUTRAL"/&gt; &lt;/Filters&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置了20 --&gt; &lt;DefaultRolloverStrategy max="20"/&gt; &lt;/RollingFile&gt; &lt;RollingFile name="RollingFileError" fileName="$&#123;sys:user.home&#125;/logs/hpaasvc/error.log" filePattern="$&#123;sys:user.home&#125;/logs/hpaasvc/$$&#123;date:yyyy-MM&#125;/error-%d&#123;yyyy-MM-dd&#125;-%i.log"&gt; &lt;ThresholdFilter level="ERROR"/&gt; &lt;PatternLayout pattern="[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n"/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size="100 MB"/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;loggers&gt; &lt;!--过滤掉spring和hibernate的一些无用的debug信息--&gt; &lt;logger name="org.springframework" level="INFO"&gt; &lt;/logger&gt; &lt;root level="all"&gt; &lt;appender-ref ref="Console"/&gt; &lt;appender-ref ref="RollingFileDebug"/&gt; &lt;appender-ref ref="RollingFileInfo"/&gt; &lt;appender-ref ref="RollingFileWarn"/&gt; &lt;appender-ref ref="RollingFileError"/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 使用Log4j2进行日志记录12345678910111213141516@RestController@RequestMapping("/users")public class UserController &#123; public final Logger logger= LoggerFactory.getLogger(getClass()); @Autowired UserService userService; @PostMapping("/login") public RestResult login(@RequestBody User loginUser) &#123; logger.debug("/users/login [Post] -&gt; loginUser : &#123;&#125;", loginUser); return userService.login(loginUser); &#125;&#125; 总结：在使用Log4j2后我个人感觉，他的优势和劣势都是使用xml进行配置的方便，为什么说这也是劣势呢，因为我看了好多才搞懂各个xml的参数含义等等，当然了，我觉得这不是Log4j2的劣势，是我的劣势 -_- 。]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Log4j2</tag>
        <tag>SLF4J</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot集成Redis]]></title>
    <url>%2F2019%2F07%2F25%2Fspringboot%E9%9B%86%E6%88%90Redis%2F</url>
    <content type="text"><![CDATA[由于之前在项目中集成了身份验证以及权限功能，所以在请求操作时会有大量数据库读操作来获取用户的角色以及所拥有的权限，这对于数据库来说是一个巨大的性能开支，所以我打算尝试使用redis来减少数据库读写，而是使用缓存读写，本文是在看完集成Redis入门后所做的总结，所以对很多地方还有疑问，请大家酌情参考 一、添加Redis依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt;&lt;/dependency&gt; 二、application.properties中添加配置参数123456789101112131415# Redis相关spring.redis.database=0spring.redis.host=localhostspring.redis.port=6379spring.redis.password=# 连接池最大连接数（使用负值表示没有限制）spring.redis.jedis.pool.max-active=8# 连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.jedis.pool.max-wait=-1# 连接池中的最大空闲连接spring.redis.jedis.pool.max-idle=8# 连接池中的最小空闲连接spring.redis.jedis.pool.min-idle=0# 连接超时时间（毫秒）spring.redis.timeout=0 三、书写Redis工具类 本部分是很久之前参考的他人的代码，由于时间久远，原博文链接找不到了，所以在此无法放上原文章链接，对原博主表示歉意，如果有知道原链接的，请发给我谢谢123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558@Componentpublic class RedisUtil &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; public RedisUtil(RedisTemplate&lt;String, Object&gt; redisTemplate) &#123; this.redisTemplate = redisTemplate; &#125; /** * 指定缓存失效时间 * * @param key 键 * @param time 时间(秒) * @return */ public boolean expire(String key, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.expire(key, time, TimeUnit.SECONDS); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 根据key 获取过期时间 * * @param key 键 不能为null * @return 时间(秒) 返回0代表为永久有效 */ public long getExpire(String key) &#123; return redisTemplate.getExpire(key, TimeUnit.SECONDS); &#125; /** * 判断key是否存在 * * @param key 键 * @return true 存在 false不存在 */ public boolean hasKey(String key) &#123; try &#123; return redisTemplate.hasKey(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 删除缓存 * * @param key 可以传一个值 或多个 */ @SuppressWarnings("unchecked") public void del(String... key) &#123; if (key != null &amp;&amp; key.length &gt; 0) &#123; if (key.length == 1) &#123; redisTemplate.delete(key[0]); &#125; else &#123; redisTemplate.delete(CollectionUtils.arrayToList(key)); &#125; &#125; &#125; //============================String============================= /** * 普通缓存获取 * * @param key 键 * @return 值 */ public Object get(String key) &#123; return key == null ? null : redisTemplate.opsForValue().get(key); &#125; /** * 普通缓存放入 * * @param key 键 * @param value 值 * @return true成功 false失败 */ public boolean set(String key, Object value) &#123; try &#123; redisTemplate.opsForValue().set(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 普通缓存放入并设置时间 * * @param key 键 * @param value 值 * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * @return true成功 false 失败 */ public boolean set(String key, Object value, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); &#125; else &#123; set(key, value); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 递增 * * @param key 键 * @param delta 要增加几(大于0) * @return */ public long incr(String key, long delta) &#123; if (delta &lt; 0) &#123; throw new RuntimeException("递增因子必须大于0"); &#125; return redisTemplate.opsForValue().increment(key, delta); &#125; /** * 递减 * * @param key 键 * @param delta 要减少几(小于0) * @return */ public long decr(String key, long delta) &#123; if (delta &lt; 0) &#123; throw new RuntimeException("递减因子必须大于0"); &#125; return redisTemplate.opsForValue().increment(key, -delta); &#125; //================================Map================================= /** * HashGet * * @param key 键 不能为null * @param item 项 不能为null * @return 值 */ public Object hget(String key, String item) &#123; return redisTemplate.opsForHash().get(key, item); &#125; /** * 获取hashKey对应的所有键值 * * @param key 键 * @return 对应的多个键值 */ public Map&lt;Object, Object&gt; hmget(String key) &#123; return redisTemplate.opsForHash().entries(key); &#125; /** * HashSet * * @param key 键 * @param map 对应多个键值 * @return true 成功 false 失败 */ public boolean hmset(String key, Map&lt;String, Object&gt; map) &#123; try &#123; redisTemplate.opsForHash().putAll(key, map); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * HashSet 并设置时间 * * @param key 键 * @param map 对应多个键值 * @param time 时间(秒) * @return true成功 false失败 */ public boolean hmset(String key, Map&lt;String, Object&gt; map, long time) &#123; try &#123; redisTemplate.opsForHash().putAll(key, map); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 向一张hash表中放入数据,如果不存在将创建 * * @param key 键 * @param item 项 * @param value 值 * @return true 成功 false失败 */ public boolean hset(String key, String item, Object value) &#123; try &#123; redisTemplate.opsForHash().put(key, item, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 向一张hash表中放入数据,如果不存在将创建 * * @param key 键 * @param item 项 * @param value 值 * @param time 时间(秒) 注意:如果已存在的hash表有时间,这里将会替换原有的时间 * @return true 成功 false失败 */ public boolean hset(String key, String item, Object value, long time) &#123; try &#123; redisTemplate.opsForHash().put(key, item, value); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 删除hash表中的值 * * @param key 键 不能为null * @param item 项 可以使多个 不能为null */ public void hdel(String key, Object... item) &#123; redisTemplate.opsForHash().delete(key, item); &#125; /** * 判断hash表中是否有该项的值 * * @param key 键 不能为null * @param item 项 不能为null * @return true 存在 false不存在 */ public boolean hHasKey(String key, String item) &#123; return redisTemplate.opsForHash().hasKey(key, item); &#125; /** * hash递增 如果不存在,就会创建一个 并把新增后的值返回 * * @param key 键 * @param item 项 * @param by 要增加几(大于0) * @return */ public double hincr(String key, String item, double by) &#123; return redisTemplate.opsForHash().increment(key, item, by); &#125; /** * hash递减 * * @param key 键 * @param item 项 * @param by 要减少记(小于0) * @return */ public double hdecr(String key, String item, double by) &#123; return redisTemplate.opsForHash().increment(key, item, -by); &#125; //============================set============================= /** * 根据key获取Set中的所有值 * * @param key 键 * @return */ public Set&lt;Object&gt; sGet(String key) &#123; try &#123; return redisTemplate.opsForSet().members(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 根据value从一个set中查询,是否存在 * * @param key 键 * @param value 值 * @return true 存在 false不存在 */ public boolean sHasKey(String key, Object value) &#123; try &#123; return redisTemplate.opsForSet().isMember(key, value); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 将数据放入set缓存 * * @param key 键 * @param values 值 可以是多个 * @return 成功个数 */ public long sSet(String key, Object... values) &#123; try &#123; return redisTemplate.opsForSet().add(key, values); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 将set数据放入缓存 * * @param key 键 * @param time 时间(秒) * @param values 值 可以是多个 * @return 成功个数 */ public long sSetAndTime(String key, long time, Object... values) &#123; try &#123; Long count = redisTemplate.opsForSet().add(key, values); if (time &gt; 0) &#123; expire(key, time); &#125; return count; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 获取set缓存的长度 * * @param key 键 * @return */ public long sGetSetSize(String key) &#123; try &#123; return redisTemplate.opsForSet().size(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 移除值为value的 * * @param key 键 * @param values 值 可以是多个 * @return 移除的个数 */ public long setRemove(String key, Object... values) &#123; try &#123; Long count = redisTemplate.opsForSet().remove(key, values); return count; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; //===============================list================================= /** * 获取list缓存的内容 * * @param key 键 * @param start 开始 * @param end 结束 0 到 -1代表所有值 * @return */ public List&lt;Object&gt; lGet(String key, long start, long end) &#123; try &#123; return redisTemplate.opsForList().range(key, start, end); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 获取list缓存的长度 * * @param key 键 * @return */ public long lGetListSize(String key) &#123; try &#123; return redisTemplate.opsForList().size(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 通过索引 获取list中的值 * * @param key 键 * @param index 索引 index&gt;=0时， 0 表头，1 第二个元素，依次类推；index&lt;0时，-1，表尾，-2倒数第二个元素，依次类推 * @return */ public Object lGetIndex(String key, long index) &#123; try &#123; return redisTemplate.opsForList().index(key, index); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 将list放入缓存 * * @param key 键 * @param value 值 * @return */ public boolean lSet(String key, Object value) &#123; try &#123; redisTemplate.opsForList().rightPush(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 将list放入缓存 * * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, Object value, long time) &#123; try &#123; redisTemplate.opsForList().rightPush(key, value); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 将list放入缓存 * * @param key 键 * @param value 值 * @return */ public boolean lSet(String key, List&lt;Object&gt; value) &#123; try &#123; redisTemplate.opsForList().rightPushAll(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 将list放入缓存 * * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, List&lt;Object&gt; value, long time) &#123; try &#123; redisTemplate.opsForList().rightPushAll(key, value); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 根据索引修改list中的某条数据 * * @param key 键 * @param index 索引 * @param value 值 * @return */ public boolean lUpdateIndex(String key, long index, Object value) &#123; try &#123; redisTemplate.opsForList().set(key, index, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 移除N个值为value * * @param key 键 * @param count 移除多少个 * @param value 值 * @return 移除的个数 */ public long lRemove(String key, long count, Object value) &#123; try &#123; Long remove = redisTemplate.opsForList().remove(key, count, value); return remove; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125;&#125; 四、使用redis缓存123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Servicepublic class ShiroServiceImpl implements ShiroService &#123; public Logger logger = LoggerFactory.getLogger(getClass()); @Autowired PermissionDao permissionDao; @Autowired RoleDao roleDao; @Autowired RedisUtil redisUtil; @Override public Set&lt;String&gt; listUserPerms(int userId) &#123; Set&lt;String&gt; permsSet = new HashSet&lt;&gt;(); String key = "Shiro:Perms:" + userId; if(redisUtil.hasKey(key))&#123; permsSet = (Set&lt;String&gt;) redisUtil.get(key); logger.info("ShiroServiceImpl.listUserPerms : 从缓存中获取了用户 &#123;&#125; 的权限 &gt;&gt; &#123;&#125; ", userId, permsSet); &#125;else &#123; List&lt;String&gt; permsList = permissionDao.listUserPerms(userId); for (String perms : permsList) &#123; if (perms == null || perms.isEmpty()) &#123; continue; &#125; permsSet.addAll(Arrays.asList(perms.trim().split(","))); &#125; redisUtil.set(key, permsSet); logger.info("ShiroServiceImpl.listUserPerms : 向缓存中插入了用户 &#123;&#125; 的权限 &gt;&gt; &#123;&#125; ", userId, permsSet); &#125; return permsSet; &#125; @Override public Set&lt;String&gt; listUserRoles(int userId) &#123; Set&lt;String&gt; userRolesSet; String key = "Shiro:Roles:" + userId; if(redisUtil.hasKey(key))&#123; userRolesSet = (Set&lt;String&gt;) redisUtil.get(key); logger.info("ShiroServiceImpl.listUserRoles : 从缓存中获取了用户 &#123;&#125; 的角色 &gt;&gt; &#123;&#125; ", userId, userRolesSet); &#125;else &#123; userRolesSet = roleDao.listUserRoles(userId); redisUtil.set(key, userRolesSet); logger.info("ShiroServiceImpl.listUserRoles : 向缓存中插入了用户 &#123;&#125; 的角色 &gt;&gt; &#123;&#125; ", userId, userRolesSet); &#125; return roleDao.listUserRoles(userId); &#125;&#125; 总结：由于我此次参考资料不多，所以感觉这个总结中还是有很多欠缺，后期会重新对SpringBoot集成Redis进行新的总结]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot集成定时任务功能]]></title>
    <url>%2F2019%2F07%2F24%2Fspringboot%E9%9B%86%E6%88%90%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[参考文章：http://www.ityouknow.com/springboot/2016/12/02/spring-boot-scheduler.html 1.集成pom包依赖1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.在启动类中添加注释 在启动类中添加@EnableScheduling即可开启定时12345678@SpringBootApplication@EnableSchedulingpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 3.创建任意的一个定时线程实现类12345678@Componentpublic class SchedulerTask &#123; @Scheduled(cron = "0 0,30 * * * *") //cron接受cron表达式，根据cron表达式确定定时规则 public void schedulerTask() &#123; // todo 业务 &#125;&#125; cron表达式可以参考这篇博文：https://www.jianshu.com/p/e9ce1a7e1ed1 4.fixedRate 参数说明@Scheduled(fixedRate = 6000) ：上一次开始执行时间点之后6秒再执行@Scheduled(fixedDelay = 6000) ：上一次执行完毕时间点之后6秒再执行@Scheduled(initialDelay=1000, fixedRate=6000) ：第一次延迟1秒后执行，之后按 fixedRate 的规则每6秒执行一次]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>SchedulerTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot集成多线程功能]]></title>
    <url>%2F2019%2F07%2F22%2Fspringboot%E9%9B%86%E6%88%90%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[参考文章https://segmentfault.com/a/1190000015766938https://blog.csdn.net/qq_34545192/article/details/80484780 在平时我们写多线程可能更多是使用new Thread() 或者创建线程池来实现的，但是在阿里的java开发规范中要求不要自己直接创建新线程，而是通过线程池来实现的，恰好spring boot支持多线程的开发，所以我尝试通过多线程的方式来暂时解决原来同步发送邮件是的请求时间过长的问题。当然多线程并不能彻底解决实现异步，还是需要通过消息中间件来实现功能的解耦，实现真正的异步 1.在springboot的启动类添加@EnableAsync注解123456789@SpringBootApplication@EnableAsyncpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.添加ThreadConfig的配置类12345678910111213141516171819@Configuration@EnableAsyncpublic class ThreadConfig implements AsyncConfigurer &#123; @Override public Executor getAsyncExecutor() &#123; ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); executor.setMaxPoolSize(15); executor.setQueueCapacity(25); executor.initialize(); return executor; &#125; @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() &#123; return null; &#125;&#125; 3.下面我们编写多线程部分的业务实现123456789101112131415161718192021222324252627282930@Componentpublic class EmailTool &#123; @Autowired JavaMailSender javaMailSender; @Value("$&#123;spring.mail.username&#125;") private String sendFrom; //读取配置文件中的参数 @Async public void sendEmailAsync(EmailEntity emailEntity, CountDownLatch latch) &#123; sendEmail(emailEntity); latch.countDown(); &#125; public void sendEmail(EmailEntity emailEntity) &#123; MimeMessage message = javaMailSender.createMimeMessage(); MimeMessageHelper helper; try &#123; helper = new MimeMessageHelper(message, true);//这里可以自定义发信名称 helper.setFrom(sendFrom); helper.setTo(emailEntity.getSendTo()); helper.setSubject(emailEntity.getSubject()); helper.setText(emailEntity.getMessage(), true); &#125; catch (MessagingException e) &#123; e.printStackTrace(); &#125; javaMailSender.send(message); &#125;&#125; 通过上述步骤就可以实现sendEmailAsync函数的异步功能了,但是在使用这个方法的时候我们要注意的问题就是 @Async无效的问题异步方法和调用方法一定要 写在不同的类中 ,如果写在一个类中,是没有效果的]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>SchedulerTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot集成Shiro]]></title>
    <url>%2F2019%2F07%2F20%2Fspringboot%E9%9B%86%E6%88%90Shiro%2F</url>
    <content type="text"><![CDATA[一、介绍Apache Shiro是一个强大且易用的Java安全框架,执行身份验证、授权、密码和会话管理。使用Shiro的易于理解的API,您可以快速、轻松地获得任何应用程序,从最小的移动应用程序到最大的网络和企业应用程序。本文是使用Shiro + JWT(Json Web Token)实现的，对于jwt部分有疑问的可以参考之前jwt相关文章，本博文中的一部分函数使用的是JWT那一篇文章文章所写的函数。 参考博文 https://blog.csdn.net/w_stronger/article/details/73109248 二、创建项目使用idea创建空项目并添加以下依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt; 三、创建自定义Realm123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//MyRealm.java@Servicepublic class MyRealm extends AuthorizingRealm &#123; //用户相关 @Autowired private UserDao userDao; //shiro数据相关 @Autowired private ShiroService shiroService; /** * 大坑！，必须重写此方法，不然Shiro会报错 */ @Override public boolean supports(AuthenticationToken token) &#123; return token instanceof JWTToken; &#125; /** * 只有当需要检测用户权限的时候才会调用此方法，例如checkRole,checkPermission之类的 */ @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123; //获取用户token String token = (String) principals.getPrimaryPrincipal(); //通过token获取用户信息 String username = JWTUtil.getUsername(token); User user = userDao.getUserAllInfoByTel(username); int userId = user.getId(); //获取用户拥有操作 Set&lt;String&gt; permsSet = shiroService.listUserPerms(userId); SimpleAuthorizationInfo info = new SimpleAuthorizationInfo(); info.addStringPermissions(permsSet); //获取用户拥有角色 Set&lt;String&gt; rolesSet = shiroService.listUserRoles(userId); info.setRoles(rolesSet); return info; &#125; /** * 默认使用此方法进行用户名正确与否验证，错误抛出异常即可。 */ @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken auth) throws AuthenticationException &#123; String token = (String) auth.getCredentials(); // 解密获得username，用于和数据库进行对比，此部分的身份验证请参考我的JWT相关博文 String username = JWTUtil.getUsername(token); if (username == null) &#123; throw new AuthenticationException("Username or password error"); &#125; User user = userDao.getUserAllInfoByTel(username); if (user == null) &#123; throw new AuthenticationException("Username or password error"); &#125; //验证信息 if (!JWTUtil.verify(token, username, user.getPassword())) &#123; throw new AuthenticationException("Username or password error"); &#125; return new SimpleAuthenticationInfo(token, token, "myRealm"); &#125;&#125; 四、自定义JWT拦截器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112//JWTFilter.javapublic class JWTFilter extends BasicHttpAuthenticationFilter &#123; private Logger logger = LoggerFactory.getLogger(getClass()); /** * 这里我们详细说明下为什么最终返回的都是true，即允许访问 * 例如我们提供一个地址 GET /article * 登入用户和游客看到的内容是不同的 * 如果在这里返回了false，请求会被直接拦截，用户看不到任何东西 * 所以我们在这里返回true，Controller中可以通过 subject.isAuthenticated() 来判断用户是否登入 * 如果有些资源只有登入用户才能访问，我们只需要在方法上面加上 @RequiresAuthentication 注解即可 * 但是这样做有一个缺点，就是不能够对GET,POST等请求进行分别过滤鉴权(因为我们重写了官方的方法)，但实际上对应用影响不大 */ @Override protected boolean isAccessAllowed(ServletRequest request, ServletResponse response, Object mappedValue) &#123; // 查看当前Header中是否携带Authorization属性(Token)，有的话就进行登录认证授权 if (this.isLoginAttempt(request, response)) &#123; try &#123; // 进行Shiro的登录UserRealm this.executeLogin(request, response); &#125; catch (Exception e) &#123; // 认证出现异常，传递错误信息msg String msg = e.getMessage(); // 获取应用异常(该Cause是导致抛出此throwable(异常)的throwable(异常)) Throwable throwable = e.getCause(); if (throwable instanceof SignatureVerificationException) &#123; // 该异常为JWT的AccessToken认证失败(Token或者密钥不正确) msg = "Token或者密钥不正确(" + throwable.getMessage() + ")"; &#125; else if (throwable instanceof TokenExpiredException) &#123; &#125; else &#123; // 应用异常不为空 if (throwable != null) &#123; // 获取应用异常msg msg = throwable.getMessage(); &#125; &#125; /* 错误两种处理方式 1. 将非法请求转发到/401的Controller处理，抛出自定义无权访问异常被全局捕捉再返回Response信息 2. 无需转发，直接返回Response信息 一般使用第二种(更方便) */ return false; &#125; &#125; else &#123; // 没有携带Token HttpServletRequest httpServletRequest = WebUtils.toHttp(request); // 获取当前请求类型 String httpMethod = httpServletRequest.getMethod(); // 获取当前请求URI String requestURI = httpServletRequest.getRequestURI(); logger.info("当前请求 &#123;&#125; Authorization属性(Token)为空 请求类型 &#123;&#125;", requestURI, httpMethod); &#125; return true; &#125; /** * 判断用户是否想要登入。 * 检测header里面是否包含Authorization字段即可 */ @Override protected boolean isLoginAttempt(ServletRequest request, ServletResponse response) &#123; HttpServletRequest req = (HttpServletRequest) request; String authorization = req.getHeader("AuthKey"); return authorization != null; &#125; /** * 进行AccessToken登录认证授权 */ @Override protected boolean executeLogin(ServletRequest request, ServletResponse response) throws Exception &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; String authorization = httpServletRequest.getHeader("AuthKey"); JWTToken token = new JWTToken(authorization); // 提交给realm进行登入，如果错误他会抛出异常并被捕获 getSubject(request, response).login(token); // 如果没有抛出异常则代表登入成功，返回true return true; &#125; /** * 对跨域提供支持 */ @Override protected boolean preHandle(ServletRequest request, ServletResponse response) throws Exception &#123; HttpServletRequest httpServletRequest = (HttpServletRequest) request; HttpServletResponse httpServletResponse = (HttpServletResponse) response; httpServletResponse.setHeader("Access-control-Allow-Origin", httpServletRequest.getHeader("Origin")); httpServletResponse.setHeader("Access-Control-Allow-Methods", "GET,POST,OPTIONS,PUT,DELETE"); httpServletResponse.setHeader("Access-Control-Allow-Headers", httpServletRequest.getHeader("Access-Control-Request-Headers")); // 跨域时会首先发送一个option请求，这里我们给option请求直接返回正常状态 if (httpServletRequest.getMethod().equals(RequestMethod.OPTIONS.name())) &#123; httpServletResponse.setStatus(HttpStatus.OK.value()); return false; &#125; return super.preHandle(request, response); &#125; /** * 将非法请求跳转到 /401 */ private void response401(ServletRequest req, ServletResponse resp) &#123; try &#123; HttpServletResponse httpServletResponse = (HttpServletResponse) resp; httpServletResponse.sendRedirect("/401"); &#125; catch (IOException e) &#123; logger.error(e.getMessage()); &#125; &#125;&#125; 五、添加配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//ShiroConfig.java@Configurationpublic class ShiroConfig &#123; @Bean("securityManager") public DefaultWebSecurityManager getManager(MyRealm realm) &#123; //使用自定义realm DefaultWebSecurityManager manager = new DefaultWebSecurityManager(); manager.setRealm(realm); /* * 关闭shiro自带的session，详情见文档 * http://shiro.apache.org/session-management.html#SessionManagement-StatelessApplications%28Sessionless%29 */ DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO(); DefaultSessionStorageEvaluator defaultSessionStorageEvaluator = new DefaultSessionStorageEvaluator(); defaultSessionStorageEvaluator.setSessionStorageEnabled(false); subjectDAO.setSessionStorageEvaluator(defaultSessionStorageEvaluator); manager.setSubjectDAO(subjectDAO); return manager; &#125; @Bean("shiroFilter") public ShiroFilterFactoryBean factory(DefaultWebSecurityManager securityManager) &#123; ShiroFilterFactoryBean factoryBean = new ShiroFilterFactoryBean(); factoryBean.setSecurityManager(securityManager); // 添加自己的过滤器并且取名为jwt Map&lt;String, Filter&gt; filterMap = new HashMap&lt;&gt;(); filterMap.put("jwt", new JWTFilter()); factoryBean.setFilters(filterMap); /* * 自定义url规则 * http://shiro.apache.org/web.html#urls- */ Map&lt;String, String&gt; filterRuleMap = new HashMap&lt;&gt;(); // 所有请求通过我们自己的JWT Filter filterRuleMap.put("/users/login", "anon"); filterRuleMap.put("/users", "jwt"); filterRuleMap.put("/users/*", "jwt"); factoryBean.setFilterChainDefinitionMap(filterRuleMap); return factoryBean; &#125; /** * 下面的代码是添加注解支持 */ @Bean @DependsOn("lifecycleBeanPostProcessor") public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123; DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); // 强制使用cglib，防止重复代理和可能引起代理出错的问题 // https://zhuanlan.zhihu.com/p/29161098 defaultAdvisorAutoProxyCreator.setProxyTargetClass(true); return defaultAdvisorAutoProxyCreator; &#125; @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() &#123; return new LifecycleBeanPostProcessor(); &#125; @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) &#123; AuthorizationAttributeSourceAdvisor advisor = new AuthorizationAttributeSourceAdvisor(); advisor.setSecurityManager(securityManager); return advisor; &#125;&#125; 注：一定要将所有需要判断拦截的请求使用自定义的拦截器进行拦截 6、使用注解对接口进行权限判断以及身份验证123456789101112131415/** * 这个函数的意思是需要请求这个接口的用户角色需要时admin， * 同时具有sys:user:save的权限 */@PostMapping@RequiresRoles("admin")@RequiresPermissions(value = "sys:user:save")public RestResult addUser(@RequestBody User user) &#123; logger.debug("/users [Post] -&gt; user : &#123;&#125;", user); try &#123; return userService.insertUser(user); &#125; catch (Exception e) &#123; return RestResultFactory.restResult(402, "用户创建失败"); &#125;&#125; 7、对于常见注解的介绍Shiro共有5个注解，接下来我们就详细说说吧 1.RequiresAuthentication: 使用该注解标注的类，实例，方法在访问或调用时，当前Subject必须在当前session中已经过认证。 2.RequiresGuest: 使用该注解标注的类，实例，方法在访问或调用时，当前Subject可以是“gust”身份，不需要经过认证或者在原先的session中存在记录。 3.RequiresPermissions: 当前Subject需要拥有某些特定的权限时，才能执行被该注解标注的方法。如果当前Subject不具有这样的权限，则方法不会被执行。 4.RequiresRoles: 当前Subject必须拥有所有指定的角色时，才能访问被该注解标注的方法。如果当前Subject不同时拥有所有指定角色，则方法不会执行还会抛出AuthorizationException异常。 5.RequiresUser 当前Subject必须是应用的用户，才能访问或调用被该注解标注的类，实例，方法。 使用方法： Shiro的认证注解处理是有内定的处理顺序的，如果有个多个注解的话，前面的通过了会继续检查后面的，若不通过则直接返回，处理顺序依次为（与实际声明顺序无关）： RequiresRoles RequiresPermissions RequiresAuthentication RequiresUser RequiresGuest例如：你同时声明了RequiresRoles和RequiresPermissions，那就要求拥有此角色的同时还得拥有相应的权限。 1) RequiresRoles 可以用在Controller或者方法上。可以多个roles，多个roles时默认逻辑为 AND也就是所有具备所有role才能访问。123456@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface RequiresRoles &#123; String[] value(); Logical logical() default Logical.AND; &#125; 示例12345678//属于user角色@RequiresRoles("user")//必须同时属于user和admin角色@RequiresRoles(&#123;"user","admin"&#125;)//属于user或者admin之一;修改logical为OR 即可@RequiresRoles(value=&#123;"user","admin"&#125;,logical=Logical.OR) 2) RequiresPermissions123456@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface RequiresPermissions &#123; String[] value(); Logical logical() default Logical.AND; &#125; 示例12345678//符合index:hello权限要求@RequiresPermissions("index:hello")//必须同时复核index:hello和index:world权限要求@RequiresPermissions(&#123;"index:hello","index:world"&#125;)//符合index:hello或index:world权限要求即可@RequiresPermissions(value=&#123;"index:hello","index:world"&#125;,logical=Logical.OR) 1) RequiresAuthentication，RequiresUser，RequiresGuest 这三个的使用方法一样 @RequiresAuthentication @RequiresUser @RequiresGusst 总结：总体来说，shiro的存在大大的降低了关于权限部分的开发时间，使我们有更多精力关注业务开发，也让我们能快速的开发出一个完善的权限系统，避免自己开发权限时出现的权限考虑不周到的情况]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用JWT]]></title>
    <url>%2F2019%2F07%2F16%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JWT%2F</url>
    <content type="text"><![CDATA[参考博文：https://www.jianshu.com/p/576dbf44b2ae 一、介绍 Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。 二、JWT样式格式12345678910eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c//第一部分我们称它为头部（header)eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9//第二部分我们称其为载荷（payload, 类似于飞机上承载的物品)yJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ//第三部分是签证（signature)SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c 三、如何使用JWT 关于JWT在此我就不做过多的介绍，下面直接看，如何通过代码实现JWT 1.导入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.auth0&lt;/groupId&gt; &lt;artifactId&gt;java-jwt&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; 2.创建Token实体类12345678910111213141516171819//JWTTokenpublic class JWTToken implements AuthenticationToken &#123; private String token; public JWTToken(String token) &#123; this.token = token; &#125; @Override public Object getPrincipal() &#123; return token; &#125; @Override public Object getCredentials() &#123; return token; &#125;&#125; 3.书写工具类（包括加密解密）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//JWTUtil.javapublic class JWTUtil &#123; //超时时间 private static final long EXPIRE_TIME = 5 * 60 * 1000; /** * 生成签名 * * @param username 用户名 * @param secret 密码 * @return 密钥 */ public static String sign(String username, String secret) &#123; try &#123; Date date = new Date(System.currentTimeMillis() + EXPIRE_TIME); Algorithm algorithm = Algorithm.HMAC256(secret); return JWT.create().withClaim("username", username) .withExpiresAt(date)//添加超时时间 .sign(algorithm); &#125; catch (UnsupportedEncodingException e) &#123; return null; &#125; &#125; /** * 获取token中的用户名，无需secret解密，也可以类似的获取token中其他在payload中的数据 * @param token 密钥 * @return 用户名 */ public static String getUsername(String token) &#123; try &#123; DecodedJWT jwt = JWT.decode(token); return jwt.getClaim("username").asString(); &#125; catch (Exception e) &#123; return null; &#125; &#125; /** * 校验token是否正确 * * @param token 密钥 * @param username 用户名 * @param secret 密码 * @return 是否正确 */ public static boolean verify(String token, String username, String secret) &#123; try &#123; Algorithm algorithm = Algorithm.HMAC256(secret); JWTVerifier verifier = JWT.require(algorithm) .withClaim("username", username) .build(); DecodedJWT jwt = verifier.verify(token); return true; &#125; catch (TokenExpiredException e) &#123; return false; &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); return false; &#125; &#125;&#125; 总结: JWT只是一个单独的工具，要和其他的框架或者自行实现一些其他功能才能真的将JWT应用在后台开发中，后面我将总结如何使用Shiro + JWT实现一个简单的权限认证功能。]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于MySQL日期相关语句]]></title>
    <url>%2F2019%2F07%2F12%2F%E5%85%B3%E4%BA%8EMySQL%E6%97%A5%E6%9C%9F%E7%9B%B8%E5%85%B3%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[1.MySQL按照时间范围进行查询的SQL语句12# DATE_FORMAT(字段名, 格式化规则) startDate endDate指代查询范围select * from t_name where DATE_FORMAT(c_date, '%Y%m%d%H%i') between startDate and endDate; 2.MySQL中的时间的加减(以加法为例)1select date_add('2018-06-26 23:59:59',INTERVAL 1 hour); 输出结果为 ‘2018-06-27 0:59:59’ 3.较为复杂的MySQL语句，用来查询当前时间一个小时后的时间12# 数据格式：201907121251（2019年07月12日12时51分）select DATE_FORMAT(DATE_ADD(DATE_FORMAT(now(), '%Y-%m-%d %H:%i'), interval '1' hour),'%Y%m%d%H%i'); 更多相关可以参考下面两篇文章https://yq.aliyun.com/articles/526458https://www.cnblogs.com/zhongchi/archive/2010/05/04/1727096.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于编程方式实现条件装配]]></title>
    <url>%2F2019%2F06%2F14%2F%E5%9F%BA%E4%BA%8E%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E6%9D%A1%E4%BB%B6%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[本文主要讲通过编程方式来实现条件装配 —— @Condition 首先我们添加判断类OnSystemPropertyCondition.java 1234567891011121314151617181920212223242526272829package xyz.suiwo.diveinspringboot.condition;import org.springframework.context.annotation.Condition;import org.springframework.context.annotation.ConditionContext;import org.springframework.core.type.AnnotatedTypeMetadata;import java.lang.annotation.Annotation;import java.util.Map;/** * @author suiwo * @title: OnSystemPropertyCondition * @date 2019-06-06 15:04 * * 系统属性条件判断 */public class OnSystemPropertyCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; Map&lt;String, Object&gt; attributes = metadata.getAnnotationAttributes(ConditionalOnSystemProperty.class.getName()); String propertyName = String.valueOf(attributes.get("name")); String propertyValue = String.valueOf(attributes.get("value")); String javaPropertyValue = System.getProperty(propertyName); System.out.println(javaPropertyValue); return propertyValue.equals(javaPropertyValue); &#125;&#125; 然后添加注解类ConditionalOnSystemProperty.java 1234567891011121314151617181920212223242526272829303132package xyz.suiwo.diveinspringboot.condition;import org.springframework.context.annotation.Conditional;import java.lang.annotation.*;/** * @author suiwo * @title: ConditionalOnSystemProperty * @date 2019-06-06 15:02 * * Java系统属性条件判断 */@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Documented@Conditional(OnSystemPropertyCondition.class)public @interface ConditionalOnSystemProperty &#123; /** * Java系统属性名称 * @return */ String name(); /** * Java系统属性值 * @return */ String value();&#125; 最后我们添加引导启动类ConditionalSystemPropertyBootstrap.java 1234567891011121314151617181920212223242526272829303132333435363738394041package xyz.suiwo.diveinspringboot.bootstrap;import org.springframework.boot.WebApplicationType;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.context.annotation.Bean;import xyz.suiwo.diveinspringboot.condition.ConditionalOnSystemProperty;/** * @author suiwo * @title: ConditionalSystemPropertyBootstrap * @date 2019-06-06 15:10 * * 系统属性条件引导类 */public class ConditionalSystemPropertyBootstrap &#123; @Bean @ConditionalOnSystemProperty(name = "user.name",value = "zhangsan") public String helloWorld()&#123; return "Hello world zhangsan"; &#125; @Bean @ConditionalOnSystemProperty(name = "user.name",value = "suiwo") public String helloW()&#123; return "Hello world suiwo"; &#125; public static void main(String[] args) &#123; ConfigurableApplicationContext context = new SpringApplicationBuilder(ConditionalSystemPropertyBootstrap.class) .web(WebApplicationType.NONE) .run(args); System.out.println(context.getBean("helloW",String.class)); System.out.println(context.getBean("helloWorld",String.class)); //关闭上下文 context.close(); &#125;&#125; 当我们运行后会发现终端运行结果1234567891011121314151617181920212223 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.5.RELEASE)2019-06-14 11:28:50.783 INFO 30704 --- [ main] s.d.b.ConditionalSystemPropertyBootstrap : Starting ConditionalSystemPropertyBootstrap on suiwoMBP with PID 30704 (/Users/dive-in-spring-boot/target/classes started by suiwo in /Users/dive-in-spring-boot)2019-06-14 11:28:50.790 INFO 30704 --- [ main] s.d.b.ConditionalSystemPropertyBootstrap : No active profile set, falling back to default profiles: defaultsuiwosuiwo2019-06-14 11:28:51.251 INFO 30704 --- [ main] s.d.b.ConditionalSystemPropertyBootstrap : Started ConditionalSystemPropertyBootstrap in 1.218 seconds (JVM running for 1.945)Hello world suiwoException in thread &quot;main&quot; org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named &apos;helloWorld&apos; available at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:769) at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1221) at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:294) at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:204) at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1111) at xyz.suiwo.diveinspringboot.bootstrap.ConditionalSystemPropertyBootstrap.main(ConditionalSystemPropertyBootstrap.java:36)Process finished with exit code 1 由此可见当不满足条件时，此时bean无法装配]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于配置方式实现自定义条件装配]]></title>
    <url>%2F2019%2F06%2F14%2F%E5%9F%BA%E4%BA%8E%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%9D%A1%E4%BB%B6%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[本文主要讲通过配置方式来实现自定义条件装配 —— @Profile 我们尝试使用Profile实现两个计算服务，分别是Java7的for循环以及Java8的lambda表达式 首先我们先新建一个计算服务接口CalculateService.java 12345678910111213141516package xyz.suiwo.diveinspringboot.service;/** * @author suiwo * @title: CalculateService * @date 2019-06-06 13:55 */public interface CalculateService &#123; /** * sum求和 * @param values 多个整数 * @return */ Integer sum(Integer... values);&#125; 添加两种方式的实现类Java7CalculateService.java 1234567891011121314151617181920212223242526272829package xyz.suiwo.diveinspringboot.service;import org.springframework.context.annotation.Profile;import org.springframework.stereotype.Service;/** * @author suiwo * @title: Java7CalculateService * @date 2019-06-06 13:56 */@Service@Profile("Java7")public class Java7CalculateService implements CalculateService&#123; @Override public Integer sum(Integer... values) &#123; System.out.println("Java7实现"); int sum = 0; for(int i : values)&#123; sum += i; &#125; return sum; &#125; public static void main(String[] args) &#123; CalculateService calculateService = new Java7CalculateService(); System.out.println(calculateService.sum(1,2,3,4,5,6,7,8,9,10)); &#125;&#125; Java8CalculateService.java 123456789101112131415161718192021222324252627package xyz.suiwo.diveinspringboot.service;import org.springframework.context.annotation.Profile;import org.springframework.stereotype.Service;import java.util.stream.Stream;/** * @author suiwo * @title: Java8CalculateService * @date 2019-06-06 13:58 */@Service@Profile("Java8")public class Java8CalculateService implements CalculateService &#123; @Override public Integer sum(Integer... values) &#123; System.out.println("Java8实现"); int sum = Stream.of(values).reduce(0,Integer::sum); return sum; &#125; public static void main(String[] args) &#123; CalculateService calculateService = new Java7CalculateService(); System.out.println(calculateService.sum(1,2,3,4,5,6,7,8,9,10)); &#125;&#125; 下面我们来添加启动类CalculateServiceBootstrap 1234567891011121314151617181920212223242526272829303132package xyz.suiwo.diveinspringboot.bootstrap;import org.springframework.boot.WebApplicationType;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.context.ConfigurableApplicationContext;import xyz.suiwo.diveinspringboot.service.CalculateService;/** * @author suiwo * @title: CalculateServiceBootstrap * @date 2019-06-06 14:01 */@SpringBootApplication(scanBasePackages = "xyz.suiwo.diveinspringboot.service")public class CalculateServiceBootstrap &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = new SpringApplicationBuilder(CalculateServiceBootstrap.class) .web(WebApplicationType.NONE) .profiles("Java8") .run(args); CalculateService calculateService = context .getBean(CalculateService.class); System.out.println("sum:" + calculateService.sum(1,2,3,4,5,6,7,8,9,10)); //关闭上下文 context.close(); &#125;&#125; 此时终端打印结果如下 123456789101112131415 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.5.RELEASE)2019-06-14 11:09:17.221 INFO 30420 --- [ main] x.s.d.b.CalculateServiceBootstrap : Starting CalculateServiceBootstrap on XXX with PID 30434 (/Users/dive-in-spring-boot/target/classes started by XXX in /Users/dive-in-spring-boot)2019-06-14 11:09:17.228 INFO 30420 --- [ main] x.s.d.b.CalculateServiceBootstrap : The following profiles are active: Java82019-06-14 11:09:20.062 INFO 30420 --- [ main] x.s.d.b.CalculateServiceBootstrap : Started CalculateServiceBootstrap in 3.416 seconds (JVM running for 4.569)Java8实现sum:55Process finished with exit code 0 若将profile改成Java7，则终端结果为 12345678910111213 . ____ _ __ _ _ /\\ / ___&apos;_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) &apos; |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.1.5.RELEASE)2019-06-14 11:10:17.847 INFO 30434 --- [ main] x.s.d.b.CalculateServiceBootstrap : Starting CalculateServiceBootstrap on XXX with PID 30434 (/Users/dive-in-spring-boot/target/classes started by XXX in /Users/dive-in-spring-boot)2019-06-14 11:10:17.851 INFO 30434 --- [ main] x.s.d.b.CalculateServiceBootstrap : The following profiles are active: Java72019-06-14 11:10:20.149 INFO 30434 --- [ main] x.s.d.b.CalculateServiceBootstrap : Started CalculateServiceBootstrap in 2.822 seconds (JVM running for 3.95)Java7实现sum:55]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java通过RMI实现手写RPC框架]]></title>
    <url>%2F2019%2F04%2F24%2FJava%E9%80%9A%E8%BF%87RMI%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99RPC%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[参考视频：https://www.bilibili.com/video/av30168877/?p=3参考文章：https://blog.csdn.net/shan9liang/article/details/8995023 1.RPC与RMI RMI(remote method invocation，面向对象的远程方法调用) RPC（remote procedure call，远程过程调用） RPC是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。在RPC中，当一个请求到达RPC服务器时，这个请求就包含了一个参数集和一个文本值，通常形成“classname.methodname”的形式。这就向RPC服务器表明，被请求的方法在为 “classname”的类中，名叫“methodname”。然后RPC服务器就去搜索与之相匹配的类和方法，并把它作为那种方法参数类型的输入。这里的参数类型是与RPC请求中的类型是匹配的。一旦匹配成功，这个方法就被调用了，其结果被编码后返回客户方。 Java RMI 指的是远程方法调用 (Remote Method Invocation)。它是一种机制，能够让在某个 Java 虚拟机上的对象调用另一个 Java 虚拟机中的对象上的方法。可以用此方法调用的任何对象必须实现该远程接口。 RPC与RMI区别于联系 RPC 跨语言，而 RMI只支持Java。 RMI 调用远程对象方法，允许方法返回 Java 对象以及基本数据类型，而RPC 不支持对象的概念，传送到 RPC 服务的消息由外部数据表示 (External Data Representation, XDR) 语言表示，这种语言抽象了字节序类和数据类型结构之间的差异。只有由 XDR 定义的数据类型才能被传递， 可以说 RMI 是面向对象方式的 Java RPC 。 在方法调用上，RMI中，远程接口使每个远程方法都具有方法签名。如果一个方法在服务器上执行，但是没有相匹配的签名被添加到这个远程接口上，那么这个新方法就不能被RMI客户方所调用。 代码实现：1.Server端方法接口与实现类HelloService.java 12345package server;public interface HelloService &#123; String sayHi(String name);//hi name&#125; HelloServiceImpl.java 12345678package server;public class HelloServiceImpl implements HelloService &#123; @Override public String sayHi(String name) &#123; return "Hi " + name; &#125;&#125; 2.远程调用的函数注册中心接口及其实现RegisterServerCenter.java 123456789101112package server;public interface RegisterServerCenter &#123; //服务启动 void start(); //服务终止 void stop(); //服务注册 void register(Class service, Class serviceImpl);&#125; RegisterServerCenterImpl.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package server;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;import java.net.InetSocketAddress;import java.net.ServerSocket;import java.net.Socket;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class RegisterServerCenterImpl implements RegisterServerCenter &#123; //以哈希表的形式存储注册的远程调用函数 private static Map&lt;String, Class&gt; serviceRegister = new HashMap&lt;&gt;(); //远程调用端口号 private static int port; //创建一个定长线程池，可控制线程最大并发数 //java.lang.Runtime.availableProcessors() 方法返回到Java虚拟机的可用的处理器数量 private static ExecutorService executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors()); //程序启动和关闭的标记 private static boolean isRunning = false; public RegisterServerCenterImpl(int port) &#123; this.port = port; &#125; @Override public void start() &#123; ServerSocket serverSocket = null; Socket socket; try &#123; serverSocket = new ServerSocket(); //与ServerSocket与指定端口号绑定 serverSocket.bind(new InetSocketAddress(port)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; isRunning = true; while (true) &#123; try &#123; System.out.println("---- start server ----"); //等待请求 socket = serverSocket.accept(); //启动线程完成请求 executorService.execute(new ServiceTask(socket)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; @Override public void stop() &#123; isRunning = false; //关闭线程池 executorService.shutdown(); &#125; @Override public void register(Class service, Class serviceImpl) &#123; //将可远程调用注册到map中 serviceRegister.put(service.getName(), serviceImpl); &#125; private static class ServiceTask implements Runnable &#123; private Socket socket; ServiceTask(Socket socket) &#123; this.socket = socket; &#125; @Override public void run() &#123; ObjectOutputStream outputStream = null; ObjectInputStream inputStream = null; try &#123; //接收到请求 inputStream = new ObjectInputStream(socket.getInputStream()); //获取类名、方法名、参数类型、参数值 String serviceName = inputStream.readUTF(); String methodName = inputStream.readUTF(); Class[] paramType = (Class[]) inputStream.readObject(); Object[] args = (Object[]) inputStream.readObject(); //通过服务注册表，获取类、获取方法，执行方法获取结果 Class serviceClass = serviceRegister.get(serviceName); Method method = serviceClass.getMethod(methodName, paramType); Object result = method.invoke(serviceClass.newInstance(), args); //返回结果 outputStream = new ObjectOutputStream(socket.getOutputStream()); outputStream.writeObject(result); &#125; catch (IOException | ClassNotFoundException | NoSuchMethodException | IllegalAccessException | InvocationTargetException | InstantiationException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (inputStream != null) &#123; inputStream.close(); &#125; if (outputStream != null) &#123; outputStream.close(); &#125; if (socket != null) &#123; socket.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 3.Client端代码Client.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package client;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.net.InetSocketAddress;import java.net.Socket;import java.util.Objects;public class Client &#123; /* * a:类加载器 ： 需要代理哪个类（例如HelloService接口）， * 就需要将HelloService的类加载器 传入第一个参数 * b:需要代理的对象，具备哪些方法 --接口 * 单继承，多实现 A implements B接口,c接口 * String str = new String(); * String[] str = new String[]&#123;"aaa","bb","cc"&#125; ; */ public static &lt;T&gt; T getRemoteProxyObj(Class service, InetSocketAddress inetSocketAddress) &#123; return (T) Proxy.newProxyInstance(service.getClassLoader(), new Class&lt;?&gt;[]&#123;service&#125;, new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Socket socket = new Socket(); ObjectOutputStream outputStream = null; ObjectInputStream inputStream = null; try &#123; //与端口建立连接 socket.connect(inetSocketAddress); outputStream = new ObjectOutputStream(socket.getOutputStream()); //按顺序将参数传给server端 outputStream.writeUTF(service.getName()); outputStream.writeUTF(method.getName()); outputStream.writeObject(method.getParameterTypes()); outputStream.writeObject(args); //获取返回的结果 inputStream = new ObjectInputStream(socket.getInputStream()); return inputStream.readObject(); &#125; finally &#123; Objects.requireNonNull(inputStream).close(); Objects.requireNonNull(outputStream).close(); socket.close(); &#125; &#125; &#125;); &#125;&#125; 4.服务端启动类RPCServerTest.java 1234567891011121314151617181920212223package test;import server.HelloService;import server.HelloServiceImpl;import server.RegisterServerCenter;import server.RegisterServerCenterImpl;public class RPCServerTest &#123; public static void main(String[] args) &#123; //开启一个线程 new Thread(new Runnable() &#123; @Override public void run() &#123; //服务中心 RegisterServerCenter server = new RegisterServerCenterImpl(9999); //将HelloService接口及实现类 注册到 服务中心 server.register(HelloService.class, HelloServiceImpl.class); server.start(); &#125; &#125;).start();//start() &#125;&#125; 5.客户端启动类ClientRPCTest.java 1234567891011121314package test;import client.Client;import server.HelloService;import java.net.InetSocketAddress;public class ClientRPCTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; //通过类反射机制类参数 HelloService service = Client.getRemoteProxyObj(Class.forName("server.HelloService"), new InetSocketAddress("127.0.0.1", 9999)); System.out.println((service.sayHi("zhangsan"))); &#125;&#125; 此时启动两个启动类便可实现RPC远程方法调用]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RPC</tag>
        <tag>多线程</tag>
        <tag>动态代理</tag>
        <tag>RMI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized及其优化]]></title>
    <url>%2F2019%2F04%2F21%2Fsynchronized%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[参考博文：http://www.cnblogs.com/wade-luffy/p/5969418.htmlhttp://www.cnblogs.com/kniught-ice/p/5189997.htmlhttps://www.zhihu.com/question/270564693 一、锁是什么？ 在java中对象都可以作为锁。普通同步方法：锁是当前实例对象。静态同步方法：锁是当前的class对象。同步方法块：锁是sychonized括号中的对象。 1、根据获取的锁的分类:获取对象锁和获取类锁获取对象锁的两种用法 同步代码块synchronized (this) , synchronized (类实例对象),锁是小括号()中的实例对象。 同步非静态方法 synchronized method , 锁是当前对象的实例对象。 获取类锁的两种用法 同步代码块synchronized (类.class), 锁是小括号()中的类对象(Class对象)。 同步静态方法 synchronized static method, 锁是当前对象的类对象(Class对象)。 2、对象锁和类锁的总结 有线程访问对象的同步代码块时 ,另外的线程可以访问该对象的非同步代码块; 若锁住的是同一个对象,一个线程在访问对象的同步代码块时,另一个访问对象的同步代码块的线程会被阻塞; 若锁住的是同一个对象,一个线程在访问对象的同步方法时,另一个访问对象同步方法的线程会被阻塞; 若锁住的是同一个对象,一个线程在访问对象的同步代码块时,另一个访问对象同步方法的线程会被阻塞,反之亦然; 同一个类的不同对象的对象锁互不干扰; 类锁由于也是一种特殊的对象锁,因此表现和上述1,2,3,4一致，而由于一个类只有一把对象锁,所以同一个类的不同对象使用类锁将会是同步的; 类锁和对象锁互不干扰。 3、其他锁相关知识点： jvm是基于进入和退出monitor对象来实现方法的同步和代码块的同步。 synchronized锁的不是代码，锁的是对象。 Java提供了synchronized关键字来支持内在锁。Synchronized关键字可以放在方法的前面、对象的前面、类的前面。 Java虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现，无论是显式同步(有明确的monitorenter和monitorexit指令,即同步代码块)还是隐式同步都是如此。 二、synchronized底层实现 1、synchronized底层语义原理 在JVM的规范中，有这么一些话:“在JVM中,每个对象和类在逻辑上都是和一个监视器相关联的，为了实现监视器的排他性监视能力，JVM为每一个对象和类都关联一个锁，锁住了一个对象，就是获得对象相关联的监视器” 在Java中，每个对象都会有一个monitor对象监视器。 Java虚拟机中的一个线程在它到达监视区域开始处的时候请求一个锁。JAVA程序中每一个监视区域都和一个对象引用相关联。某一线程占有这个对象的时候，先monitor的计数器是不是0，如果是0还没有线程占有，这个时候线程占有这个对象，并且对这个对象的monitor+1;如果不为0，表示这个线程已经被其他线程占有，这个线程等待。当线程释放占有权的时候，monitor-1; 同一线程可以对同一对象进行多次加锁，+1, +1，重入性,而一个锁就像一种任何时候只允许一个线程拥有的特权.一个线程可以允许多次对同一对象上锁.对于每一个对象来说,java虚拟机维护一个计数器,记录对象被加了多少次锁,没被锁的对象的计数器是0,线程每加锁一次，计数器就加1,每释放一次,计数器就减1.当计数器跳到0的时候,锁就被完全释放了. 在Java语言中，同步用的最多的地方可能是被synchronized修饰的同步方法。同步方法并不是由monitorenter和monitorexit指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的ACC_ SYNCHRONIZED标志来隐式实现的。下面先来了解一个概念Java对象头，这对深入理解synchronized实现原理非常关键。 2、下面我们通过代码进行理解当使用synchronized关键字对方法加上同步锁1234567891011public class SynchronizedDemo&#123; private static int m = 0; public synchronized static void synchronizedFun()&#123; try &#123; Thread.sleep(2); m++; &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 我们对字节码文件使用javap -v时，我们会发现反编译的文件中对方法添加synchronized关键字，会在该方法头的flags中存在一个ACC_SYNCHRONIZED（隐式同步） 123456789101112131415161718192021222324252627282930public static synchronized void synchronizedFun(); descriptor: ()V flags: ACC_PUBLIC, ACC_STATIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=0 0: ldc2_w #2 // long 2l 3: invokestatic #4 // Method java/lang/Thread.sleep:(J)V 6: getstatic #5 // Field m:I 9: iconst_1 10: iadd 11: putstatic #5 // Field m:I 14: goto 22 17: astore_0 18: aload_0 19: invokevirtual #7 // Method java/lang/InterruptedException.printStackTrace:()V 22: return Exception table: from to target type 0 14 17 Class java/lang/InterruptedException LineNumberTable: line 5: 0 line 6: 6 line 9: 14 line 7: 17 line 8: 18 line 10: 22 StackMapTable: number_of_entries = 2 frame_type = 81 /* same_locals_1_stack_item */ stack = [ class java/lang/InterruptedException ] frame_type = 4 /* same */ 当使用synchronized关键字对代码块加上同步锁123456789101112public class SynchronizedDemo&#123; public static void synchronizedFun()&#123; try &#123; synchronized(this)&#123; TimeUnit.MINUTES.sleep(2); m++; &#125; &#125; catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 我们对字节码文件使用javap -v时，我们会发现反编译的文件中发现第3行出现了monitorexit，第19行和第25行出现了monitorexit，对于为什么只有一个enter却有两个exit是因为，同步锁是原子性的，所以有一个为当失败时进行回滚的exit。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public void synchronizedFun(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: ldc2_w #2 // long 2l 7: invokestatic #4 // Method java/lang/Thread.sleep:(J)V 10: getstatic #5 // Field m:I 13: iconst_1 14: iadd 15: putstatic #5 // Field m:I 18: aload_1 19: monitorexit 20: goto 28 23: astore_2 24: aload_1 25: monitorexit 26: aload_2 27: athrow 28: goto 36 31: astore_1 32: aload_1 33: invokevirtual #7 // Method java/lang/InterruptedException.printStackTrace:()V 36: return Exception table: from to target type 4 20 23 any 23 26 23 any 0 28 31 Class java/lang/InterruptedException LineNumberTable: line 5: 0 line 6: 4 line 7: 10 line 8: 18 line 11: 28 line 9: 31 line 10: 32 line 12: 36 StackMapTable: number_of_entries = 4 frame_type = 255 /* full_frame */ offset_delta = 23 locals = [ class SynchronizedDemo, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 frame_type = 66 /* same_locals_1_stack_item */ stack = [ class java/lang/InterruptedException ] frame_type = 4 /* same */ 三、理解Java对象头与Monitor在JVM中，对象在内存中的布局分为三块区域:对象头、实例数据和对齐填充。 实例变量:存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据:由于虚拟机要求对象起始地址必须是8字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐，这点了解即可。 Java头对象，它实现synchronized的锁对象的基础，这点我们重点分析它，一般而言，synchronized使用的锁对象是存储在Java对象头里的，jvm中采用2个字来存储对象头(如果对象是数组则会分配3个字，多出来的1个字记录的是数组长度)，其主要结构是由Mark Word和Class Metadata Address组成，其结构说明如下表: 长度 内容 说明 32/64bit Mark Word 存储对象的hashCode或锁信息等 32/64bit Class Metadata Address 存储到对象类型数据的指针 32/64bit Array length 数组的长度(如果当前对象是数组) 1、对象头的Mark Word默认存储的对象的hashCode，分代年龄和锁标志位 锁状态 25bit 4bit 1bit是否是偏向锁 2bit锁标志位 无锁状态 对象的hashCode 对象分代年龄 0 01 2、 Mark Word的状态变化 四、synchronized的优化-偏向锁、轻量级锁、重量级锁 synchronized是java多线程编程的元老级人物，也被称为重量级锁 偏向锁和轻量级锁之所以会在性能上比重量级锁好是因为本质上偏向锁和轻量级锁仅仅使用了CAS 1、偏向锁：仅适用于锁没有竞争的情况，假设共享变量只有一个线程访问。如果有其他线程竞争锁，锁则会膨胀为轻量级锁。加锁方式： 初始时对象处于biasable状态，并且ThreadID为0即biasable&amp;unbiased状态。 当一个线程视图锁住处于biasable&amp;unbiased状态的对象时，通过一个CAS锁将自己的ThreadID放置到Mark Word中的相应位置，如果CAS操作成功则进入第三步，否则进入第四步。 当进入此步则表示所没有竞争，Object继续保持biasable状态，但是这是的ThreadID字段设置成了偏向锁所有者的ID，然后执行同步代码块。 当线程执行CAS获取偏向锁失败，表示在该锁对象上存在竞争并且这个时候另一个线程获得偏向锁的所有权。当到达全局安全点是获取偏向锁的线程被挂起，并将Object设置为LightWeight Lock（轻量级锁）状态并且Mark Word中的Lock Record指向刚才持有偏向锁线程的Monitor record（监视器记录），最后被阻塞在安全点的线程被释放，进入到轻量级锁的执行路径中，同时被撤销偏向锁的线程继续往下执行同步代码。 解锁过程： 偏向锁解锁过程很简单，只需要测试下是否Object上的偏向锁模式是否还存在，如果存在则解锁成功不需要任何其他额外的操作。 2、轻量级锁：适用于锁有多个竞争，但是在一个同步方法块周期中锁不存在竞争，如果在同步周期内有其他线程竞争锁，锁会膨胀为重量级锁。加锁过程： 线程在执行同步块之前，JVM会现在当前线程的栈帧中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word，然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获取锁，如果失败，则进行自旋获取锁，当自选获取锁仍然失败是，表示当前线程存在两条或两条以上线程竞争同一个锁，则轻量级锁膨胀成重量级锁。 解锁过程： 轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示同步过程已完成。如果失败则表示有其他线程尝试过获取锁，则要将释放锁的同时唤醒被挂起的线程。 3.重量级锁：竞争激烈的情况下使用重量级锁。 重量锁在JVM中又叫对象监视器（Monitor），它很像C中的Mutex，除了具备Mutex(0|1)互斥的功能，它还负责实现了Semaphore(信号量)的功能，也就是说它至少包含一个竞争锁的队列，和一个信号阻塞队列（wait队列），前者负责做互斥，后一个用于做线程同步。 4、通过JVM参数来修改锁状态 偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟-XX：BiasedLockingStartupDelay = 0。如果你确定自己应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁-XX:-UseBiasedLocking=false，那么默认会进入轻量级锁状态。 - 5、自旋锁 线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作。同时我们可以发现，很多对象锁的锁定状态只会持续很短的一段时间，例如整数的自加操作，在很短的时间内阻塞并唤醒线程显然不值得，为此引入了自旋锁。 所谓“自旋”，就是让线程去执行一个无意义的循环，循环结束后再去重新竞争锁，如果竞争不到继续循环，循环过程中线程会一直处于running状态，但是基于JVM的线程调度，会出让时间片，所以其他线程依旧有申请锁和释放锁的机会。 自旋锁省去了阻塞锁的时间空间（队列的维护等）开销，但是长时间自旋就变成了“忙式等待”，忙式等待显然还不如阻塞锁。所以自旋的次数一般控制在一个范围内，例如10,100等，在超出这个范围后，自旋锁会升级为阻塞锁。 6、自旋锁和轻量级锁的关系 轻量级锁是一种状态，而自旋锁是一种获取锁的方式。线程首先会通过CAS获取锁，失败后通过自旋锁来尝试获取锁，再失败锁就膨胀为重量级锁。所以轻量级锁状态下可能会有自旋锁的参与（cas将对象头的标记指向锁记录指针失败的时候） 五、偏向锁，轻量级锁，重量级锁对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距 如果线程间存在锁竞争，会带来额外的锁撤销的消耗 适用于只有一个线程访问同步块场景 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度 如果始终得不到锁竞争的线程使用自旋会消耗CPU 追求响应时间,锁占用时间很短 重量级锁 线程竞争不使用自旋，不会消耗CPU 线程阻塞，响应时间缓慢 追求吞吐量,锁占用时间较长]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java多线程</tag>
        <tag>锁</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized,lock和volatile的区别]]></title>
    <url>%2F2019%2F04%2F20%2Fsynchronized%2Clock%E5%92%8Cvolatile%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[参考文章：https://www.jianshu.com/p/2344a3e68ca9https://cloud.tencent.com/developer/article/1369358 1、synchronizedJava语言的关键字，可用来给对象和方法或者代码块加锁，当它锁定一个方法或者一个代码块的时候，同一时刻最多只有一个线程执行这段代码。当两个并发线程访问同一个对象object中的这个加锁同步代码块时，一个时间内只能有一个线程得到执行。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。然而，当一个线程访问object的一个加锁代码块时，另一个线程仍然可以访问该object中的非加锁代码块。 2、Locksynchronized是Java语言的关键字，是内置特性，而ReentrantLock是一个类(实现Lock接口的类)，通过该类可以实现线程的同步。关于Lock的具体解析可以点击传送门 3、volatilevolatile是一个类型修饰符（type specifier）。它是被设计用来修饰被不同线程访问和修改的变量。确保本条指令不会因编译器的优化而省略，且要求每次直接读值。 4、synchronized, lock和volatile区别（可见性、原子性、有序性） 属性 Synchronized lock volatile 解释 可见性 √ √ √ 变量被操作之后，能够快速写入内存，并提醒其他线程重读，加锁是通过一个一个执行保证了可见性。 原子性 √ √ × 做的过程中，不要有相关的来打扰，不相关的我们也不关心，加锁是通过一个一个执行保证了流程不会被相关的打扰。 有序性 √ √ √ 在Java内存模型中，允许编译器和处理器对指令进行重排序，但是重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 5、synchronized与lock区别 类别 synchronized Lock 存在层次 Java的关键字，在jvm层面上 是一个类 锁的释放 1、以获取锁的线程执行完同步代码，释放锁 2、线程执行发生异常，jvm会让线程释放锁 在finally中必须释放锁，不然容易造成线程死锁 锁的获取 假设A线程获得锁，B线程等待。如果A线程阻塞，B线程会一直等待 分情况而定，Lock有多个锁获取的方式，具体下面会说道，大致就是可以尝试获得锁，线程可以不用一直等待 锁状态 无法判断 可以判断 锁类型 可重入 不可中断 非公平 可重入 可判断 可公平（两者皆可） 性能 少量同步 大量同步]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java多线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中Lock接口解析]]></title>
    <url>%2F2019%2F04%2F20%2FJava%E4%B8%ADLock%E6%8E%A5%E5%8F%A3%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文非原创，为转载文章，原文链接：https://www.jianshu.com/p/2344a3e68ca9 一、Lock synchronized是Java语言的关键字，是内置特性，而ReentrantLock是一个类(实现Lock接口的类)，通过该类可以实现线程的同步。Lock是一个接口，源码很简单，主要是声明了四个方法： 12345678public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long var1, TimeUnit var3) throws InterruptedException; void unlock(); Condition newCondition();&#125; 1.Lock一般的使用如下：123456789Lock lock= ...;//获取锁lock.lock();try&#123; //处理任务&#125;catch(Exception e)&#123;&#125;finally&#123; lock.unlock();//释放锁&#125; lock()、tryLock()、tryLock(long time, TimeUnit unit)和lockInterruptibly()是用来获取锁的，unLock()方法是用来释放锁的，其放在finally块里执行，可以保证锁一定被释放，newCondition方法下面会做介绍（通过该方法可以生成一个Condition对象，而Condition是一个多线程间协调通信的工具类）。 2.Lock接口的主要方法介绍： lock()：获取不到锁就不罢休，否则线程一直处于block状态。 tryLock()：尝试性地获取锁，不管有没有获取到都马上返回，拿到锁就返回true，不然就返回false 。 tryLock(long time, TimeUnit unit)：如果获取不到锁，就等待一段时间，超时返回false。 lockInterruptibly()：该方法稍微难理解一些，在说该方法之前，先说说线程的中断机制，每个线程都有一个中断标志，不过这里要分两种情况说明： 线程在sleep、wait或者join， 这个时候如果有别的线程调用该线程的 interrupt（）方法，此线程会被唤醒并被要求处理InterruptedException。 如果线程处在运行状态， 则在调用该线程的interrupt（）方法时，不会响应该中断。lockInterruptibly()和上面的第一种情况是一样的， 线程在获取锁被阻塞时，如果调用lockInterruptibly()方法，该线程会被唤醒并被要求处理InterruptedException。下面给出一个响应中断的简单例子： 12345678910111213141516171819202122232425262728public class Test&#123; public static void main(String[] args)&#123; MyRunnable myRunnable = new Test().new MyRunnable(); Thread thread1 = new Thread(myRunnable,"thread1"); Thread thread2 = new Thread(myRunnable,"thread2"); thread1.start(); thread2.start(); thread2.interrupt(); &#125; public class MyRunnable implements Runnable&#123; private Lock lock=new ReentrantLock(); @Override public synchronized void run() &#123; try&#123; lock.lockInterruptibly(); System.out.println(Thread.currentThread().getName() +"获取了锁"); Thread.sleep(5000); &#125;catch(InterruptedException e) &#123; e.printStackTrace(); System.out.println(Thread.currentThread().getName() +"响应中断"); &#125;finally&#123; lock.unlock(); System.out.println(Thread.currentThread().getName() +"释放了锁"); &#125; &#125; &#125;&#125; 执行结果如下： 123thread1获取了锁thread1释放了锁thread2响应中断 thread2在响应中断后，在finally块里执行unlock方法时，会抛出java.lang.IllegalMonitorStateException异常（因为thread2并没有获取到锁，只是在等待获取锁的时候响应了中断，这时再释放锁就会抛出异常）。 3.newCondition()方法上面简单介绍了ReentrantLock的使用，下面具体介绍使用ReentrantLock的中的newCondition方法实现一个生产者消费者的例子。生产者、消费者例子：两个线程A、B，A生产牙刷并将其放到一个缓冲队列中，B从缓冲队列中购买（消费）牙刷（说明：缓冲队列的大小是有限制的），这样就会出现如下两种情况。 当缓冲队列已满时，A并不能再生产牙刷，只能等B从缓冲队列购买牙刷； 当缓冲队列为空时，B不能再从缓冲队列中购买牙刷，只能等A生产牙刷放到缓冲队列后才能购买。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798public class ToothBrushDemo &#123; public static void main(String[] args) &#123; final ToothBrushBusiness toothBrushBusiness = new ToothBrushDemo().new ToothBrushBusiness(); new Thread(new Runnable() &#123; @Override public void run() &#123; executeRunnable(toothBrushBusiness, true); &#125; &#125;, "牙刷生产者1").start(); new Thread(new Runnable() &#123; @Override public void run() &#123; executeRunnable(toothBrushBusiness, false); &#125; &#125;, "牙刷消费者1").start(); &#125; //循环执行50次 public static void executeRunnable(ToothBrushBusiness toothBrushBusiness, boolean isProducer) &#123; for (int i = 0; i &lt; 50; i++) &#123; if (isProducer) &#123; toothBrushBusiness.produceToothBrush(); &#125; else &#123; toothBrushBusiness.consumeToothBrush(); &#125; &#125; &#125; public class ToothBrushBusiness &#123; //定义一个大小为10的牙刷缓冲队列 private GoodQueue&lt;ToothBrush&gt; toothBrushQueue = new GoodQueue&lt;ToothBrush&gt;(new ToothBrush[10]); private int number = 1; private final ReentrantLock lock = new ReentrantLock(); private final Condition notEmpty = lock.newCondition(); private final Condition notFull = lock.newCondition(); public ToothBrushBusiness() &#123; &#125; //生产牙刷 public void produceToothBrush() &#123; lock.lock(); try &#123; //牙刷缓冲队列已满,则生产牙刷线程等待 while (toothBrushQueue.isFull()) &#123; notFull.await(); &#125; ToothBrush toothBrush = new ToothBrush(number); toothBrushQueue.enQueue(toothBrush); System.out.println("生产: " + toothBrush.toString()); number++; //牙刷缓冲队列加入牙刷后,唤醒消费牙刷线程 notEmpty.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (GoodQueueException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; //消费牙刷 public void consumeToothBrush() &#123; lock.lock(); try &#123; //牙刷缓冲队列为空,则消费牙刷线程等待 while (toothBrushQueue.isEmpty()) &#123; notEmpty.await(); &#125; ToothBrush toothBrush = toothBrushQueue.deQueue(); System.out.println("消费: " + toothBrush.toString()); //从牙刷缓冲队列取出牙刷后,唤醒生产牙刷线程 notFull.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (GoodQueueException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; public class ToothBrush &#123; private int number; public ToothBrush(int number) &#123; this.number = number; &#125; @Override public String toString() &#123; return "牙刷编号&#123;" + "number=" + number + '&#125;'; &#125; &#125;&#125; 这里缓冲队列的大小设成了10，定义了一个可重入锁lock，两个状态标记对象notEmpty，notFull，分别用来标记缓冲队列是否为空，是否已满。 当缓冲队列已满时，调用notFull.await方法用来阻塞生产牙刷线程。 当缓冲队列为空时，调用notEmpty.await方法用来阻塞购买牙刷线程。 notEmpty.signal用来唤醒消费牙刷线程，notFull.signal用来唤醒生产牙刷线程。 4.Object和Conditon对应关系如下： Object Condition 休眠 wait await 唤醒特定线程 notify signal 唤醒所有线程 notifyAll signalAll 对于同一个锁，我们可以创建多个Condition，就是多个监视器的意思。在不同的情况下使用不同的Condition，Condition是被绑定到Lock上的，要创建一个Lock的Condition必须用newCondition()方法。 二、ReadWriteLockReentrantLock（可重入锁）是唯一实现了Lock接口的类，并且ReentrantLock提供了更多的方法。 synchronized和ReentrantLock都是可重入锁，可重入性举个简单的例子，当一个线程执行到某个synchronized方法时，比如说method1，而在method1中会调用另外一个synchronized方法method2，此时线程不必重新去申请锁，而是可以直接执行方法method2。 ReentrantReadWriteLock简介上面的响应中断的例子已经地使用到了ReentrantLock，下面来介绍另外一种锁，可重入读写锁ReentrantReadWriteLock，该类实现了ReadWriteLock接口，该接口的源码如下： 1234public interface ReadWriteLock &#123; Lock readLock(); Lock writeLock();&#125; ReentrantReadWriteLock会使用两把锁来解决问题，一个读锁，一个写锁。 线程进入读锁的前提条件： 没有其他线程的写锁 没有写请求，或者有写请求但调用线程和持有锁的线程是同一个线程 进入写锁的前提条件： 没有其他线程的读锁 没有其他线程的写锁 需要提前了解的概念： 锁降级：从写锁变成读锁； 锁升级：从读锁变成写锁。 读锁是可以被多线程共享的，写锁是单线程独占的。也就是说写锁的并发限制比读锁高，这可能就是升级/降级名称的来源。 ReadWriteLock接口只有获取读锁和写锁的方法，而ReentrantReadWriteLock是实现了ReadWriteLock接口，接着对其应用场景做简单介绍。 应用场景：假设一个共享的文件，其属性是可读，如果某个时间有100个线程在同时读取该文件，如果通过synchronized或者Lock来实现线程的同步访问，那么有个问题来了，当这100个线程的某个线程获取到了锁后，其它的线程都要等该线程释放了锁才能进行读操作，这样就会造成系统资源和时间极大的浪费，而ReentrantReadWriteLock正好解决了这个问题。下面给一个简单的例子，并根据代码以及输出结果做简要说明： 123456789101112131415161718192021222324252627282930public class Test &#123; public static void main(String[] args) &#123; MyRunnable myRunnable = newTest().new MyRunnable(); Thread thread1 = new Thread(myRunnable, "thread1"); Thread thread2 = new Thread(myRunnable, "thread2"); Thread thread3 = new Thread(myRunnable, "thread3"); thread1.start(); thread2.start(); thread3.start(); &#125; public class MyRunnable implements Runnable &#123; private ReadLock lock = new ReentrantReadWriteLock().readLock(); @Override public synchronized void run() &#123; try &#123; lock.lock(); int i = 0; while (i &lt; 5) &#123; System.out.println(Thread.currentThread().getName() + "正在进行读操作"); i++; &#125; System.out.println(Thread.currentThread().getName() + "读操作完毕"); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125;&#125; 输出结果： 123456789101112131415161718thread1正在进行读操作thread1正在进行读操作thread1正在进行读操作thread1正在进行读操作thread1正在进行读操作thread1读操作完毕thread3正在进行读操作thread3正在进行读操作thread3正在进行读操作thread3正在进行读操作thread3正在进行读操作thread3读操作完毕thread2正在进行读操作thread2正在进行读操作thread2正在进行读操作thread2正在进行读操作thread2正在进行读操作thread2读操作完毕 从输出结果可以看出，三个线程并没有交替输出，这是因为这里只是读取了5次，但将读取次数i的值改成一个较大的数值如100000时，输出结果就会交替的出现。 看了好多人的博文，在我看来，这个Lock的用处就是可以细化加锁和解锁的操作，使锁操作更加直观，可控]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java多线程</tag>
        <tag>锁</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今天我也要当一个玄学boy]]></title>
    <url>%2F2019%2F04%2F19%2F%E4%BB%8A%E5%A4%A9%E6%88%91%E4%B9%9F%E8%A6%81%E5%BD%93%E4%B8%80%E4%B8%AA%E7%8E%84%E5%AD%A6boy%2F</url>
    <content type="text"><![CDATA[今天在github上看到了一个优秀Java进阶知识点项目，顺带竟然还发现了一个优秀的歌曲《我的offer在哪里》，最近各种投递简历、笔试、面试，搞得头晕脑胀，身心俱疲，看来我也要玄学一下，顺便我博客的歌曲也正好换成这个，毕竟玄学就是第一生产力(ಡωಡ)。传送门一并送上~ 互联网 Java 工程师进阶知识完全扫盲 滚过来学习 劳(quan)逸(kao)结(xuan)合(xue) 网易云插件如下：1&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=1321616516&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt; 下面是歌词：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788每天起床第一句 先给自己打个气每次刷新查成绩 都要说声保佑me魔镜魔镜告诉我 我的offer在哪里努力 我要努力 我要变成收割机offer offer我要变成收割机offer offer为了拿下BAT 天天提着一口气大厂小厂投简历 刷光面经笔试题天生我才难自弃 可惜面试都悲剧努力 我要努力 我要变成收割机Wow在哪里在哪里在哪在哪里在哪里在哪在哪里在哪里在哪在哪里在哪里在哪我的offer在哪里燃烧我的卡路里拜拜 二次元 综艺直播动作片 言情玄幻宫斗剧拿走拿走别客气拜拜 铂金一 戒掉农药戒吃鸡 通宵开黑玩游戏别再熬夜伤身体来来 沉住气 专业课本重拾起 牛客网上刷真题保温杯里泡枸杞来来 深呼吸 对照镜子做练习单面群面全模拟 不拿offer不放弃为了拿下BAT 天天提着一口气大厂小厂投简历 刷光面经笔试题天生我才难自弃 可惜面试都悲剧努力 我要努力 我要变成收割机Wow在哪里在哪里在哪在哪里在哪里在哪在哪里在哪里在哪在哪里在哪里在哪我的offer在哪里燃烧我的卡路里拜拜 二次元 综艺直播动作片 言情玄幻宫斗剧拿走拿走别客气拜拜 铂金一 戒掉农药戒吃鸡 通宵开黑玩游戏别再熬夜伤身体来来 沉住气 专业课本重拾起 牛客网上刷真题保温杯里泡枸杞来来 深呼吸 对照镜子做练习单面群面全模拟 不拿offer不放弃奇了怪了 小的时候明明是妈妈说考上大学就好惹 没烦恼直到熟悉的大学的寝室都住不上了 原来毕了业没人要 才烦恼希望 offer钱是多哒HR超nice哒不如跟着节奏没在怕的 努努力别让校招季卡住你 卡住你不拿offer不放弃燃烧我的卡路里拜拜 二次元 综艺直播动作片 言情玄幻宫斗剧拿走拿走别客气拜拜 铂金一 戒掉农药戒吃鸡 通宵开黑玩游戏别再熬夜伤身体来来 沉住气 专业课本重拾起 牛客网上刷真题保温杯里泡枸杞来来 深呼吸 对照镜子做练习单面群面全模拟 不拿offer不放弃不放弃燃烧我的卡路里不放弃燃烧我的卡路里我要变成收割机 文章刚写完就收到了七牛的笔试通知。。。。。看来玄学有望啊，哈哈哈哈哈哈，加油++，未来可期。]]></content>
      <categories>
        <category>企图玄学</category>
      </categories>
      <tags>
        <tag>企图玄学</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见面试题的基础总结（JVM篇）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9A%84%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88JVM%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于这些内容都是比较早之前进行的整理的，所以有的部分是参考了他人的博文，但是由于是之前找的，所以具体的博文链接找不到了，如果原博主看到这个文章或者有人知道其中部分内容的原博文，请与我联系，我将加上原链接，谢谢 1、为什么Java一次编译，到处可运行 因为只需要将java文件编译成字节码文件，在运行平台只要有JVM虚拟机就可以运行。这样Java在不同的平台也不需要重新编译，只需要虚拟机把字节码文件转换成具体平台的机器指令。 2、JVM是如何加载文件的 首先对于字节码文件，使用Class Loader将class文件记载到内存中。然后使用Exction Engine对命令进行解析。对于使用了不同开发文件的原生库我们可以使用Native interface来为Java所用。 3、Java反射机制 对于任何一个类，我们都可以知道这个类的所有属性和方法，对弈任何一个对象我们都能调用他的任意方法和属性。这种动态获取信息以及动态调用对象的方法就是成为Java语言的反射机制。 4、Class Loader Class Loader在Java中有着非常重要的作用，它主要工作在Class装载的加载过程，主要作用是从系统外获取Class的二进制数据流。所有的class都是由他来加载很系统，然后交给虚拟机进行连接初始化等操作。过程为加载-链接（校验-准备-解析）-初始化 5、loadClass和forName区别 对于loadClass没有链接，而forName获取的是已经初始化的。优点就是没有链接初始化可以减少资源浪费，需要的时候再进行初始化。 6、JVM内存模型 主要分为程序计数器，虚拟机栈，本地方法栈 这几个是线程私有的。而堆（包含常量池，数组和类对象）以及MetaSpace（类加载信息）是线程共享的。 程序计数器：记录当前线程执行的字节码行号的指示器，通过改变这个值来选取下一条需要执行的字节码指令。和线程是一对一的关系即“线程私有”的，对于Native方法计数为Undefine Java虚拟机栈：包含Java方法执行的内存模型，包含多个栈帧。每个栈帧都含有局部变量表，操作栈还有动态连接，返回地址等数据 本地方法栈：与虚拟机栈相似，主要作用是标注了native方法 局部变量存储的是方法执行过程中的所有变量。操作数栈：入栈、出栈、复制、交换、产生消费变量 7、递归为什么会引发StackOverFlowError异常 多次递归一直入栈超出虚拟栈深度 8、JVM三大性能调优参数 -Xss规定虚拟栈大大小 -Xms堆的初始值 -Xmx堆能达到的最大值 9、Java内存模型中的堆和栈的区别-内存分配策略 静态存储：编译时确定每个数据目标在运行时的存储空间需求 栈式存储：数据区需求在编译时未知，运行时模块入口确定 堆式存储：编译时或者运行时入口都无法确认，动态分配 10、堆和栈的关系与区别 联系: 引用对象或者数组时，栈中变量保存的是堆中目标的首地址 区别: 管理方式:栈自动释放，堆需要GC 空间大小:栈比堆小 碎片相关:栈产生的随便远小于堆 分配方式:栈支持动态和静态分配，而堆仅支持动态分配 效率:因为底层数据结构不同所以栈效率高于堆 11、元空间、堆、栈、线程独占部分的联系——————内存角度123456789101112131415public class HelloWorld&#123; private String name; public void sayHello()&#123; System.out.println("Hello" + name); &#125; public void setName(String name)&#123; this.name = name; &#125; public static void main(String[] args) &#123; int a = 1; HelloWorld hw = new HelloWorld(); hw.setName("test"); hw.sayHello(); &#125;&#125; 引用参数本地变量局部变量]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见面试题的基础总结（Java多线程篇）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9A%84%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于这些内容都是比较早之前进行的整理的，所以有的部分是参考了他人的博文，但是由于是之前找的，所以具体的博文链接找不到了，如果原博主看到这个文章或者有人知道其中部分内容的原博文，请与我联系，我将加上原链接，谢谢 1、进程和线程的区别 运行一个程序会产生一个进程，进程包括至少一个线程 每个进程对应一个JVM实例，多个线程则是共享JVM的堆 Java采用单线程编程模型，程序会自动创建主线程 主线程可以创建子线程，原则上要后于子线程完成执行 2、Thead和Runnable什么关系Thread是实现了Runnable接口的类，使得run支持多线程应为类的单一继承原则，所以推荐多使用Runnable接口 3、如何给run（）传参 构造函数传参 成员变量传参 比如常见的set方法 回调函数传参参考：https://blog.csdn.net/saycheesenn/article/details/52956331 4、如何实现处理线程的返回值 主线程等待法 主线程等待子线程执行完毕 使用join（）阻塞当前线程等待执行完毕 通过Callable接口实现：通过FutureTask或者线程池获取 如果使用FutureTask可以使用isDone方法可以判断是否执行完成 对于线程池 5、线程的状态： 新建：创建后还未启动的线程 运行：包含Runnable和Ready状态 无限期等待：需要显式唤醒 例如没有设置Timeout参数的Object.wait() 和 Thread.join() 期限等待：在一定时间后会由系统自动唤醒 例如Thread.sleep()，设置了参数的Object.wait()和 Thread.join() 阻塞状态：等待获取排它锁 结束：已终止线程的状态，线程已经结束执行 6、sleep和wait区别 sleep是Thread类的方法，wait是Object类的方法 sleep可以在任何地方使用 wait只能在synchronized方法或者synchronized块中使用 wait是通知当前线程等待然后释放对象锁，notify也是，所以如果没有获取对象锁就是没有意义的了 7、notify和notifyAll的区别首先建立两个概念： 锁池：假设对象A已经拥有了某个对象的锁，而其他线程想要调用这个对象的synchronize方法，所以其他线程会进入阻塞状态进入锁池等待锁的释放 等待池：假设线程A调用了某个对象的wait方法，线程A就会释放当前的锁然后进入等待池，进入等待池的线程不会去竞争锁 notifyAll会让所有处于等待池的线程全部进入锁池去竞争获取锁的机会 notify只会随机选取一个处于等待池中的线程进入锁池去竞争获取锁得机会 8、Yield 当调用Thread.yield方法函数时，会给线程调度器一个当前线程愿意让出CPU使用的暗示，但是线程调度器可能会忽略这个暗示 9、如何中断线程 通过调用stop（）方法停止线程，通过suspend和resume方法（已经弃用） 调用interrupt()，通知线程应该中断了 如果线程处于被阻塞状态，那么线程将立即退出被阻塞状态，并抛出一个InterruptedException异常 如果线程处于正常活动状态，那么该线程的中断标志位将设置为true，被设置的中断标志位的线程将继续正常运行不受影响。 正常运行的任务是，经常检查本线程的中断标志位，如果被设置了中断标志就自行停止线程 如果线程处于活动状态，那么僵该线程的中断标志位设置为true，将设置中断标志的线程将继续正常运行，不受影响。 10、线程状态间的转换]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见面试题的基础总结（JavaGC篇）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9A%84%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88JavaGC%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于这些内容都是比较早之前进行的整理的，所以有的部分是参考了他人的博文，但是由于是之前找的，所以具体的博文链接找不到了，如果原博主看到这个文章或者有人知道其中部分内容的原博文，请与我联系，我将加上原链接，谢谢 1、Java的垃圾回收判断对象是否为垃圾有两种算法。 引用计数算法，本算法就是通过引用计数器来统计，当被引用+1，完成引用-1.任何对象实例引用为0时就可以当做垃圾收集，优点是执行效率高，问题是循环引用会出现问题。 可达性分析算法，通过判断对象的引用链来决定对象是否可达。如果不可达便被回收。可作为GC ROOT的对象有虚拟机栈中引用的对象、方法区中的常量引用的对象，方法区中的类静态属性引用的对象还有Native方法引用的对象，以及活跃线程的引用对象。 2、常见的垃圾回收算法 标签-清除算法对存活对象进行标记，清除是从头到尾进行线性遍历，回收不可达的对象内存。问题时容易碎片化。 复制算法分为对象面和空闲面。存活的对象从对象面复制到空闲面，同时将对象面的清空。优点是解决了碎片化的问题，顺序分配内存简单高效，适用于对象存活率低的场景。年轻代因为存活较少，所以适合于用复制算法 标记-整理算法标记就是从GC ROOT进行扫描对存活对象进行标记，然后移动所有存活的对象，且按照内存地址依次有序排列，然后将末端的内存地址以后的内存都进行回收。有点是避免了内存的不连续行，且不用进行两块内存互换，适用于存活率高德场景。 分代收集算法分代收集算法就是多种算法的集合，其中年轻代使用复制算法，老年代使用标记整理算法。JDK7之前有新生代，老年代和永久代。JDK7之后只有新生代和老年代没有了新生代。 年轻代分为eden区和两个servivor区，当进行垃圾回收时，存活的对象计数会加一，同时复制到servivor中的from区，然后将eden区清空，当第二次进行GC时eden区同理，但是servivor区仍然会计数加1然后复制到另一个servivor中。直到某个存活对象的技术超过某个值然后该对象进入老年代。 老年代常用的是标记-清除和标记整理算法,其中新生代老年代一半了比例为1:2 3、对于新生代如何晋升老年代有三种情况： 经历了多次MinorGC后仍然存活 survivor区中存放不下大对象 新生成的大对象 4、常见得到调优参数 -XX:SurvivorRatio : Eden和Survivor的比值,默认8 : 1 -XX:NewRatio: 老年代和年轻代内存大小的比例 -XX:MaxTenuringThreshold:对象从年轻代晋升到老生代经过GC次数的最大阈值 5、触发GC的条件为老年代的FULL GC还有MajorGC比年轻代的MinorGC慢，但是执行频率低。 老年代空间不足 CMS GC后老年代不足 年轻代晋升老年代的空间大于剩余空间 调用了System.gs() 6、常见垃圾收集器 ParNew收集器( -XX:+UseParNewGC ,复制算法) 多线程收集,其余的行为、特点和Serial收集器一样 单核执行效率不如Serial ,在多核下执行才有优势 CMS收集器(标记清除算法) 垃圾回收线程几乎可以与用户线程同时工作，对停顿比较敏感，并且可以提供更强的硬件，如果JVM中存在较多存在时间较长的对象，更适合使用CMS。其中初始标记和重新标记是需要s-t-w 初始标记：进入s-t-w 并发标记：并发追溯标记程序不会停顿 并发预清理：查找并发标记阶段新生代晋升老年代的对象 重新标记：暂停虚拟机扫描CMS堆中的剩余对象 并发清理，清理对象，程序不会停顿 并发重置：重置CMS收集器的数据结构 G1收集器 Garbage First收集器(复制+标记整理算法) 本收集器是将整个Java堆内存划分为多个大小相等的region，然后新生代和老年代不进行了物理隔离。 特点： 并发和并行 分带收集 空间整合 可预测的停顿 7、强引用，软引用，弱引用，虚引用 强引用是最常见的引用如： Object object = new Object（）；即使内存不足也不会被回收，但是我们可以通过将对象设置为null来弱化引用，使其被回收。 软引用是对象处在有用但是非必须的状态，只有内存不足时才会被回收可以实现高速缓存，使用方法如下：SoftReference softStr = new SoftReference(str); 弱引用比软引用更弱一下，然后GC时会被回收，适用于偶尔使用但是不影响垃圾收集的对象。 8、其他的常见概念Stop-the-world 在GC过程中会有Stop-the-world也就是执行GC时虚拟机会停止应用程序的执行，而且是在所有GC算法中都会发生，多数的GC通过减少s-t-w发生时间来提高程序性能 SafePoint 分析过程中对象引用关系不会改变的点，常见的点有方法调用，循环跳转，异常跳转。安全点数量应该适中，过多过少都不好。 JVM有两种运行模式Server和Client 常见两个模式server启动慢但是启动后性能更好，因为server底层是一个更大的虚拟机,可以使用java -version来查询是哪一种模式下的 Object的finalize方法 Object的finalize方法是不确定的，不一定会执行。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Java垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见面试题的基础总结（数据库篇）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9A%84%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于这些内容都是比较早之前进行的整理的，所以有的部分是参考了他人的博文，但是由于是之前找的，所以具体的博文链接找不到了，如果原博主看到这个文章或者有人知道其中部分内容的原博文，请与我联系，我将加上原链接，谢谢 1、为什么要使用索引 为了快速查询数据 2、如何创建索引 唯一索引CREATE UNIQUE INDEX 索引名称 ON 表名称 (列名称) 简单索引CREATE INDEX 索引名称 ON 表名称 (列名称) 3、普通索引，唯一索引，主键索引，全文索引，组合索引 一个表只能有一个主键索引，可以有多个唯一索引； 主键索引一定是唯一索引， 唯一索引不是主键索引；。 普通索引：最基本的索引，没有任何限制 唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。 主键索引：它是一种特殊的唯一索引，不允许有空值。 全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。 组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。 4、密集索引，稀疏索引 密集索引：每个搜索码值都对应一个索引值 稀疏索引：只为某些值建立索引 5、什么是最左前缀原则12345678CREATE TABLE `user2` ( `userid` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(20) NOT NULL DEFAULT '', `password` varchar(20) NOT NULL DEFAULT '', `usertype` varchar(20) NOT NULL DEFAULT '', PRIMARY KEY (`userid`), KEY `a_b_c_index` (`username`,`password`,`usertype`)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8; 上表中有一个联合索引，下面开始验证最左匹配原则。 当存在username时会使用索引查询：explain select * from user2 where username = &#39;1&#39; and password = &#39;1&#39;; 当没有username时，不会使用索引查询：explain select * from user2 where password = &#39;1&#39;; 当有username，但顺序乱序时也可以使用索引：explain select * from user2 where password = &#39;1&#39; and username = &#39;1&#39;; 在最左匹配原则中，有如下说明： 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 最左前缀的成因也不难理解：就是生成索引是先按照第一个索引，再按照第二个索引。如果只有第二个条件，此时第二个条件的索引是无序的。 6、索引失效的几种情况 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引 like查询以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 7、主键，复合主键，联合主键 主键是能唯一表示一条数据的字段 复合主键是多个字段的组合 联合主键是多个表之间的通过给的数据表主键然后创建的联合主键 8、常见索引的数据结构二叉搜索树 B-treeb+-tree Hash 9、b+树和Hash优缺点 B+树的磁盘读写代价更低 B +树的查询效率更加稳定 B+树更有利于对数据库的扫描 Hash仅仅能满足“=”, “IN”, Hash不能使用范围查询 Hash无法被用来避免数据的排序操作 Hash不能利用部分索引键查询 Hash不能避免表扫描 Hash遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高 10、b数和b+数区别 b数每个节点都存储key和data，叶子节点的指针为null b+数只有叶子节点存储data，叶子节点包含了这个数的所有键值，叶子节点不存值 11、如何定位慢查询 根据慢日志查询SQL语句 使用 show variables like ‘%query%’ 可以将slow_query_log选项开启，同时可以查看慢日志的路径 使用Explain关键字，即在普通SQL语句前加上Explain，可通过查看extra项和type项看是否使用到了索引 最左匹配原则 12、索引是建立的越多越好吗 数据量小不需要索引 经常变更同时也需要维护索引，意味着更多的维护成本 更多索引以为这更多的空间 13、InnoDB和MyISAM区别以及各自适合场景 InnoDb数据索引在一个文件中。MyISAM数据与索引为两个文件 MyISAM默认用的是表级锁,不支持行级锁 InnoDB默认用的是行级锁,也支持表级锁 MyISAM适合于频繁执行全表count语句，同时对增删改频率不高，查询频繁，没有事务 InnoDB适合于增删改查都相当频繁，同时可靠性要求比较高，要求支持事务 14、数据库不同隔离级别下遇到的问题本问题可参考本博客之前的文章：传送门 15、事务的ACID 原子性 一致性 隔离性 持久性 16、数据库的优化读写分离 读写分离从字面意思就可以理解，就是把对数据库的读操作和写操作分离开。读写分离在网站发展初期可以一定程度上缓解读写并发时产生锁的问题，将读写压力分担到多台服务器上，通常用于读远大于写的场景。 读写分离的基本原理是让主数据库处理事务性增、改、删操作（INSERT、UPDATE、DELETE），而从数据库处理SELECT查询操作。数据库复制被用来把事务性操作导致的变更同步到集群中的从数据库。单表的数据量限制，当单表数据量到一定条数之后数据库性能会显著下降。数据多了之后，对数据库的读、写就会很多。分库减少单台数据库的压力。 主从复制 主从复制，是用来建立一个和主数据库完全一样的数据库环境，称为从数据库； 主数据库一般是实时的业务数据库，从数据库的作用和使用场合一般有几个： 一是作为后备数据库，主数据库服务器故障后，可切换到从数据库继续工作； 二是可在从数据库作备份、数据统计等工作，这样不影响主数据库的性能； 17、InnoDB如何手动设置共享锁和排它锁 共享锁也叫读锁，简称S锁，原理：一个事务获取了一个数据行的共享锁，其他事务能获得该行对应的共享锁，但不能获得排他锁，即一个事务在读取一个数据行的时候，其他事务也可以读，但不能对该数据行进行增删改。 排他锁也叫写锁，简称x锁，原理：一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁（排他锁或者共享锁），即一个事务在读取一个数据行的时候，其他事务不能对该数据行进行增删改查。如何设置共享锁和排他锁？ 设置共享锁：SELECT ... LOCK IN SHARE MODE; 设置排他锁：SELECT ... FOR UPDATE; 18、redis基础数据结构 redis相关问题可参考之前的一些文章： redis如何实现异步队列 redis如何实现分布式锁 string：是一个可变的字节数组 set name zhangsan（增加） get name（删除） strlen name（获取长度） getrange name 1 10（获取1-10的字符） append name ishahaha（追加字符，此时name 为 zhangsanis） decrby name 100（name的值减100） Incrby name 100（name的值加100） List：是一个列表数据结构，且是双向链表常见操作：rpop，rpush，lpop，lpush，llen（长度），lrange，lset name 1 lisi（将list的第二个元素修改成lisi），lrem（删除） Hash：等价于HashMap常见操作：hset（添加一个元素），hmset（添加多个元素），hget（获取一个元素），hmget（获取多个元素），hkeys（获取所有键），hvals（获取所有值），hdel（删除元素） Set：类似于HashSet，所以value指向同一个键。常见操作：sadd（添加），smembers（获取所有value），scard（获取长度），srandmember（获取随机元素），srem（删除元素），spop（随机删除元素），sismember（判断元素是否存在） Sortset：类似于优先队列set中每一个元素有一个score常见操作：zadd（添加），zcard（获取长度），zrem（删除元素），zremrangebyrank（根据排名删除），zremrangebyscore（根据成绩删除）]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见面试题的基础总结（计网篇）]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9A%84%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93%EF%BC%88%E8%AE%A1%E7%BD%91%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于这些内容都是比较早之前进行的整理的，所以有的部分是参考了他人的博文，但是由于是之前找的，所以具体的博文链接找不到了，如果原博主看到这个文章或者有人知道其中部分内容的原博文，请与我联系，我将加上原链接，谢谢 1、OSI七层协议模型 TCP/IP四层体系结构 OSI七层协议模型 TCP/IP四层体系结构 对应网络协议 应用层 应用层（对应OSI应用层、表示层、会话层） HTTP、TFTP、NFS、WAIS、SMTP 表示层 Telnet、Rlogin、SNMP、Gopher 会话层 SMTP、DNS 传输层 传输层 TCP、UDP 网络层 IP、ICMP、ARP、RARP、AKP、UUCP 数据链路层 网络接口层（对应OSI数据链路层、物理层） FDDI、Ethernet、Arpanet、PDN、SLIP、PPP 物理层 IEEE 802.1A、IEEE 802.2到IEEE 802.11 2、TCP三次握手: C端:客户端; S服务端 第一次握手: C端向S端发送SYN数据包〈SYN=1，序列号=x)。A迸入SYN_ SENT状志，等待服务端确认。 第二次握手: S端收到SYN数据包并进行确认(SYN=1, ACK number=x+1, ACK=1,序列号=y)，再发送SYN+ACK数据包给C端，S端迸入SYN_ RCVD状志。 第三次握手: C端收到SYN+ACK数据包，如果ACK number=x+1,将ACK number设置为y+1, ACK=1，向S端发送ACK数据包，C端和S端都进入ESTABLISHED (已连接)状志。 简述:我连你，你同意，我再连你（成功）。 为什么要三次握手?为了防止已失效的连接请求报文段突然又传到了服务端，产生错误。同时保证发送双方的消息发送与接收功能都可用。 解释:报文段已发送，在某个网络节点发生滞留，导致连接释放,释放后报文才到达另一端。 例如: C端发送SYN报文给S端，连接被释放后，S端才收到报文并误认为这是C端的新连接，给C端发送SYN+ACK报文，这是无法得到C端回应的，因为连接已无效。 3、TCP四次挥手 第一次挥手: A给B发送FIN报文(序列号=x)，A进入FIN_WAIT_1状态，表示A没有数据给B了。 第二次挥手: B收到FIN报文后，给A发送ACK报文(ACK=x+1)，A进入FIN_WAIT_2状态,B同意A关闭请求。 第三次挥手: B向A发送FIN报文(序列号=y)，请求关闭连接，B进入LAST_ACK状态。 第四次挥手: A收到FIN报文,向B发送ACK报文(ACK=y+1) , A进入TIME_WAIT状态，B收到ACK报文后关闭连接，A在2MSL后依然没收到回复，证明B端己关闭，A就可以关闭连接了。 简述:我要关闭，你同意，你要关闭，我同意你先关闭我再关闭。 为什么TCP要四次挥手? TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。 TCP是全双工模式，主机1请求关闭连接，不再发送数据了，但是可以接收主机2的数据，主机2不再发送数据了，才算关闭，这样减小了丢失数据的风险。 4、TIME-WAIT和CLOSE-TIME原因TCP要保证在所有可能的情况下使得所有的数据都能够被正确送达。当你关闭一个socket时，主动关闭一端的socket将进入TIME_WAIT状态，而被动关闭一方则转入CLOSED状态，这的确能够保证所有的数据都被传输。当一个socket关闭的时候，是通过两端四次握手完成的，当一端调用close()时，就说明本端没有数据要发送了。这好似看来在握手完成以后，socket就都可以处于初始的CLOSED状态了，其实不然。原因是这样安排状态有两个问题， 首先，我们没有任何机制保证最后的一个ACK能够正常传输，第二，网络上仍然有可能有残余的数据包(wandering duplicates)，我们也必须能够正常处理。 5、HTTP常见请求 Get 请求指定页面 Head 获取报头 Post 请求可能会导致资源建立 Put 修改资源 Delete 删除资源 Options 获取服务器性能 6、HTTP常见状态码 1xx 信息，服务器收到请求，需要请求者继续执行操作 2xx 成功，操作被成功接收并处理 3xx 重定向，需要进一步的操作以完成请求 4xx 客户端错误，请求包含语法错误或无法完成请求 5xx 服务器错误，服务器在处理请求的过程中发生了错误 7、HTTP与HTTPSHTTP使用80端口，HTTPS使用443端口，其中HTTPS是由SSL+HTTP协议构建的可进行加密传输，身份认证的网络协议。SSL是安全套接层在传输层 8、HTTP keep-alive参考：https://segmentfault.com/a/1190000012894416我们知道HTTP协议采用“请求-应答”模式，当使用普通模式，即非KeepAlive模式时，每个请求/应答客户和服务器都要新建一个连接，完成 之后立即断开连接（HTTP协议为无连接的协议）；当使用Keep-Alive模式（又称持久连接、连接重用）时，Keep-Alive功能使客户端到服 务器端的连接持续有效，当出现对服务器的后继请求时，Keep-Alive功能避免了建立或者重新建立连接。 9、TCP keepalive 概念: 在使用TCP长连接（复用已建立TCP连接）的场景下，需要对TCP连接进行保活，避免被网关干掉连接。在应用层，可以通过定时发送心跳包的方式实现。而Linux已提供的TCP KEEPALIVE，在应用层可不关心心跳包何时发送、发送什么内容，由OS管理：OS会在该TCP连接上定时发送探测包，探测包既起到连接保活的作用，也能自动检测连接的有效性，并自动关闭无效连接。 原理: 建立TCP连接时，就有定时器与之绑定，其中的一些定时器就用于处理keepalive过程。当keepalive定时器到0的时候，便会给对端发送一个不包含数据部分的keepalive探测包（probe packet），如果收到了keepalive探测包的回复消息，那就可以断定连接依然是OK的。如果我们没有收到对端keepalive探测包的回复消息，我们便可以断定连接已经不可用，进而采取一些措施。但Keepalive会额外产生一些网络数据包外，这些包将加大网络流量，对路由器和防火墙造成一定的负担。 10、TCP滑动窗口TCP滑动窗口具有拥塞控制和保证可靠性的功能 对于拥塞控制，滑动窗口是可变大小的，如果滑动窗口发生拥塞控制则将窗口大小置为1，然后对长度进行2的指数增长，直到窗口大小可满足数据传输或者大小到达阈值。 对于可靠性，接收双方具有同样大小的窗口，然后对数据进行编号，如果接收端没有收到某部分信息就会发送请求给发送方然后重新发送未接收到的部分。 11、TCP/UDP比较 TCP UDP TCP面向连接 UDP无连接 TCP是无界的 UDP是有界的 TCP可靠 UDP不可靠 TCP具有拥塞控制 UDP没有 TCP效率低 UDP效率高 TCP适合一对一连接 UDP适合广播、多播 TCP结构复杂 UDP结构简单 TCP能保证发送顺序 UDP无法保证 TCP使用字节流 UDP面向数据报 为什么TCP是无界的：例如TCP可能将一个连续数据分成多块发送，此时无法确认数据大小。 为什么UDP不可靠：因为TCP是面向连接的具有重传等机制，而UDP不会重传。 数据报：封装数据，目的地址，源地址，端口号 12、Cookie、Session Cookie是由服务器端生成，发送给User-Agent（一般是浏览器），浏览器会将Cookie的key/value保存到某个目录下的文本文件内，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用cookie）。 Cookie名称和值可以由服务器端开发自己定义，这样服务器可以知道该用户是否合法用户以及是否需要重新登录等，服务器可以设置或读取Cookies中包含信息，借此维护用户跟服务器会话中的状态。 Cookie和Session的区别与关系 session 在服务器端，cookie 在客户端（浏览器） session 默认被存在在服务器的一个文件里（不是内存） session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie ，同时 session 也会失效（但是可以通过其它方式实现，比如在 url 中传递 session_id） session 可以放在 文件、数据库、或内存中都可以。 用户验证这种场合一般会用 session 由于http协议是无状态的，服务器需要记录用户的状态，所以cookie和session都是用来保持状态的方案，session又依赖cookie。 13、Token（1）Session和Token的区别 session一般在cookie中传递而token一般放在header中 （2）Token的使用 Json Web Token jwt的token包括三个部分，分别是header，payload，还有signature，header就是放的类型还有加密方式，然后payload主要就是放签发信息，签发时间还有身份权限等自定义的信息，最后一个签名就是对前两部分进行加密，防止被人篡改，将用户的非私密信息传给前端。 14、当打开一个浏览器输入url到请求道页面的整个过程 DNS解析 将域名转化为IP地址 TCP连接 与服务器建立连接 发送HTTP请求 服务器处理请求并返回HTTP报文 浏览器解析渲染页面 连接结束 15、restful常见请求方法 restfual分别有GET \ POST \ PUT \ DELETE \ TRACE \ HEAD \ OPTIONS \ PATCH \ 等几种请求方法 POST : POST请求通常用来创建一个实体，也就是一个没有ID的资源。 GET：从服务器取回数据（只是取回数据，而不会产生其他的影响）。这是一个幂等的方法（译者注：使用相同的参数重复执行，应该能够获取到相同的结果）。 PUT ：PUT请求和POST请求类似，但是一般用来更新一个已有的实体。通过把已经存在的资源的ID和新的实体用PUT请求上传的服务器，来更新资源。 DELETE ： DELETE方法用来从服务器上删除资源。和PUT类似，你需要把要删除的资源的ID上传给服务器。 16、其他问题 有没有网络编程，有，怎么看连接状态？netstat，有哪些？ESTABLISHED，LISTEN等等，有异常情况吗？TIME_WAIT很多，为什么？大量短链接 奖品秒杀模型设计]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温Java网络编程（实现简易TCP，UDP应用）]]></title>
    <url>%2F2019%2F04%2F07%2F%E9%87%8D%E6%B8%A9Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%EF%BC%88%E5%AE%9E%E7%8E%B0%E7%AE%80%E6%98%93TCP%EF%BC%8CUDP%E5%BA%94%E7%94%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[关于TCP以及UDP的相关知识，在此就不做相关总结，如有需要可以参考https://blog.csdn.net/li_ning_/article/details/52117463 一、使用TCP实现一个简易登录功能1.创建一个实体类 User.java1234567891011121314151617181920212223242526272829303132package com.tcp.multithreadingTcp;import java.io.Serializable;public class User implements Serializable &#123; private String username; private String password; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; @Override public String toString() &#123; return "User&#123;" + "username='" + username + '\'' + ", password='" + password + '\'' + '&#125;'; &#125;&#125; 2.服务器端代码 LoginServer.java123456789101112131415161718192021222324package com.tcp.multithreadingTcp;import java.io.*;import java.net.ServerSocket;import java.net.Socket;public class LoginServer &#123; public static void main(String[] args) throws IOException &#123; //创建一个ServerSocket，指定监听的端口 ServerSocket serverSocket = new ServerSocket(8888); int i = 0; while (true) &#123; //使用ServerSocket监听用户请求 Socket socket = serverSocket.accept();//如果没有请求则阻塞 //启动一个登录线程 new Thread(new LoginThread(socket)).start(); System.out.println("你是访问该服务器的第" + ++i + "个用户,你的ip地址为" + socket.getInetAddress().getHostAddress()); socket.close(); &#125; &#125;&#125; 3.登录线程代码 LoginThread.java12345678910111213141516171819202122232425262728293031323334353637package com.tcp.multithreadingTcp;import java.io.*;import java.net.Socket;public class LoginThread implements Runnable &#123; private Socket socket; LoginThread(Socket socket) &#123; this.socket = socket; &#125; //处理用户请求 @Override public void run() &#123; //接收来自客户端的数据并输出 try &#123; ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); User user = (User) objectInputStream.readObject(); System.out.println(user.toString()); //发送反馈 DataOutputStream dataOutputStream = new DataOutputStream(socket.getOutputStream()); if ("zhangsan".equals(user.getUsername()) &amp;&amp; "1234".equals(user.getPassword())) &#123; dataOutputStream.writeUTF("登录成功"); &#125; else &#123; dataOutputStream.writeUTF("登录失败"); &#125; //关闭资源 dataOutputStream.close(); objectInputStream.close(); &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 4.客户端代码 LoginClient.java12345678910111213141516171819202122232425262728293031323334353637383940package com.tcp.multithreadingTcp;import java.io.*;import java.net.InetAddress;import java.net.Socket;import java.util.Scanner;public class LoginClient &#123; public static void main(String[] args) throws IOException &#123; //创建一个Socket，指定服务器端的ip与端口 Socket socket = new Socket(InetAddress.getLocalHost(), 8888); Scanner sc = new Scanner(System.in); System.out.print("请输入用户名:"); String username = sc.nextLine(); System.out.print("请输入密码:"); String password = sc.nextLine(); User user = new User(); user.setUsername(username); user.setPassword(password); //发送登录信息 OutputStream outputStream = socket.getOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(outputStream); objectOutputStream.writeObject(user); //接收数据 InputStream inputStream = socket.getInputStream(); DataInputStream dataInputStream = new DataInputStream(inputStream); String res = dataInputStream.readUTF(); System.out.println(res); //关闭资源 dataInputStream.close(); inputStream.close(); objectOutputStream.close(); outputStream.close(); socket.close(); &#125;&#125; 运行结果:(客户端) 123请输入用户名:zhangsan请输入密码:1234登录成功 运行结果:(服务端) 12User&#123;username=&apos;zhangsan&apos;, password=&apos;1234&apos;&#125;你是访问该服务器的第1个用户,你的ip地址为xxx.xxx.xxx.xxx 二、使用UDP实现一个简易聊天室功能 UDP相较于TCP实现简单一些，所以代码量也稍微的少一些 1.服务端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.udp.moreThanOneUdp;import java.io.IOException;import java.net.DatagramPacket;import java.net.DatagramSocket;import java.net.InetAddress;import java.util.Scanner;/** * 功能:在线客服. * 技能:使用UDP网络编程完成 * 注意 * 1. UDP编程中客户端和服务器端区分不明显,双方代码差别不大 * 2.主动发起请求的一段是客户端，接收请求的是服务器端;一旦通信开始，客户端和服务器端无差别 * 3.不涉及IO流 * 4.主要API * - DatagramSocket:socket,作用是发送和接收DatagramPacket * - DatagramPacket:数据报（封装数据，目的地址，源地址，端口号） */public class AskServer &#123; public static void main(String[] args) throws IOException &#123; //创建一个DatagramSocket,用来发送和接收数据 DatagramSocket socket = new DatagramSocket(8888);//服务器端接收数据的端口 Scanner sc = new Scanner(System.in); while (true) &#123; //使用DatagramSocket接收一个DatagramPacket byte[] buf = new byte[1024]; DatagramPacket datagramPacket1 = new DatagramPacket(buf, buf.length); socket.receive(datagramPacket1); System.out.println("对方说:" + new String(datagramPacket1.getData(), 0, datagramPacket1.getLength())); //使用DatagramSocket发送一个DatagramPacket System.out.print("请您输入:"); String info = sc.nextLine(); InetAddress inetAddress = datagramPacket1.getAddress(); int port = datagramPacket1.getPort(); DatagramPacket datagramPacket2 = new DatagramPacket(info.getBytes(), info.getBytes().length, inetAddress, port); socket.send(datagramPacket2); if (info.equals("bye")) &#123; break; &#125; &#125; System.out.println("聊天结束"); //关闭资源 socket.close(); &#125;&#125; 2.客户端代码123456789101112131415161718192021222324252627282930313233343536package com.udp.moreThanOneUdp;import java.io.IOException;import java.net.DatagramPacket;import java.net.DatagramSocket;import java.net.InetAddress;import java.util.Scanner;public class AskClient &#123; public static void main(String[] args) throws IOException &#123; //创建一个DatagramSocket,用来发送和接收数据 DatagramSocket socket = new DatagramSocket(8889);//客户端接收数据的端口,可以省略，会自动分配 Scanner sc = new Scanner(System.in); while (true) &#123; //使用DatagramSocket发送一个DatagramPacket System.out.print("请您输入:"); String info = sc.nextLine(); InetAddress inetAddress = InetAddress.getLocalHost(); DatagramPacket datagramPacket1 = new DatagramPacket(info.getBytes(), info.getBytes().length, inetAddress, 8888); socket.send(datagramPacket1); if (info.equals("bye")) &#123; break; &#125; //使用DatagramSocket接收一个DatagramPacket byte[] buf = new byte[1000]; DatagramPacket datagramPacket2 = new DatagramPacket(buf, buf.length); socket.receive(datagramPacket2); System.out.println("对方说:" + new String(buf, 0, datagramPacket2.getLength())); &#125; System.out.println("聊天结束"); //关闭资源 socket.close(); &#125;&#125; 运行结果:(客户端) 12345678请您输入:你好呀对方说:你是谁请您输入:我是张三，你呢对方说:我是李四请您输入:好的，我要忙了对方说:bye请您输入:bye聊天结束 运行结果:(服务端) 1234567对方说:你好呀请您输入:你是谁对方说:我是张三，你呢请您输入:我是李四对方说:好的，我要忙了请您输入:bye聊天结束 三、使用TCP实现文件上传以及文件下载功能 文件上传的本质:文件从客户端到服务器端的复制 文件下载的本质:文件从服务器端到客户端的复制 文件的上传和下载都是两次文件复制 1.文件上传功能服务端代码123456789101112131415161718192021222324252627282930313233343536package com.tcp.uploadTcp;import java.io.*;import java.net.ServerSocket;import java.net.Socket;public class UploadServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(8888); Socket socket = serverSocket.accept(); //创建一个输入流和一个输出流 BufferedInputStream bufferedInputStream = new BufferedInputStream(socket.getInputStream()); BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(new FileOutputStream(new File("/test1.txt"))); //使用输入流和输出流完成文件复制 //中转站 int n; //读取末尾的一个字节的内容赋给n n = bufferedInputStream.read(); while (n != -1)&#123; //写一个字节 bufferedOutputStream.write(n); System.out.println(1); //读一个字节 n = bufferedInputStream.read(); &#125; //关闭资源 bufferedInputStream.close(); bufferedOutputStream.close(); socket.close(); serverSocket.close(); &#125;&#125; 2.文件上传功能客户端代码12345678910111213141516171819202122package com.tcp.uploadTcp;import java.io.*;import java.net.InetAddress;import java.net.ServerSocket;import java.net.Socket;import java.net.UnknownHostException;public class UploadClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket(InetAddress.getLocalHost(),8888); //创建一个输入流和一个输出流 BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(new File("/test.txt"))); BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(socket.getOutputStream()); //关闭资源 bufferedInputStream.close(); bufferedOutputStream.close(); socket.close(); &#125;&#125; 1.文件下载功能服务端代码123456789101112131415161718192021222324252627282930313233343536package com.tcp.downloadTcp;import java.io.*;import java.net.ServerSocket;import java.net.Socket;public class DownloadServer &#123; public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(8888); Socket socket = serverSocket.accept(); //创建一个输入流和一个输出流 BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(new FileOutputStream(new File("/test.txt"))); BufferedInputStream bufferedInputStream = new BufferedInputStream(socket.getInputStream()); //使用输入流和输出流完成文件复制 //中转站 int n; //读取末尾的一个字节的内容赋给n n = bufferedInputStream.read(); while (n != -1)&#123; //写一个字节 bufferedOutputStream.write(n); //读一个字节 n = bufferedInputStream.read(); &#125; //关闭资源 bufferedInputStream.close(); bufferedOutputStream.close(); socket.close(); serverSocket.close(); &#125;&#125; 2.文件下载功能客户端代码1234567891011121314151617181920package com.tcp.downloadTcp;import java.io.*;import java.net.InetAddress;import java.net.Socket;public class DownloadClient &#123; public static void main(String[] args) throws IOException &#123; Socket socket = new Socket(InetAddress.getLocalHost(),8888); //创建一个输入流和一个输出流 BufferedInputStream bufferedInputStream = new BufferedInputStream(socket.getInputStream()); BufferedOutputStream bufferedOutputStream = new BufferedOutputStream(new FileOutputStream(new File("/test1.txt"))); //关闭资源 bufferedInputStream.close(); bufferedOutputStream.close(); socket.close(); &#125;&#125; 其实上传功能和下载功能的逻辑是相同的，只不过一个是文件c =&gt; s,一个是文件s =&gt; c 参考视频：https://www.bilibili.com/video/av31123719/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程基础</tag>
        <tag>Socket编程</tag>
        <tag>TCP/UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温Java反射机制]]></title>
    <url>%2F2019%2F04%2F01%2F%E9%87%8D%E6%B8%A9Java%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[本文主要是为了实战Java反射，简单工厂只是一个背景，故对简单工厂模式不做解释了。 1、首先先做个一个使用了简单工厂模式的demo。Fruit是一个接口，里面含有一个get()方法 12345package SimpleFactory;public interface Fruit &#123; public void get();&#125; 2、Apple与Banana为两个实现了Fruit的接口，其中Apple类中方法较多用于实现Java反射中的各种情况。 12345678910111213141516171819202122232425262728293031323334package SimpleFactory;public class Apple implements Fruit &#123; private String name = "name"; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public void get() &#123; System.out.println("This is apple"); &#125; //无参无返回值情况 private void aha()&#123; System.out.println("This is apple, but you can't see this word." + name); &#125; //有参无返回值情况 private void ahaString(String str)&#123; System.out.println("This is apple, but you can't see this word." + str); &#125; //有参有返回值情况 private String ahaReturn(String str)&#123; System.out.println("This is apple, but you can't see this word." + str); return "yes"; &#125;&#125; 3、FruitFactory为简单工厂模式创建类（为了后期再Main函数中体现Java反射机制，正确的应该像注释中那样，直接返回类） 1234567891011121314151617181920212223242526272829303132package SimpleFactory;public class FruitFactory &#123; public static Class getFruit(String typeName)&#123; try&#123; //通过字符串获取相关的类并返回 return Class.forName(typeName); &#125;catch (ClassNotFoundException e)&#123; e.printStackTrace(); &#125; return null; &#125;&#125;//package SimpleFactory;////public class FruitFactory &#123;// public static Fruit getFruit(String typeName)&#123;// Class fruitClass = null;// try &#123;// fruitClass = Class.forName(typeName);// return (Fruit) fruitClass.newInstance();// &#125; catch (ClassNotFoundException e) &#123;// e.printStackTrace();// &#125; catch (IllegalAccessException e) &#123;// e.printStackTrace();// &#125; catch (InstantiationException e) &#123;// e.printStackTrace();// &#125;// return null;// &#125;//&#125; 4、下面是执行一下常见的反射操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package SimpleFactory;import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class MainSimpleFactory &#123; public static void main(String[] args) &#123; //反射机制 try &#123; //获取Apple的Class Class appleClass = FruitFactory.getFruit("SimpleFactory.Apple"); //创建实例 Fruit apple = (Fruit) appleClass.newInstance(); //执行实例的public方法 apple.get(); //获取private方法 Method getAha = appleClass.getDeclaredMethod("aha"); //设置Accessible否则无法执行 getAha.setAccessible(true); //执行该无参无返回值的方法 getAha.invoke(apple); //获取private属性 Field name = appleClass.getDeclaredField("name"); //设置Accessible否则无法执行 name.setAccessible(true); //修改该name的值 name.set(apple, "newName"); //修改后再一次执行getAha方法 getAha.invoke(apple); //获取private方法 Method getAhaString = appleClass.getDeclaredMethod("ahaString",String.class); //设置Accessible否则无法执行 getAhaString.setAccessible(true); //执行该有参无返回值的方法 getAhaString.invoke(apple, "false"); //获取private方法 Method getAhaReturn = appleClass.getDeclaredMethod("ahaReturn",String.class); //设置Accessible否则无法执行 getAhaReturn.setAccessible(true); //执行该有参有返回值的方法 String res = (String) getAhaReturn.invoke(apple, "true"); System.out.println(res); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; //与上面相同，只不过是使用简单工厂创建了Banana类的对象 try &#123; Class appleClass = FruitFactory.getFruit("SimpleFactory.Banana"); Fruit banana = (Fruit) appleClass.newInstance(); banana.get(); Method getAha = appleClass.getDeclaredMethod("ahaa"); getAha.setAccessible(true); getAha.invoke(banana); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (NoSuchMethodException e) &#123; e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 5、执行结果如下： 12345678This is appleThis is apple, but you can&apos;t see this word.nameThis is apple, but you can&apos;t see this word.newNameThis is apple, but you can&apos;t see this word.falseThis is apple, but you can&apos;t see this word.trueyesThis is BananaThis is banana, but you can&apos;t see this word.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程基础</tag>
        <tag>Java反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见排序算法]]></title>
    <url>%2F2019%2F03%2F12%2F%E5%B8%B8%E8%A7%81%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[今天实在不想刷笔试题就把常见的排序手敲了一遍 1.选择排序123456789101112131415161718192021222324252627282930313233343536373839class ChoiceSort&lt;T extends Comparable&gt;&#123; public static void main(String[] args) &#123; ChoiceSort&lt;Integer&gt; choiceSort = new ChoiceSort&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,4,2,3,0,5,9,7&#125;; choiceSort.sort(arr); choiceSort.show(arr); &#125; public void sort(T[] arr)&#123; int n = arr.length; for(int i = 0; i &lt; n; i++)&#123; int min = i; for(int j = i + 1; j &lt; n; j++)&#123; if(less(arr[j] , arr[min]))&#123; min = j; &#125; &#125; exch(arr, i ,min); &#125; &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] arr, int i, int j)&#123; T tmp = arr[i]; arr[i] = arr[j]; arr[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125; 2.插入排序123456789101112131415161718192021222324252627282930313233class InsertSort&lt;T extends Comparable&gt;&#123; public static void main(String[] args) &#123; InsertSort&lt;Integer&gt; insertSort = new InsertSort&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,4,2,3,0,5,9,7&#125;; insertSort.sort(arr); insertSort.show(arr); &#125; public void sort(T[] arr)&#123; for(int i = 1; i &lt; arr.length; i++)&#123; for(int j = i; j &gt; 0 &amp;&amp; less(arr[j], arr[j-1]) ; j--)&#123; exch(arr, j, j -1); &#125; &#125; &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] a, int i, int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125; 3.冒泡排序123456789101112131415161718192021222324252627282930class BubbleSort&lt;T extends Comparable&gt;&#123; public static void main(String[] args)&#123; BubbleSort&lt;Integer&gt; bubbleSort = new BubbleSort&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,6,2,3,88,-2,6,2,8,0,5,9,7&#125;; for(int i = arr.length - 1; i &gt;= 0; i--)&#123; for(int j = 0; j &lt; i; j++)&#123; if(!bubbleSort.less(arr[j], arr[j+1]))&#123; bubbleSort.exch(arr, j, j+1); &#125; &#125; &#125; bubbleSort.show(arr); &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] a, int i, int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125; 4.归并排序自顶向下的排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class MergeSortStartByTop&lt;T extends Comparable&gt;&#123; private T[] tmpArr; public static void main(String[] args) &#123; MergeSortStartByTop&lt;Integer&gt; mergeSort = new MergeSortStartByTop&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,4,2,3,0,5,9,7&#125;; mergeSort.sort(arr); mergeSort.show(arr); &#125; public void sort(T[] arr)&#123; tmpArr = (T[]) new Comparable[arr.length]; sort(arr, 0, arr.length - 1); &#125; private void sort(T[] arr, int l, int r)&#123; if(l &gt;= r)&#123; return; &#125; int mid = l + (r - l)/2; sort(arr, l, mid); sort(arr, mid + 1, r); merge(arr, mid, l, r); &#125; private void merge(T[] arr, int mid, int l, int r)&#123; int i = l; int j = mid+1; for(int k = l; k &lt;= r; k++)&#123; tmpArr[k] = arr[k]; &#125; for(int k = l; k &lt;= r; k++)&#123; if(i &gt; mid)&#123; arr[k] = tmpArr[j++]; &#125;else if(j &gt; r)&#123; arr[k] = tmpArr[i++]; &#125;else if(less(tmpArr[i], tmpArr[j]))&#123; arr[k] = tmpArr[i++]; &#125;else &#123; arr[k] = tmpArr[j++]; &#125; &#125; &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] a, int i, int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125; 自底向上的排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class MergeSortStartByButton&lt;T extends Comparable&gt;&#123; private T[] tmpArr; public static void main(String[] args) &#123; MergeSortStartByButton&lt;Integer&gt; mergeSort = new MergeSortStartByButton&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,4,2,3,0,5,9,7&#125;; mergeSort.sort(arr); mergeSort.show(arr); &#125; public void sort(T[] arr)&#123; tmpArr = (T[]) new Comparable[arr.length]; int width = 0; int index = 0; //外循环为每次归并排序,每组数据的宽度,每组数据的宽度之后进行2倍递增 for (width = 1; width &lt; arr.length; width = width * 2) &#123; //内循环为基于每组数据宽度,进行多组数据的归并排序 //index += 2 * width 因为一次归并排序都是使用2组数据进行排序,所以每次 // 递增2组数据的偏移量 //index &lt; (size - width) 这里表示如果排序的索引位置连1组数据个数都不够了 // 那就没必要处理了,因为排序至少需要1组多的数据. for (index = 0; index &lt; (arr.length - width); index += 2 * width ) &#123; int l = index; int r = index + (2 * width - 1); int mid = index + (r - l) / 2; merge(arr, mid, l, r); &#125; &#125; &#125; private void merge(T[] arr, int mid, int l, int r)&#123; int i = l; int j = mid+1; for(int k = l; k &lt;= r; k++)&#123; tmpArr[k] = arr[k]; &#125; for(int k = l; k &lt;= r; k++)&#123; if(i &gt; mid)&#123; arr[k] = tmpArr[j++]; &#125;else if(j &gt; r)&#123; arr[k] = tmpArr[i++]; &#125;else if(less(tmpArr[i], tmpArr[j]))&#123; arr[k] = tmpArr[i++]; &#125;else &#123; arr[k] = tmpArr[j++]; &#125; &#125; &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] a, int i, int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125; 5.快速排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class QuickSort&lt;T extends Comparable&gt;&#123; public static void main(String[] args) &#123; QuickSort&lt;Integer&gt; quickSort = new QuickSort&lt;&gt;(); Integer[] arr = new Integer[]&#123;1,6,2,3,88,-2,6,2,8,0,5,9,7&#125;; quickSort.sort(arr); System.out.println("-----"); quickSort.show(arr); &#125; public void sort(T[] arr)&#123; sort(arr,0, arr.length - 1); &#125; public void sort(T[] arr, int l, int r)&#123; if(l &gt;= r)&#123; return; &#125; int i = partation1(arr, l, r); sort(arr, l, i - 1); sort(arr, i + 1, r); &#125; //基准值为右边的 public int partation(T[] arr, int l, int r)&#123; int i = l-1; int j = r; T t = arr[r]; while(true)&#123; while(less(arr[++i], t))&#123; if(i == r)&#123; break; &#125; &#125; while (less(t, arr[--j]))&#123; if(j == l)&#123; break; &#125; &#125; if(i &gt;= j)&#123; break; &#125; exch(arr, i , j); &#125; exch(arr, r, i); return i; &#125; //基准值为左边的 public int partation1(T[] arr, int l, int r)&#123; int i = l; int j = r + 1; T t = arr[l]; while(true)&#123; while(less(arr[++i], t))&#123; if(i == r)&#123; break; &#125; &#125; while (less(t, arr[--j]))&#123; if(j == l)&#123; break; &#125; &#125; if(i &gt;= j)&#123; break; &#125; exch(arr, i , j); &#125; exch(arr, l, j); return j; &#125; private boolean less(T t1, T t2)&#123; return t1.compareTo(t2) &lt; 0; &#125; private void exch(T[] a, int i, int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp; &#125; public void show(T[] arr)&#123; for(T t : arr)&#123; System.out.println(t); &#125; &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis如何实现异步队列]]></title>
    <url>%2F2019%2F02%2F14%2Fredis%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[一.如何使用Redis做异步队列1.使用List作为队列, RPUSH生产消息, LPOP消费消息➢缺点:没有等待队列里有值就直接消费➢弥补:可以通过在应用层引入Sleep机制去调用LPOP重试 2. BLPOP key [key .. timeout :阻塞直到队列有消息或者超时➢缺点:只能供-一个消费者消费 二.如何使用Redis做异步队列pub/sub :主题订阅者模式 发送者(pub)发送消息,订阅者(sub)接收消息 订阅者可以订阅任意数量的频道hex 下面我们就演示一下，首先我们打开三个终端，两个作为接收者，一个作为发送者，由下图可见当我们发送消息时只有指定的订阅者可以获取到消息 三.pub/sub的缺点消息的发布是无状态的,无法保证可达，若订阅者在发送者发布消息期间下线，之后我们再上线将无法接受到刚才发送的消息，解决办法就是使用消息队列]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>异步队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis如何实现分布式锁]]></title>
    <url>%2F2019%2F02%2F13%2Fredis%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[1.分布式锁需要解决的问题 互斥性：任意时刻只能有一个客户端拥有锁，不能同时多个客户端获取 安全性：锁只能被持有该锁的用户删除，而不能被其他用户删除 死锁：获取锁的客户端因为某些原因而宕机，而未能释放锁，其他客户端无法获取此锁，需要有机制来避免该类问题的发生 容错：当部分节点宕机，客户端仍能获取锁或者释放锁 2.如何通过Redis实现分布式锁:(非完善方法)SETNX key value :如果key不存在,则创建并赋值 时间复杂度: 0(1) 返回值:设置成功,返回1;设置失败,返回0。 但是此时我们获取的key是长期有效的，所以我们应该如何解决长期有效的问题呢？ 如何解决SETNX长期有效的问题EXPIRE key seconds 设置key的生存时间,当key过期时(生存时间为0) ,会被自动删除 缺点：原子性得不到满足下面是伪代码12345678//该程序存在危险，如果执行到第二行就崩溃了，则此时key会被一直占用而无法被释放RedisService redisService = SpringUtils.getBean(Redi sService.class); long status = redisService.setnx(key, "1");if(status == 1) &#123; redisService.expire(key, expire); //执行独占资源逻辑 doOcuppiedWork();&#125; 3.如何通过Redis实现分布式锁:(正确方式)SET key value [EX seconds] [PX milliseconds] [NX|XX] EX second :设置键的过期时间为second秒 PX millisecond :设置键的过期时间为millisecond毫秒 NX :只在键不存在时,才对键进行设置操作 XX:只在键已经存在时,才对键进行设置操作 SET操作成功完成时,返回OK ,否则返回nil下面是伪代码123456RedisService redisService = SpringUtils.getBean(RedisService.class); .String result = redisService.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);if ("OK".equals(result)) &#123; //执行独占资源逻辑 doOcuppiedWork();&#125; 4.大量的key同时过期的注意事项 集中过期,由于清除大量的key很耗时,会出现短暂的卡顿现象 解放方案:在设置key的过期时间的时候,给每个key加上随机值]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[当前读与快照读]]></title>
    <url>%2F2019%2F02%2F13%2F%E5%BD%93%E5%89%8D%E8%AF%BB%E4%B8%8E%E5%BF%AB%E7%85%A7%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[本文是根据慕课相关课程学习后，并总结了以下几篇博文后做的总结https://blog.csdn.net/silyvin/article/details/79280934https://www.cnblogs.com/cat-and-water/p/6427612.html innodb的默认事务隔离级别是rr（可重复读）。它的实现技术是mvcc。基于版本的控制协议。该技术不仅可以保证innodb的可重复读，而且可以防止幻读。但是它防止的是快照读，也就是读取的数据虽然是一致的，但是数据是历史数据。如何做到保证数据是一致的（也就是一个事务，其内部读取对应某一个数据的时候，数据都是一样的），同时读取的数据是最新的数据。innodb提供了一个间隙锁的技术。也就是结合GAP锁与行锁，达到最终目的。当使用索引进行插入的时候，innodb会将当前的节点和上一个节点加锁。这样当进行select的时候，就不允许加x锁。那么在进行该事务的时候，读取的就是最新的数据。 快照读(snapshot read) 不加锁的非阻塞读 当前读(current read) select … lock in share mode - select … lock in share mode select … for update insert update delete RC、RR级别下的InnoDB的非阻塞读如何实现 数据行里的DB_TRX_ ID（事务ID）、DB_ ROLL_PTR（回滚指针）、DB_ROW_ ID（行号）字段 undo日志 进行变更操作会产生undo日志，存储老版本数据 分为insert undo log（存储insert日志，只在事务回滚时需要，事务提交后即可删除）和update undo log（存储delete和update日志，不仅回滚需要，快照读也需要，不可随便删除） 下面我们看一下演示 上图为将field2字段由12改为32。此时我们将原数据存出来undo log，我们将回滚指针指向undo log的原数据，同时DB_TRX_ ID加1。如果我们之后进行回滚，则从undo log中获取数据，进行回滚。 同样下图为两个undo log日志 InnoDB可重复读隔离级别下如何避免幻读 表象:快照读(非阻塞读) –伪MVCC 内在: next-key锁(行锁+GAP锁) 对主键索引或者唯一索引会用Gap锁吗？？ 如果where条件全部命中,则不会用Gap锁,只会加记录锁 如果where条件部分命中或者全不命中,则会加Gap锁 如数据库中存在id为1，3，5的值，如果我们查询where id in (1,3); 此时我们会发现只有行锁，我们update id= 2;是成功的，但是如果我们查询where id in (1,4); 此时我们会发现有Gap锁，我们update id= 2;是无法成功的，只有等查询事务结束后才能update]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>事务</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务并发访问中的问题及隔离机制]]></title>
    <url>%2F2019%2F02%2F01%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E5%B9%B6%E5%8F%91%E8%AE%BF%E9%97%AE%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E9%9A%94%E7%A6%BB%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[我们直接从隔离级别从低到高进行介绍 1.更新丢失目前主流数据库都会自动进行枷锁来避免，所以不好从数据库层面进行模拟，举个例子： 取款事务 存款事务 开始事务 开始事务 查询转账余额为100元 查询转账余额为100元 存入20元,余额变为120元 提交事务 取出10元,余额改为90元 回滚事务,余额恢复为100元 更新丢失 从上表我们可以看出最终更新被覆盖导致了更新丢失 2.脏读是指一个事务读取到了另一个事务未提交的数据 如何解决：READ-COMMITTED事务隔离级别以上可避免 下面我们进行验证首先我们先使用下面的语句获取当前的事务隔离级别可知数据库的默认级别REPEATABLE-READ 1select @@tx_isolation;#查询隔离级别 下面我们使用下面的语句将当前事务的隔离级别设置为可读未提交READ-UNCOMMITTED(最低的事务隔离级别) 1set session transaction isolation level read uncommitted;#修改隔离界别为 可读未提交 下面我们模拟脏读我们在数据库中先添加一个数据 模拟步骤 1. 首先我们新建两个控制台来模仿两个不同的事务并在两个控制台都执行下面的语句，并将事务隔离级别都改为READ_UNCOMMITTED 2. 我们在控制台1中使用start transaction;来开始我们的事务 3. 我们使用select * from learn_sql.learn_sql where id = 1;此时我们查询到的money为初值300. 4. 此时我们使用update learn_sql.learn_sql set money = 400 where id = 1;执行过此命令之后我们再执行select * from learn_sql.learn_sql where id = 1;会发现此时的money已经改成了400. 5. 我们使用控制台2使用select * from learn_sql.learn_sql where id = 1;查询可知此时的money已经变成了还未提交的400元。 6.若此时控制台1未使用commit进行提交，而是rollback进行事务回滚，则此时就是发生了控制台2读取了未成功提交的数据。 解决办法：使用下面的sql语句将事务隔离级别改为READ_COMMITTED，此时如果我们执行步骤4则会发现，控制台2获取的并不是400而是300。1set session transaction isolation level read committed;#修改隔离界别为 只可读提交 3.不可重复读指事务1多次读取同一个数据，事务2在事务1多次读取过程中进行的修改提交，导致事务1多次读取的数据不同。 如何解决：REPEATABLE-READ事务隔离级别以上可避免 下面我们开始模拟，首先我们先保证事务隔离级别为READ_COMMITTED 模拟步骤 1.首先我们在控制台1中使用start transaction;开始事务，此时我们使用select * from learn_sql.learn_sql where id = 1;可知money为300，然后我们使用update learn_sql.learn_sql set money = money + 100 where id = 1; 2.此时我们使用控制台2中先使用start transaction;开始事务，再使用select * from learn_sql.learn_sql where id = 1;发现money为300。 3.接着我们在控制台1使用commit;提交修改。这是我们控制台2中使用select * from learn_sql.learn_sql where id = 1;我们突然发现原来的300，突然变成了400.这就是发生了不可重复读的问题了，即多次查询结果不一致 解决办法：我们使用下面的sql语句将食物隔离级别修改为REPEATABLE-READ1set session transaction isolation level repeatable read; 这时我们在步骤3中获取的money仍然为300，但是如果我们使用update learn_sql.learn_sql set money = money + 100 where id = 1;后再查询会发现money不是我们最后一步查询的300+100=400元，而是我们控制台1提交之后的400+100=500元，这就防止了在一次事务中多次读不一致，同时可以保证该食物提交结果正确。 4.幻读指事务1读取与搜索条件相匹配的若干行，事务2以插入或删除行的方式来修改事务1的结果集。 如何解决：SERIALIZABLE事务隔离级别可避免 下面我们开始模拟，首先我们先保证事务隔离级别为READ_COMMITTED由于mysql在技术层面上避免了REPEATABLE-READ隔离级别下发生，但是理论上并不可避免 模拟步骤 1.首先我们在控制台1中使用start transaction;开始事务，此时我们使用select * from learn_sql.learn_sql;可得到一条数据。 2.此时我们使用控制台2中先使用start transaction;开始事务，再使用insert into learn_sql.learn_sql (id,money);接着我们在控制台2使用commit;提交修改。 3.接着我们在控制台1使用update learn_sql.learn_sql set money = 1000;我们会发现这个时候我们修改的语句条数为两句，这与我们之前查询的一条不同，这就是幻读。 解决办法：把事务隔离级别修改为SERIALIZABLE之后我们会发现在执行第二步是会被lock住无法操作，从而避免了幻读。 5.总结： 事务隔离级别 更新丢失 脏读 不可重复读 幻读 未提交读 避免 发生 发生 发生 已提交读 避免 避免 发生 发生 可重复读 避免 避免 避免 发生 串行化 避免 避免 避免 避免]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>锁</tag>
        <tag>事务</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络相关知识总结之HTTP]]></title>
    <url>%2F2019%2F02%2F01%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%E4%B9%8BHTTP%2F</url>
    <content type="text"><![CDATA[1.HTTP简介HTTP协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的WWW文件都必须遵守这个标准。HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。HTTP三点注意事项： 1.HTTP是无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。2.HTTP是媒体独立的：这意味着，只要客户端和服务器知道如何处理的数据内容，任何类型的数据都可以通过HTTP发送。客户端以及服务器指定使用适合的MIME-type内容类型。3.HTTP是无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 2.HTTP消息结构客户端请求消息：客户端发送一个HTTP请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成，下图给出了请求报文的一般格式。1234GET /hello.txt HTTP/1.1User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3Host: www.example.comAccept-Language: en, mi 服务器响应消息：HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。 123456789HTTP/1.1 200 OKDate: Mon, 27 Jul 2009 12:28:53 GMTServer: ApacheLast-Modified: Wed, 22 Jul 2009 19:15:56 GMTETag: "34aa387-d-1568eb00"Accept-Ranges: bytesContent-Length: 51Vary: Accept-EncodingContent-Type: text/plain 3.HTTP请求方法根据HTTP标准，HTTP请求可以使用多种请求方法。HTTP1.0定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。 序号 方法 描述 1 GET 请求指定的页面信息，并返回实体主体。 2 HEAD 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 3 POST 向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 4 PUT 从客户端向服务器传送的数据取代指定的文档的内容。 5 DELETE 请求服务器删除指定的页面。 6 CONNECT HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 7 OPTIONS 允许客户端查看服务器的性能。 8 TRACE 回显服务器收到的请求，主要用于测试或诊断。 4.HTTP常见状态码 状态码 状态码英文名称 描述 200 OK 正常返回信息 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域- - 起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在，eg, 输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 5.HTTP中POST与GET的区别从三个层面来解答： 1.从Http报文层面: GET将请求信息放在URL , POST放在报文体中2.数据库层面: CET符合幂等性和安全性, POST不符合3.其他层面: GET可以被缓存、被存储,而POST不行]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前后端通过Json传数据，并将Json转化成实体类]]></title>
    <url>%2F2019%2F01%2F23%2F%E5%89%8D%E5%90%8E%E7%AB%AF%E9%80%9A%E8%BF%87Json%E4%BC%A0%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B9%B6%E5%B0%86Json%E8%BD%AC%E5%8C%96%E6%88%90%E5%AE%9E%E4%BD%93%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[今天尝试着用thymeleaf写马上就要使用的计算机基础知识大赛的比赛系统，然后卡在如何将多个对象通过form表单发送给后端，最后没有找到比较好的办法，只能使用js将需要发送的数据转化成Json然后使用ajax发送请求。然后在后台在将数据转化成实体类 首先我们的实体类如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.njupt.sacc.cbkc.problem.entity;public class ProblemResult &#123; private Integer id; private Integer uid; private Integer pid; private String result; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id = id; &#125; public Integer getUid() &#123; return uid; &#125; public void setUid(Integer uid) &#123; this.uid = uid; &#125; public Integer getPid() &#123; return pid; &#125; public void setPid(Integer pid) &#123; this.pid = pid; &#125; public String getResult() &#123; return result; &#125; public void setResult(String result) &#123; this.result = result; &#125; @Override public String toString() &#123; return "ProblemResult&#123;" + "id=" + id + ", uid=" + uid + ", pid=" + pid + ", result='" + result + '\'' + '&#125;'; &#125;&#125; 然后我们的目标就是前端向后端发送多个ProblemResult对象，然后在进行处理，首先我们看前端的实现方案（此时后端向前端发送的是一个List）：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;!DOCTYPE html&gt;&lt;html lang="en" xmlns:th="http://www.w3.org/1999/xhtml"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script src="http://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;hr&gt;&lt;div th:each="problem : $&#123;problems&#125;"&gt; &lt;span th:text="$&#123;problem.id&#125;"&gt;&lt;/span&gt; &lt;form th:object="$&#123;problem&#125;" th:if="$&#123;problem.isSingle == 1&#125;"&gt; &lt;input type="radio" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.aOption&#125;" value="a"&gt; &lt;input type="radio" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.bOption&#125;" value="b"&gt; &lt;input type="radio" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.cOption&#125;" value="c"&gt; &lt;input type="radio" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.dOption&#125;" value="d"&gt; &lt;/form&gt; &lt;form th:object="$&#123;problem&#125;" th:if="$&#123;problem.isSingle == 0&#125;"&gt; &lt;input type="checkbox" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.aOption&#125;" value="a"&gt; &lt;input type="checkbox" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.bOption&#125;" value="b"&gt; &lt;input type="checkbox" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.cOption&#125;" value="c"&gt; &lt;input type="checkbox" th:name="$&#123;problem.id&#125;" th:text="$&#123;problem.dOption&#125;" value="d"&gt; &lt;/form&gt; &lt;hr&gt;&lt;/div&gt;&lt;button name="save" onclick="loadXMLDoc(name)"&gt;保存&lt;/button&gt;&lt;button name="push" onclick="loadXMLDoc(name)"&gt;提交&lt;/button&gt;&lt;script type="text/javascript"&gt; //将答案转化成Json格式 //name用来判断是保存操作还是提交操作 function f(name) &#123; var json = []; //[[$[count]]]为thymeleaf中js获取值的方法 var count = [[$&#123;count&#125;]]; for (var i = 1; i &lt;= count; i++) &#123; var type = document.getElementsByName(i).item(0).getAttribute("type"); //js获取单选框的值 if (type == "radio") &#123; var radio = document.getElementsByName(i); var result = ""; for (var j = 0; j &lt; radio.length; j++) &#123; if (radio[j].checked) &#123; result = radio[j].value; break; &#125; &#125; &#125; else &#123;//js获取多选框的值 var checkbox = document.getElementsByName(i); var result = ""; for (var j = 0; j &lt; checkbox.length; j++) &#123; if (checkbox[j].checked) result = result + checkbox[j].value; &#125; &#125; var row = &#123;"pid": i, "result": result&#125;; json.push(row); &#125; return &#123;"type":name,"data":json&#125;; &#125; //发送ajax请求 function loadXMLDoc(name) &#123; var xmlhttp; if (window.XMLHttpRequest) &#123; // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp = new XMLHttpRequest(); &#125; else &#123; // IE6, IE5 浏览器执行代码 xmlhttp = new ActiveXObject("Microsoft.XMLHTTP"); &#125; xmlhttp.onreadystatechange = function () &#123; if (xmlhttp.readyState == 4 &amp;&amp; xmlhttp.status == 200) &#123; console.log("successful"); &#125; &#125;; xmlhttp.open("POST", "/test", true); xmlhttp.setRequestHeader("Content-Type", "application/json"); var data = f(name); xmlhttp.send(JSON.stringify(data)); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 传送的数据示例大致如下:12345678910111213&#123; "submit":[ &#123;"pid":1,"result":"a"&#125;, &#123;"pid":2,"result":"ab"&#125;, &#123;"pid":3,"result":""&#125;, &#123;"pid":4,"result":"abcd"&#125;, &#123;"pid":5,"result":"a"&#125;, &#123;"pid":6,"result":"d"&#125;, &#123;"pid":7,"result":"b"&#125;, &#123;"pid":8,"result":"c"&#125; ], "type":"save"&#125; 之后我们看后端如何获取数据并将数据转化成实体类：123456789101112131415@RequestMapping("/test") public String submit(@RequestBody JSONObject jsonObject)&#123; //通过key获取前端发送的json的数组 JSONArray problemResults = jsonObject.getJSONArray("data"); //通过key获取前端发送的json中的type字段 String submitType = String.valueOf(jsonObject.get("type")); List&lt;ProblemResult&gt; list = new ArrayList&lt;&gt;(); //通过遍历将json数据中的数据转化成实体类 for (Object problemResult : problemResults) &#123; //会用JSONObject包中的函数将字符串转化成实体类 ProblemResult result = (ProblemResult) JSONObject.toJavaObject((JSONObject) problemResult, ProblemResult.class); list.add(result); &#125; return "index"; &#125; 经过上面的操作，我们就成功的完成了前端向后端发送多个对象，整体思想大概就是先把需要传的所有对象看成一个数组，然后传到后端之后再对json数据进行解析，然后获取数据中的json字符串，然后通过json库中的函数将json字符串转化成实体类，全部完成 参考博文： 阿里巴巴的JSONObject对象转换 https://blog.csdn.net/a990914093/article/details/81217581JSONObject如何转换成实体类型https://blog.csdn.net/m0_38129335/article/details/80047034json数据与实体类之间的相互转换https://blog.csdn.net/nandao158/article/details/71122851]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Thymeleaf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成MyBatis]]></title>
    <url>%2F2018%2F11%2F27%2Fspringboot%E9%9B%86%E6%88%90MyBatis%2F</url>
    <content type="text"><![CDATA[一、介绍 MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 二、创建项目使用idea创建空项目并记得选择web，mysql，mybaties这几个依赖即可，在此就不赘述了，创建后包依赖大致如下：12345678910111213141516171819202122&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 三、在yml配置中添加相关配置12345678910spring: datasource: url: jdbc:mysql://localhost:3306/test username: root password: root driver-class-name: com.mysql.jdbc.Driver# 之后的填写mybatis的mapper书写路径mybatis: mapper-locations: classpath:mapper/*.xml 四、创建一个实体类SysUserEntity.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package cn.gausscode.calo.user.entity;import java.util.Date;public class SysUserEntity &#123; private int id; private String name; private String loginName; private String email; private int tel; private String password; private String picUrl; private String siteId; private Date createDate; private int createBy; private Date updateDate; private int updateId; private int delFlag; private String remarks; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getLoginName() &#123; return loginName; &#125; public void setLoginName(String loginName) &#123; this.loginName = loginName; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email = email; &#125; public int getTel() &#123; return tel; &#125; public void setTel(int tel) &#123; this.tel = tel; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public String getPicUrl() &#123; return picUrl; &#125; public void setPicUrl(String picUrl) &#123; this.picUrl = picUrl; &#125; public String getSiteId() &#123; return siteId; &#125; public void setSiteId(String siteId) &#123; this.siteId = siteId; &#125; public Date getCreateDate() &#123; return createDate; &#125; public void setCreateDate(Date createDate) &#123; this.createDate = createDate; &#125; public int getCreateBy() &#123; return createBy; &#125; public void setCreateBy(int createBy) &#123; this.createBy = createBy; &#125; public Date getUpdateDate() &#123; return updateDate; &#125; public void setUpdateDate(Date updateDate) &#123; this.updateDate = updateDate; &#125; public int getUpdateId() &#123; return updateId; &#125; public void setUpdateId(int updateId) &#123; this.updateId = updateId; &#125; public int getDelFlag() &#123; return delFlag; &#125; public void setDelFlag(int delFlag) &#123; this.delFlag = delFlag; &#125; public String getRemarks() &#123; return remarks; &#125; public void setRemarks(String remarks) &#123; this.remarks = remarks; &#125; @Override public String toString() &#123; return "SysUserEntity&#123;" + "id=" + id + ", name='" + name + '\'' + ", loginName='" + loginName + '\'' + ", email='" + email + '\'' + ", tel=" + tel + ", password='" + password + '\'' + ", picUrl='" + picUrl + '\'' + ", siteId='" + siteId + '\'' + ", createDate=" + createDate + ", createBy=" + createBy + ", updateDate=" + updateDate + ", updateId=" + updateId + ", delFlag=" + delFlag + ", remarks='" + remarks + '\'' + '&#125;'; &#125;&#125; 五、创建一个dao层，service层以及controller层SysUserDao.java 123456789101112131415package cn.gausscode.calo.user.dao;import cn.gausscode.calo.user.entity.SysUserEntity;import org.apache.ibatis.annotations.Mapper;@Mapperpublic interface SysUserDao &#123; SysUserEntity get(int id); void insert(SysUserEntity sysUserEntity); void delete(int id); void update(SysUserEntity sysUserEntity);&#125; SysUserController.java 123456789101112131415161718192021222324252627282930313233343536package cn.gausscode.calo.user.controller;import cn.gausscode.calo.user.service.SysUserService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class SysUserController &#123; @Autowired SysUserService sysUserService; @RequestMapping("/get") public String get()&#123; int id = 2; return sysUserService.get(id); &#125; @RequestMapping("/insert") public String insert()&#123; return sysUserService.insert().toString(); &#125; @RequestMapping("/delete") public String delete()&#123; sysUserService.delete(); return "successful delete"; &#125; @RequestMapping("/update") public String update()&#123; return sysUserService.update(); &#125;&#125; SysUserService.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package cn.gausscode.calo.user.service;import cn.gausscode.calo.user.dao.SysUserDao;import cn.gausscode.calo.user.entity.SysUserEntity;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.text.SimpleDateFormat;import java.util.Date;@Servicepublic class SysUserService &#123; @Autowired SysUserDao sysUserDao; public String get(int id)&#123; return sysUserDao.get(id).toString(); &#125; public SysUserEntity insert()&#123; SysUserEntity sysUserEntity = new SysUserEntity(); sysUserEntity.setName("name"); sysUserEntity.setLoginName("loginName"); sysUserEntity.setEmail("email"); sysUserEntity.setTel(123); sysUserEntity.setPassword("password"); sysUserEntity.setPicUrl("pic_url"); sysUserEntity.setSiteId("site_id"); SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); sysUserEntity.setCreateDate(new Date()); sysUserEntity.setCreateBy(1); sysUserEntity.setDelFlag(1); sysUserEntity.setRemarks("remarks"); sysUserDao.insert(sysUserEntity); return sysUserEntity; &#125; public void delete()&#123; sysUserDao.delete(1); &#125; public String update()&#123; SysUserEntity sysUserEntity = new SysUserEntity(); sysUserEntity.setId(1); sysUserEntity.setName("name1"); sysUserEntity.setLoginName("loginName1"); sysUserEntity.setEmail("email1"); sysUserEntity.setTel(1231); sysUserEntity.setPassword("password1"); sysUserEntity.setPicUrl("pic_url1"); sysUserEntity.setSiteId("site_id1"); SimpleDateFormat df = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss1"); sysUserEntity.setCreateDate(new Date()); sysUserEntity.setCreateBy(11); sysUserEntity.setDelFlag(11); sysUserEntity.setRemarks("remarks1"); sysUserDao.update(sysUserEntity); return sysUserDao.get(1).toString(); &#125;&#125; 六、书写mybatis的mapperSysUserMapper.xml 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="cn.gausscode.calo.user.dao.SysUserDao"&gt; &lt;select id="get" parameterType="int" resultType="cn.gausscode.calo.user.entity.SysUserEntity"&gt; select * from sys_user where id = #&#123;id&#125; &lt;/select&gt; &lt;insert id="insert" parameterType="cn.gausscode.calo.user.entity.SysUserEntity"&gt; insert into sys_user (name,login_name,email,tel,password,pic_url,site_id,create_date,create_by,update_date,update_by,del_flag,remarks) values (#&#123;name&#125;,#&#123;loginName&#125;,#&#123;email&#125;,#&#123;tel&#125;,#&#123;password&#125;,#&#123;picUrl&#125;,#&#123;siteId&#125;,#&#123;createDate&#125;,#&#123;createBy&#125;,#&#123;updateDate&#125;,#&#123;updateId&#125;,#&#123;delFlag&#125;,#&#123;remarks&#125;) &lt;/insert&gt; &lt;update id="delete" parameterType="int"&gt; update sys_user set del_flag = 0 where id = #&#123;id&#125; &lt;/update&gt; &lt;update id="update" parameterType="cn.gausscode.calo.user.entity.SysUserEntity"&gt; update sys_user set name = #&#123;name&#125;, login_name = #&#123;loginName&#125;, email = #&#123;email&#125;, tel = #&#123;tel&#125;, password = #&#123;password&#125;, pic_url = #&#123;picUrl&#125;, site_id = #&#123;siteId&#125;, create_date = #&#123;createDate&#125;, create_by = #&#123;createBy&#125;, update_date = #&#123;updateDate&#125;, update_by = #&#123;updateId&#125;, del_flag = #&#123;delFlag&#125;, remarks = #&#123;remarks&#125; where id = #&#123;id&#125; &lt;/update&gt;&lt;/mapper&gt; 至此增删改查功能就完成了 七、总结下面这个是整个项目的结构图 整体来说，mybatis可以大量减少了在sql代码部分的心思，特别是动态sql部分，简直太棒了，不过还没有完全学完，只是今天写了一个demo，之后会慢慢地把整个mybatis系统的学习总结一下]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Swagger报错:java.lang.NumberFormatException: For input string: ""]]></title>
    <url>%2F2018%2F11%2F15%2FSwagger%E6%8A%A5%E9%94%99-java-lang-NumberFormatException-For-input-string%2F</url>
    <content type="text"><![CDATA[使用Swagger和Springfox，我们编写了REST API，我们编写了更多可访问的API。当您运行该应用程序时，您可能会遇到一个奇怪的异常，如下所示：java.lang.NumberFormatException: For input string: &quot;&quot; 这似乎是一个错误，所以你可以忽略那个。如果它让您烦恼，您可以随时更改该文件的日志记录级别，就像他们在该问题中提出的那样。您也可以在application.properties或application.yml中执行此操作： application.properties1logging.level.io.swagger.models.parameters.AbstractSerializableParameter=error application.yml 123logging: level: io.swagger.models.parameters.AbstractSerializableParameter: error 参考文章：https://g00glen00b.be/documenting-rest-api-swagger-springfox/]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Swagger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成Swagger2]]></title>
    <url>%2F2018%2F11%2F14%2FSpringboot%E9%9B%86%E6%88%90Swagger2%2F</url>
    <content type="text"><![CDATA[Springboot集成Swagger2 一、介绍： Swagger是一个功能强大且易于使用的API开发人员工具套件，适用于团队和个人，支持从整个API生命周期（从设计和文档到测试和部署）的开发。Swagger由开源，免费和商用工具组成，允许任何人，从技术工程师到街头智能产品经理，构建每个人都喜欢的令人惊叹的API。Swagger最初是作为2010年设计RESTful API的简单开源规范而开发的。开源工具如Swagger UI，Swagger Editor和Swagger Codegen也被开发用于更好地实现和可视化规范中定义的API。Swagger项目由规范和开源工具组成，非常受欢迎，创建了一个由社区驱动的工具组成的庞大生态系统。2015年，Swagger项目被SmartBear Software收购。Swagger规范被捐赠给Linux基金会并重命名为OpenAPI规范以正式标准化REST API的描述方式。建立OpenAPI倡议是为了以公开和透明的方式指导美洲国家组织的发展。从那以后，Swagger成为最受欢迎的工具套件，可以在API生命周期中充分利用OAS的强大功能。 二、Springboot添加Swagger2依赖1234567891011&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt; 三、添加Swagger2配置文件12345678910111213141516171819202122232425262728293031323334import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.service.Contact;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;import java.util.Collections;@Configuration@EnableSwagger2public class SpringFoxConfig &#123; @Bean public Docket apiDocket()&#123; return new Docket(DocumentationType.SWAGGER_2).select().apis(RequestHandlerSelectors .basePackage("你的controller包，如com.example.controller")).paths(PathSelectors.any()) .build().apiInfo(getApiInfo()); &#125; private ApiInfo getApiInfo()&#123; return new ApiInfo( "TITLE", "DESCIPRION", "VERSION", "TEAMS OF SERVICE URL", new Contact("NAME","URL","EMAIL"), "LICENSE", "LICENSE URL", Collections.emptyList() ); &#125;&#125; WebConfig用于访问静态资源1234567891011121314151617181920212223import org.springframework.context.annotation.Configuration;import org.springframework.http.CacheControl;import org.springframework.web.servlet.config.annotation.EnableWebMvc;import org.springframework.web.servlet.config.annotation.ResourceHandlerRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;import org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;import java.util.concurrent.TimeUnit;nfiguration@EnableWebMvcpublic class WebConfig implements WebMvcConfigurer &#123; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler("swagger-ui.html") .addResourceLocations("classpath:/META-INF/resources/") .setCacheControl(CacheControl.maxAge(1, TimeUnit.HOURS).cachePublic()); registry.addResourceHandler("/webjars/**") .addResourceLocations("classpath:/META-INF/resources/webjars/") .setCacheControl(CacheControl.maxAge(1, TimeUnit.HOURS).cachePublic()); &#125;&#125; 四、添加注解1234567891011121314151617181920212223import io.swagger.annotations.ApiOperation;import io.swagger.annotations.ApiParam;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.util.MultiValueMap;import org.springframework.web.bind.annotation.*;import javax.servlet.http.HttpServletRequest;@RestControllerpublic class ForumController &#123; /** * by zhangjia * 获取板块分页后页数 */ @ApiOperation(value = "获取板块分页后页数",notes = "板块页数") @RequestMapping(value = "/pnum/&#123;fid&#125;/&#123;isDigest&#125;", produces = "application/json;charset=UTF-8") public Response pageNum(@CookieValue(defaultValue = "0") int uid, @CookieValue(defaultValue = "") String sid, @ApiParam @PathVariable int fid, @ApiParam @PathVariable int isDigest, @Autowired HttpServletRequest request) &#123; return forumService.getPageNum(fid, isDigest); &#125; 通过访问：http://localhost:8080/v2/api-docs ，能测试生成的api是否可用。此时返回的是一个json形式的页面，可读性不好。可以通过Swagger UI来生成一个可读性良好的api页面。访问：http://localhost:8080/your-app-root/swagger-ui.html 就可以看到可读性较好的api文档页面。 五、常见注解介绍Swagger通过注解表明该接口会生成文档，包括接口名、请求方法、参数、返回信息的等等。 @Api：修饰整个类，描述Controller的作用@ApiOperation：描述一个类的一个方法，或者说一个接口@ApiParam：单个参数描述@ApiModel：用对象来接收参数@ApiProperty：用对象接收参数时，描述对象的一个字段@ApiResponse：HTTP响应其中1个描述@ApiResponses：HTTP响应整体描述@ApiIgnore：使用该注解忽略这个API@ApiError ：发生错误返回的信息@ApiImplicitParam：一个请求参数@ApiImplicitParams：多个请求参数 参考文章：https://blog.csdn.net/fansunion/article/details/51923720https://blog.csdn.net/saytime/article/details/74937664https://blog.csdn.net/Phone_1070333541/article/details/80949040]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>Swagger2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法与数据结构设计周作业——大整数运算器]]></title>
    <url>%2F2018%2F11%2F13%2F%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%91%A8%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B4%E6%95%B0%E8%BF%90%E7%AE%97%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这次的算法与数据结构设计周作业题目比较简单，一个是求众数与重数，另一个题目则为大整数运算器，由于图形化界面不是硬性要求，所以项目不使用GUI，而是直接在命令行运行，具体题目如下： 众数问题 给定含有n个元素的多重集合S，每个元素在S中出现的次数称为该元素的重数，S中重数最大的元素称为众数。例如，S＝{1, 2 ,2 ,2 ,3 ,5}，S的众数是2，该众数的重数为3。要求对于给定的由n个自然数组成的多重集合S，计算S的众数及其重数。 实现该问题并不难，使用Java的HashMap创建键值对，将数字最为key，出现的次数作为value，遍历一次数组即可确定众数以及重数，代码如下。123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;public class Unity &#123; public static void main(String[] args) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(2); list.add(2); list.add(2); list.add(13); list.add(1); list.add(1); Map&lt;String, Integer&gt; map = unity(list); System.out.println(map); &#125; private static Map&lt;String, Integer&gt; unity(List&lt;Integer&gt; list) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); int max = list.get(0); int maxNum = 1; for (int a : list) &#123; if (map.containsKey(a)) &#123; map.put(a, map.get(a) + 1); if (maxNum &lt; map.get(a)) &#123; maxNum = map.get(a); max = a; &#125; &#125; else &#123; map.put(a, 1); &#125; &#125; Map&lt;String, Integer&gt; maxMap = new HashMap&lt;&gt;(); maxMap.put("众数", max); maxMap.put("重数", maxNum); return maxMap; &#125;&#125; 大整数运算器 （一）课题内容实现一个简单的大整数算术运算程序。主要功能计算两个大整数（20位）的加减法等算术运算，按指定的格式输出结果。通过此课题，熟练掌握字符串、格式输出、文件的各种操作，以及基本的计算算法思想的应用。（二）课题要求 基本要求(1) 输入功能：能实现从键盘或文本文件输入代表大整数的字符串并用合适的结构存储，能实现输入整个表达式（含大整数和相应的运算符）。(2) 输出功能：将算式及计算结果按照一定格式批量输出到屏幕及另一个文本文件中。(3) 判断功能：能够对输入的表达式判断其正确与否，如果表达式不正确则无法进行运算；表达式正确的情况下执行运算功能。(4) 运算功能：可以实现大整数的加法、减法、乘法和除法运算。 扩展要求(1) 实现一些常用的数学函数对大整数进行运算，如：大整数的平方根运算sqrt(x)、大整数的幂运算pow(x,y)等。【其他要求】（1）变量、函数命名符合规范。（2）注释详细：每个变量都要求有注释说明用途；函数有注释说明功能，对参数、返回值也要以注释的形式说明用途；关键的语句段要求有注释解释。（3）程序的层次清晰，可读性强。（4）界面美观，交互方便。 实现该问题的难度主要是在除法，加减乘皆可将大整数转化为字符串，然后进行相应的运算，除法我是用的是之前在LeetCode刷题时遇到的一个题目，20.两数相除这个题目的启发，点击传送门 可以看看我之前总结的在不使用除法以及取余的情况下如何进行除法运算。关于扩展要求我目前还未添加，添加之后我会重新修改这篇博文的！，实现的代码格式。(代码比较长，可以点击传送门进GitHub直接下载整个作业源码)运行界面比较简单，示例如下： 手动输入 从文件中读取123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537import java.io.*;import java.util.ArrayList;import java.util.List;import java.util.Scanner;import java.util.regex.Matcher;import java.util.regex.Pattern;public class BigNum &#123; public static void main(String[] args) throws IOException &#123; while (true) &#123; Scanner sc = new Scanner(System.in); System.out.println("--------------------\n请选择方式:"); System.out.println("1.手动输入\n2.从文件读取\n0.退出系统\n--------------------"); String choice = sc.nextLine(); switch (choice) &#123; case "1": arithmetic(); break; case "2": arithmeticByFile(); break; case "0": break; default: System.out.println("非法输入，请重试！"); &#125; if (choice.equals("0")) &#123; break; &#125; &#125; &#125; /** * 手动输入 */ private static void arithmetic() &#123; while (true) &#123; System.out.println("\n\n--------------------\n请选择要执行的运算："); System.out.println("1.加法"); System.out.println("2.减法"); System.out.println("3.乘法"); System.out.println("4.除法"); System.out.println("0.退出\n--------------------"); Scanner sc = new Scanner(System.in); String choice = sc.nextLine(); switch (choice) &#123; case "1": Scanner sc1 = new Scanner(System.in); System.out.print("\n\n请输入被加数："); String a1 = sc1.nextLine(); System.out.print("请输入加数："); String b1 = sc1.nextLine(); if (checkNum(a1) &amp;&amp; checkNum(b1)) &#123; String num1 = add(a1, b1); System.out.println(mergeStr(a1, b1, choice, num1)); &#125; else &#123; System.out.println("数字格式有误，请检查"); &#125; break; case "2": Scanner sc2 = new Scanner(System.in); System.out.print("\n\n请输入被减数："); String a2 = sc2.nextLine(); System.out.print("请输入减数："); String b2 = sc2.nextLine(); if (checkNum(a2) &amp;&amp; checkNum(b2)) &#123; String num2 = subtract(a2, b2); System.out.println(mergeStr(a2, b2, choice, num2)); &#125; else &#123; System.out.println("数字格式有误，请检查"); &#125; break; case "3": Scanner sc3 = new Scanner(System.in); System.out.print("\n\n请输入被乘数："); String a3 = sc3.nextLine(); System.out.print("请输入乘数："); String b3 = sc3.nextLine(); if (checkNum(a3) &amp;&amp; checkNum(b3)) &#123; String num3 = mul(a3, b3); System.out.println(mergeStr(a3, b3, choice, num3)); &#125; else &#123; System.out.println("数字格式有误，请检查"); &#125; break; case "4": Scanner sc4 = new Scanner(System.in); System.out.print("\n\n请输入被除数："); String a4 = sc4.nextLine(); System.out.print("请输入除数："); String b4 = sc4.nextLine(); if (checkNum(a4) &amp;&amp; checkNum(b4)) &#123; String num4 = division(a4, b4); System.out.println(mergeStr(a4, b4, choice, num4)); &#125; else &#123; System.out.println("数字格式有误，请检查"); &#125; break; case "0": break; default: System.out.println("非法输入，请重试！"); &#125; if (choice.equals("0")) &#123; break; &#125; &#125; &#125; /** * 以文件的形式来计算 */ private static void arithmeticByFile() &#123; String str = ""; try &#123; str = readFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; List&lt;String&gt; list = checkFileStr(str); if (list.size() == 0) &#123; System.out.println("字符串匹配时有误，请检查"); &#125; else &#123; String num = ""; String num1 = list.get(0); String sign = list.get(1); String num2 = list.get(2); if (!checkNum(num1) || !checkNum(num2)) &#123; System.out.println("数字格式有误，请检查"); return; &#125; switch (sign) &#123; case "+": num = add(num1, num2); break; case "-": num = subtract(num1, num2); break; case "*": num = mul(num1, num2); break; case "/": num = division(num1, num2); default: System.out.println("运算符有误，请检查"); return; &#125; try &#123; writeFile(str + " = " + num); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 检查文件中字符串是否合法 * * @param str 需要检查的字符串 * @return 是否合法 */ private static List&lt;String&gt; checkFileStr(String str) &#123; String pattern = "\\((-?[\\d+])\\)(\\W)\\((-?[\\d+])\\)"; List&lt;String&gt; list = new ArrayList&lt;&gt;(); // 创建 Pattern 对象 Pattern r = Pattern.compile(pattern); // 现在创建 matcher 对象 Matcher m = r.matcher(str); if (m.find()) &#123; list.add(m.group(1)); list.add(m.group(2)); list.add(m.group(3)); &#125; return list; &#125; /** * 检查输入的数字是否合法 * * @param str 需要检查的字符串 * @return 是否合法 */ private static boolean checkNum(String str) &#123; String pattern = "^-?\\d+$"; // 创建 Pattern 对象 Pattern r = Pattern.compile(pattern); // 现在创建 matcher 对象 Matcher m = r.matcher(str); return m.find() &amp;&amp; m.group(0).equals(str); &#125; /** * 读文件 * * @return 若文件不存在则返回空，正常则返回文件中字符串 */ private static String readFile() throws IOException &#123; File file = new File("file/arithmetic.txt"); if (file.exists()) &#123; FileInputStream fileInputStream = new FileInputStream(file); InputStreamReader inputStreamReader = new InputStreamReader(fileInputStream, "UTF-8"); BufferedReader bufferedReader = new BufferedReader(inputStreamReader); String line = bufferedReader.readLine(); bufferedReader.close(); inputStreamReader.close(); fileInputStream.close(); System.out.println("\n\n获取到的算式为: " + line); return line; &#125; else &#123; return ""; &#125; &#125; /** * 写文件 * * @param result 计算结果 */ private static void writeFile(String result) throws IOException &#123; File file = new File("file/result.txt"); if (file.exists()) &#123; FileOutputStream fileOutputStream = new FileOutputStream(file); OutputStreamWriter outputStreamWriter = new OutputStreamWriter(fileOutputStream, "UTF-8"); BufferedWriter bufferedWriter = new BufferedWriter(outputStreamWriter); bufferedWriter.write(result); bufferedWriter.close(); outputStreamWriter.close(); fileOutputStream.close(); System.out.println("计算结果为: " + result + "\n\n"); &#125; else &#123; System.out.println("文件不存在"); &#125; &#125; /** * 合并字符串 * * @param num1 数1 * @param num2 数2 * @param sign 运算符 * @param num 结果 * @return 合并后的字符串 */ private static String mergeStr(String num1, String num2, String sign, String num) &#123; switch (sign) &#123; case "1": sign = "+"; break; case "2": sign = "-"; break; case "3": sign = "*"; break; case "4": sign = "/"; break; &#125; return "\n答案为:" + num1 + " " + sign + " " + num2 + " = " + num; &#125; /** * 大整数的加法 * * @param a 被加数 * @param b 加数 * @return 和 */ private static String add(String a, String b) &#123; boolean plus = true; if (a.charAt(0) == '-' &amp;&amp; b.charAt(0) != '-') &#123; return subtract(b, a.replace("-", "")); &#125; else if (a.charAt(0) != '-' &amp;&amp; b.charAt(0) == '-') &#123; return subtract(a, b.replace("-", "")); &#125; else if (a.charAt(0) == '-' &amp;&amp; b.charAt(0) == '-') &#123; plus = false; &#125; a = a.replace("-", ""); b = b.replace("-", ""); if (a.length() &lt; b.length()) &#123; String tmp = a; a = b; b = tmp; &#125; char[] aList = a.toCharArray(); char[] bList = b.toCharArray(); int aLen = aList.length; int bLen = bList.length; StringBuilder num = new StringBuilder(); //是否进位 boolean carry = false; while (aLen &gt; 0 &amp;&amp; bLen &gt; 0) &#123; int addNum = Integer.parseInt(String.valueOf(aList[aLen - 1])) + Integer.parseInt(String.valueOf(bList[bLen - 1])); if (carry) &#123; addNum += 1; &#125; if (addNum &gt;= 10) &#123; addNum %= 10; carry = true; &#125; else &#123; carry = false; &#125; num.insert(0, String.valueOf(addNum)); aLen--; bLen--; &#125; while (aLen &gt; 0) &#123; int addNum = Integer.parseInt(String.valueOf(aList[aLen - 1])); if (carry) &#123; addNum += 1; if (addNum &gt;= 10) &#123; addNum %= 10; carry = true; &#125; else &#123; carry = false; &#125; &#125; num.insert(0, addNum); aLen--; &#125; if (carry) &#123; num.insert(0, 1); &#125; if (!plus) &#123; num.insert(0, "-"); &#125; return num.toString(); &#125; /** * 大整数减法 * * @param a 被减数 * @param b 减数 * @return 差 */ private static String subtract(String a, String b) &#123; boolean isPlus = true; if (a.charAt(0) != '-' &amp;&amp; b.charAt(0) == '-') &#123; return add(a, b.replace("-", "")); &#125; else if (a.charAt(0) == '-' &amp;&amp; b.charAt(0) != '-') &#123; return add(a, b.replace("-", "")); &#125; else if (a.charAt(0) == '-' &amp;&amp; b.charAt(0) == '-') &#123; String tmp = a.replace("-", ""); a = b.replace("-", ""); b = tmp; isPlus = false; &#125; a = a.replace("-", ""); b = b.replace("-", ""); if (a.length() &lt; b.length()) &#123; String tmp = a; a = b; b = tmp; isPlus = false; &#125; char[] aList = a.toCharArray(); char[] bList = b.toCharArray(); int aLen = aList.length; int bLen = bList.length; if (aLen == bLen) &#123; for (int i = aLen; i &gt; 0; i--) &#123; if (Integer.parseInt(String.valueOf(aList[i - 1])) &lt; Integer.parseInt(String.valueOf(bList[i - 1]))) &#123; isPlus = false; char[] tmpList = aList; aList = bList; bList = tmpList; break; &#125; &#125; &#125; StringBuilder num = new StringBuilder(); //是否借位 boolean borrow = false; while (aLen &gt; 0 &amp;&amp; bLen &gt; 0) &#123; int subtractNum; int m = Integer.parseInt(String.valueOf(aList[aLen - 1])); int n = Integer.parseInt(String.valueOf(bList[bLen - 1])); if (borrow) &#123; if (m - 1 &lt; n) &#123; subtractNum = m - 1 + 10 - n; borrow = true; &#125; else &#123; subtractNum = m - 1 - n; borrow = false; &#125; &#125; else &#123; if (m &lt; n) &#123; subtractNum = m + 10 - n; borrow = true; &#125; else &#123; subtractNum = m - n; borrow = false; &#125; &#125; num.insert(0, String.valueOf(subtractNum)); aLen--; bLen--; &#125; while (aLen &gt; 0) &#123; int subtractNum = Integer.parseInt(String.valueOf(aList[aLen - 1])); if (borrow) &#123; subtractNum -= 1; if (subtractNum &lt; 0) &#123; subtractNum = -subtractNum; borrow = true; &#125; else &#123; borrow = false; &#125; &#125; num.insert(0, subtractNum); aLen--; &#125; if (!isPlus) &#123; num.insert(0, "-"); &#125; return String.valueOf(Integer.parseInt(num.toString())); &#125; /** * 大整数乘法 * * @param a 被乘数 * @param b 乘数 * @return 积 */ private static String mul(String a, String b) &#123; boolean isPlus = true; if (a.charAt(0) == '-' &amp;&amp; b.charAt(0) != '-') &#123; isPlus = false; &#125; else if (a.charAt(0) != '-' &amp;&amp; b.charAt(0) == '-') &#123; isPlus = false; &#125; if (Integer.parseInt(a) == 0 || Integer.parseInt(b) == 0) &#123; return "0"; &#125; a = a.replace("-", ""); b = b.replace("-", ""); if (a.length() &lt; b.length()) &#123; String tmp = a; a = b; b = tmp; &#125; char[] aList = a.toCharArray(); char[] bList = b.toCharArray(); int bLen = bList.length; String num = "0"; while (bLen &gt; 0) &#123; int aLen = aList.length; StringBuilder mulNum = new StringBuilder(); int carry = 0; while (aLen &gt; 0) &#123; int sigleMulNum = Integer.parseInt(String.valueOf(bList[bLen - 1])) * Integer.parseInt(String.valueOf(aList[aLen - 1])) + carry; carry = sigleMulNum / 10; sigleMulNum %= 10; mulNum.insert(0, sigleMulNum); aLen--; &#125; if (carry != 0) &#123; mulNum.insert(0, carry); &#125; num = String.valueOf(Integer.parseInt(num) + Integer.parseInt(mulNum.toString()) * Integer.parseInt(String.valueOf((int) Math.pow(10, bList.length - bLen)))); bLen--; &#125; if (!isPlus) &#123; num = "-" + num; &#125; return num; &#125; /** * 大整数除法 * * @param a 被除数 * @param b 除数 * @return 商，只保留整数位 */ private static String division(String a, String b) &#123; boolean isPlus = true; int aSign = a.charAt(0) == '-' ? -1 : 1; int bSign = b.charAt(0) == '-' ? -1 : 1; if (aSign * bSign &lt; 0) &#123; isPlus = false; &#125; a = a.replace("-", ""); b = b.replace("-", ""); if (subtract(a, b).charAt(0) == '-') &#123; return "0"; &#125; String num = "0"; while (subtract(a, b).charAt(0) != '-') &#123; String numTimes = "1"; String m = b; while (subtract(a, mul(m, "2")).charAt(0) != '-') &#123; numTimes = mul(numTimes, "2"); m = mul(m, "2"); &#125; num = add(num, numTimes); a = subtract(a, m); &#125; if (!isPlus) &#123; num = "-" + num; &#125; return num; &#125; &#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode：29. 两数相除（Java）]]></title>
    <url>%2F2018%2F10%2F23%2FLeetCode%EF%BC%9A29-%E4%B8%A4%E6%95%B0%E7%9B%B8%E9%99%A4%EF%BC%88Java%EF%BC%89%2F</url>
    <content type="text"><![CDATA[29. 两数相除（Java） 参考文章：https://blog.csdn.net/qq_31442743/article/details/81181554给定两个整数，被除数 dividend和除数 divisor。将两数相除，要求不使用乘法、除法和 mod 运算符。返回被除数 dividend除以除数 divisor得到的商。示例 1:12输入: dividend = 10, divisor = 3输出: 3 示例 2:12输入: dividend = 7, divisor = -3输出: -2 说明: 被除数和除数均为 32 位有符号整数。 除数不为 0。 假设我们的环境只能存储 32 位有符号整数，其数值范围是 [−231, 231 − 1]。本题中，如果除法结果溢出，则返回 231 − 1。 二进制相关的形式来实现。”&lt;&lt;”和”&gt;&gt;”分别为将数的整体左移和右移，例如a&lt;&lt;1表示将a向左移动一位，即变为原来的二倍。 思路:当被除数大于等于除数时(否则的话就为0了)，我们设置两个变量t和p，并分别初始化为除数和1(最小的情况)，当被除数大于等于t的二倍时，将t和p同时扩大二倍(左移)，并将返回值加上p，除数减去t。和二进制类似，例如29除以8，8扩大二倍，16小于29，再扩大二倍，超过29，于是29减去之前的16，返回值加上2。第二次循环时因为此时的13小于8的二倍，故加上1，整个循环结束，最终结果为2+1=3，很明显符合。此外注意判断结果正负号的正负号时亦或的作用。 代码如下: 123456789101112131415161718192021222324class Solution &#123; public int divide(int dividend, int divisor) &#123; if(divisor == 0 || (dividend == Integer.MIN_VALUE &amp;&amp; divisor == -1))&#123;//考虑特殊情况 return Integer.MAX_VALUE; &#125; int sign = ((dividend &lt; 0) ^ (divisor &lt; 0)) ? -1 : 1;//异或运算 long ms = (long)dividend; long ns = (long)divisor; ms = Math.abs(ms); ns = Math.abs(ns); int num = 0; while(ms &gt;= ns)&#123; long m = ns; long n = 1; while(ms &gt;= (m &lt;&lt; 1))&#123; m &lt;&lt;= 1; n &lt;&lt;= 1; &#125; num += n; ms -= m; &#125; return num * sign; &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构与算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot集成WebSocket功能]]></title>
    <url>%2F2018%2F10%2F13%2FSpringboot%E9%9B%86%E6%88%90WebSocket%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[Springboot集成WebSocket功能 由于MT管理器论坛需要添加聊天功能，在网上搜了很多，最后发现了websocket可以用于实时通信和聊天室功能，然后看了慕课上的一个网课，跟着他做出来了一个demo，下面就来看一下什么是websocket吧 在菜鸟教程中的解释是这样的 WebSocket 是 HTML5 开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。WebSocket 使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。在 WebSocket API 中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。在 WebSocket API 中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。现在，很多网站为了实现推送技术，所用的技术都是 Ajax 轮询。轮询是在特定的的时间间隔（如每1秒），由浏览器对服务器发出HTTP请求，然后由服务器返回最新的数据给客户端的浏览器。这种传统的模式带来很明显的缺点，即浏览器需要不断的向服务器发出请求，然而HTTP请求可能包含较长的头部，其中真正有效的数据可能只是很小的一部分，显然这样会浪费很多的带宽等资源。HTML5 定义的 WebSocket 协议，能更好的节省服务器资源和带宽，并且能够更实时地进行通讯。浏览器通过 JavaScript 向服务器发出建立 WebSocket 连接的请求，连接建立以后，客户端和服务器端就可以通过 TCP 连接直接交换数据。当你获取 Web Socket 连接后，你可以通过 send() 方法来向服务器发送数据，并通过 onmessage 事件来接收服务器返回的数据。以下 API 用于创建 WebSocket 对象。 简单来说就是一个可以不用使用轮训就可以实现后端主动向前端发送消息的一个协议。现在我们来看一下如何在springboot中实现这个 1.添加依赖123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-websocket&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;webjars-locator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;sockjs-client&lt;/artifactId&gt; &lt;version&gt;1.0.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;stomp-websocket&lt;/artifactId&gt; &lt;version&gt;2.3.3&lt;/version&gt;&lt;/dependency&gt; 2.写配置文件WebSocketConfig.java 123456789101112131415161718192021222324252627282930313233package xyz.suiwo.websocketdemo.Config;import org.springframework.context.annotation.Configuration;import org.springframework.messaging.simp.config.MessageBrokerRegistry;import org.springframework.web.socket.config.annotation.EnableWebSocketMessageBroker;import org.springframework.web.socket.config.annotation.StompEndpointRegistry;import org.springframework.web.socket.config.annotation.WebSocketMessageBrokerConfigurer;@Configuration@EnableWebSocketMessageBrokerpublic class WebSocketConfig implements WebSocketMessageBrokerConfigurer &#123; /** * 注册端点，发布或者订阅消息的时候需要连接此端点 * setAllowedOrigins 非必须，*表示允许其他域进行连接 * withSockJS 表示开始sockejs支持 */ @Override public void registerStompEndpoints(StompEndpointRegistry registry) &#123; registry.addEndpoint("/endpoint-websocket").setAllowedOrigins("*").withSockJS(); &#125; /** * 配置消息代理(中介) * enableSimpleBroker 服务端推送给客户端的路径前缀 * setApplicationDestinationPrefixes 客户端发送数据给服务器端的一个前缀 */ @Override public void configureMessageBroker(MessageBrokerRegistry registry) &#123; registry.enableSimpleBroker("/getMessage"); registry.setApplicationDestinationPrefixes("/sendMessage"); &#125;&#125; 3.消息的实体类这是我们需要新建一个model用来表示发送的消息类Message.java 12345678910111213141516171819202122232425262728293031323334353637383940package xyz.suiwo.websocketdemo.model;public class Message &#123; private String fromUser; private String toUser; private String message; public String getFromUser() &#123; return fromUser; &#125; public void setFromUser(String fromUser) &#123; this.fromUser = fromUser; &#125; public String getToUser() &#123; return toUser; &#125; public void setToUser(String toUser) &#123; this.toUser = toUser; &#125; public String getMessage() &#123; return message; &#125; public void setMessage(String message) &#123; this.message = message; &#125; @Override public String toString() &#123; return "Message&#123;" + "fromUser='" + fromUser + '\'' + ", toUser='" + toUser + '\'' + ", message='" + message + '\'' + '&#125;'; &#125;&#125; 4.controller方法我们一共需要书写两个controller一个是用来将消息主动推送给前端（被发送方的），还有一个是将消息以treemap的方式发送给前端让前端（当前用户的）展示。ChatController.java(用于将消息直接展示给前端显示) 12345678910111213141516171819202122package xyz.suiwo.websocketdemo.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RestController;import javax.servlet.http.HttpServletRequest;import java.util.TreeMap;@RestControllerpublic class ChatController &#123; @RequestMapping(value = "/chatMessage", method = RequestMethod.POST) public TreeMap&lt;String, String&gt; add(@Autowired HttpServletRequest request) &#123; TreeMap&lt;String, String&gt; treeMap = new TreeMap&lt;String, String&gt;(); treeMap.put("toUser", request.getParameter("toUser")); treeMap.put("fromUser", request.getParameter("fromUser")); treeMap.put("message", request.getParameter("message")); return treeMap; &#125;&#125; WebSocketController.java（用于将消息主动推送给被发送方） 123456789101112131415161718192021package xyz.suiwo.websocketdemo.controller;import org.springframework.messaging.handler.annotation.MessageMapping;import org.springframework.stereotype.Controller;import xyz.suiwo.websocketdemo.Service.WebSocketService;import xyz.suiwo.websocketdemo.model.Message;@Controllerpublic class WebSocketController &#123; private WebSocketService webSocketService; public WebSocketController(WebSocketService webSocketService) &#123; this.webSocketService = webSocketService; &#125; @MessageMapping(value = "/single/chat") public void sendMessage(Message message)&#123; webSocketService.sendMessageTo(message.getFromUser(), message.getToUser(), message.getMessage()); &#125;&#125; 4.前端展示前端的接收我们需要用的socket.js，具体使用方式。。。我也没有仔细看，只是直接把网站提供的前端源码copy过来了，需要的同学可以参考一下app.js 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677var stompClient = null;function setConnected(connected) &#123; $("#connect").prop("disabled", connected); $("#disconnect").prop("disabled", !connected); if (connected) &#123; $("#conversation").show(); &#125; else &#123; $("#conversation").hide(); &#125; $("#notice").html("");&#125;function connect() &#123; var from = $("#from").val(); var to = $("#to").val(); var socket = new SockJS('/endpoint-websocket'); stompClient = Stomp.over(socket); stompClient.connect(&#123;&#125;, function (frame) &#123; setConnected(true); console.log('Connected: ' + frame); stompClient.subscribe('/getMessage/single/' + from + to, function (result) &#123; showContent(result.body); &#125;); &#125;);&#125;function disconnect() &#123; if (stompClient !== null) &#123; stompClient.disconnect(); &#125; setConnected(false); console.log("Disconnected");&#125;function sendName() &#123; var toUser = document.getElementById("to").value; var fromUser = document.getElementById("from").value; var message = document.getElementById("content").value; stompClient.send("/sendMessage/single/chat", &#123;&#125;, JSON.stringify(&#123; 'message': $("#content").val(), 'toUser': $("#to").val(), 'fromUser': $("#from").val() &#125;)); if (window.XMLHttpRequest) &#123; // IE7+, Firefox, Chrome, Opera, Safari 浏览器执行代码 xmlhttp = new XMLHttpRequest(); &#125; else &#123; // IE6, IE5 浏览器执行代码 xmlhttp = new ActiveXObject("Microsoft.XMLHTTP"); &#125; xmlhttp.open("POST", "/chatMessage", false); xmlhttp.setRequestHeader('content-type', 'application/x-www-form-urlencoded'); //正式发送请求 xmlhttp.send('toUser=' + toUser + '&amp;fromUser=' + fromUser + '&amp;message=' + message);&#125;function showContent(body) &#123; $("#notice").append("&lt;tr&gt;&lt;td&gt;" + body + "&lt;/td&gt;&lt;/tr&gt;");&#125;$(function () &#123; $("form").on('submit', function (e) &#123; e.preventDefault(); &#125;); $("#connect").click(function () &#123; connect(); &#125;); $("#disconnect").click(function () &#123; disconnect(); &#125;); $("#send").click(function () &#123; sendName(); &#125;);&#125;); 这个代码我已经放在了GitHub上有需要的同学可以参考一下 传送门 视频教程 传送门]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Springboot</tag>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode：10 正则表达式匹配（Java）]]></title>
    <url>%2F2018%2F10%2F05%2FLeetCode%EF%BC%9A10-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D%EF%BC%88Java%EF%BC%89%2F</url>
    <content type="text"><![CDATA[LeetCode：10 正则表达式匹配 给定一个字符串 (s) 和一个字符模式 (p)。实现支持 ‘.’ 和 ‘*’ 的正则表达式匹配。12&apos;.&apos; 匹配任意单个字符。&apos;*&apos; 匹配零个或多个前面的元素。 匹配应该覆盖整个字符串 (s) ，而不是部分字符串。说明:12s 可能为空，且只包含从 a-z 的小写字母。p 可能为空，且只包含从 a-z 的小写字母，以及字符 . 和 *。 示例 1:12345输入:s = &quot;aa&quot;p = &quot;a&quot;输出: false解释: &quot;a&quot; 无法匹配 &quot;aa&quot; 整个字符串。 示例 2:12345输入:s = &quot;aa&quot;p = &quot;a*&quot;输出: true解释: &apos;*&apos; 代表可匹配零个或多个前面的元素, 即可以匹配 &apos;a&apos; 。因此, 重复 &apos;a&apos; 一次, 字符串可变为 &quot;aa&quot;。 示例 3:12345输入:s = &quot;ab&quot;p = &quot;.*&quot;输出: true解释: &quot;.*&quot; 表示可匹配零个或多个(&apos;*&apos;)任意字符(&apos;.&apos;)。 示例 4:12345输入:s = &quot;aab&quot;p = &quot;c*a*b&quot;输出: true解释: &apos;c&apos; 可以不被重复, &apos;a&apos; 可以被重复一次。因此可以匹配字符串 &quot;aab&quot;。 示例 5:1234输入:s = &quot;mississippi&quot;p = &quot;mis*is*p*.&quot;输出: false 代码如下：123456789101112131415161718192021public class Solution &#123; public boolean isMatch(String text, String pattern) &#123; //如果都为空则匹配成功 if (pattern.isEmpty()) return text.isEmpty(); //第一个是否匹配上 boolean first_match = (!text.isEmpty() &amp;&amp; (pattern.charAt(0) == text.charAt(0) || pattern.charAt(0) == '.')); if (pattern.length() &gt;= 2 &amp;&amp; pattern.charAt(1) == '*') &#123; //看有没有可能,剩下的pattern匹配上全部的text //看有没有可能,剩下的text匹配整个pattern //isMatch(text, pattern.substring(2)) 指当p第二个为*时，前面的字符不影响匹配所以可以忽略，所以将*以及*之前的一个字符删除后匹配之后的字符，这就是为什么用pattern.substring(2) //如果第一个已经匹配成功，并且第二个字符为*时，这是我们就要判断之后的需要匹配的字符串是否是多个前面的元素（*的功能），这就是first_match &amp;&amp; isMatch(text.substring(1), pattern))的意义 return (isMatch(text, pattern.substring(2)) || (first_match &amp;&amp; isMatch(text.substring(1), pattern))); &#125; else &#123; //没有星星的情况:第一个字符相等,而且剩下的text,匹配上剩下的pattern，没有星星且第一个匹配成功，那么s和p同时向右移动一位看是否仍然能匹配成功 return first_match &amp;&amp; isMatch(text.substring(1), pattern.substring(1)); &#125; &#125;&#125; 本文代码参考：https://blog.csdn.net/hit1110310422/article/details/80791446]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构与算法</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java的接口和抽象类]]></title>
    <url>%2F2018%2F09%2F02%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E7%9A%84%E6%8E%A5%E5%8F%A3%E5%92%8C%E6%8A%BD%E8%B1%A1%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[一.抽象类 在了解抽象类之前，先来了解一下抽象方法。抽象方法是一种特殊的方法：它只有声明，而没有具体的实现。抽象方法的声明格式为：1abstract void fun(); 抽象方法必须用abstract关键字进行修饰。如果一个类含有抽象方法，则称这个类为抽象类，抽象类必须在类前用abstract关键字修饰。因为抽象类中含有无具体实现的方法，所以不能用抽象类创建对象。 下面要注意一个问题：在《JAVA编程思想》一书中，将抽象类定义为“包含抽象方法的类”，但是后面发现如果一个类不包含抽象方法，只是用abstract修饰的话也是抽象类。也就是说抽象类不一定必须含有抽象方法。个人觉得这个属于钻牛角尖的问题吧，因为如果一个抽象类不包含任何抽象方法，为何还要设计为抽象类？所以暂且记住这个概念吧，不必去深究为什么。123[public] abstract class ClassName &#123; abstract void fun();&#125; 从这里可以看出，抽象类就是为了继承而存在的，如果你定义了一个抽象类，却不去继承它，那么等于白白创建了这个抽象类，因为你不能用它来做任何事情。对于一个父类，如果它的某个方法在父类中实现出来没有任何意义，必须根据子类的实际需求来进行不同的实现，那么就可以将这个方法声明为abstract方法，此时这个类也就成为abstract类了。 包含抽象方法的类称为抽象类，但并不意味着抽象类中只能有抽象方法，它和普通类一样，同样可以拥有成员变量和普通的成员方法。注意，抽象类和普通类的主要有三点区别： 抽象方法必须为public或者protected（因为如果为private，则不能被子类继承，子类便无法实现该方法），缺省情况下默认为public。 抽象类不能用来创建对象； 如果一个类继承于一个抽象类，则子类必须实现父类的抽象方法。如果子类没有实现父类的抽象方法，则必须将子类也定义为为abstract类。 在其他方面，抽象类和普通的类并没有区别。 二.接口 接口，英文称作interface，在软件工程中，接口泛指供别人调用的方法或者函数。从这里，我们可以体会到Java语言设计者的初衷，它是对行为的抽象。在Java中，定一个接口的形式如下： 123[public] interface InterfaceName &#123; &#125; 接口中可以含有 变量和方法。但是要注意，接口中的变量会被隐式地指定为public static final变量（并且只能是public static final变量，用private修饰会报编译错误），而方法会被隐式地指定为public abstract方法且只能是public abstract方法（用其他关键字，比如private、protected、static、 final等修饰会报编译错误），并且接口中所有的方法不能有具体的实现，也就是说，接口中的方法必须都是抽象方法。从这里可以隐约看出接口和抽象类的区别，接口是一种极度抽象的类型，它比抽象类更加“抽象”，并且一般情况下不在接口中定义变量。 要让一个类遵循某组特地的接口需要使用implements关键字，具体格式如下：123class ClassName implements Interface1,Interface2,[....]&#123;&#125; 可以看出，允许一个类遵循多个特定的接口。如果一个非抽象类遵循了某个接口，就必须实现该接口中的所有方法。对于遵循某个接口的抽象类，可以不实现该接口中的抽象方法。 抽象类与接口的区别： 1、概念不一样。接口是对动作的抽象，抽象类是对本质的抽象。抽象类表示的是，这个对象是什么。接口表示的是，这个对象能做什么。比如，男人，女人，这两个类（如果是类的话……），他们的抽象类是人。说明，他们都是人。人可以吃东西，狗也可以吃东西，你可以把“吃东西”定义成一个接口，然后让这些类去实现它。所以，在高级语言上，一个类只能继承一个类（抽象类）(正如人不可能同时是生物和非生物)，但是可以实现多个接口(吃饭接口、走路接口)。 2、使用不一样： a.抽象类 和 接口 都是用来抽象具体对象的. 但是接口的抽象级别最高 b.抽象类可以有具体的方法 和属性, 接口只能有抽象方法和不可变常- 量 c.抽象类主要用来抽象类别,接口主要用来抽象功能. d.抽象类中，且不包含任何实现，派生类必须覆盖它们。接口中所有方法都必须是未实现的。 e.接口是设计的结果 ，抽象类是重构的结果 3、使用方向：当你关注一个事物的本质的时候，用抽象类；当你关注一个操作的时候，用接口。注意：抽象类的功能要远超过接口，但是，定义抽象类的代价高。因为高级语言来说（从实际设计上来说也是）每个类只能继承一个类。在这个类中，你必须继承或编写出其所有子类的所有共性。虽然接口在功能上会弱化许多，但是它只是针对一个动作的描述。而且你可以在一个类中同时实现多个接口。在设计阶段会降低难度的。 下面看一个网上流传最广泛的例子：门和警报的例子：门都有open( )和close( )两个动作，此时我们可以定义通过抽象类和接口来定义这个抽象概念： 1234abstract class Door &#123; public abstract void open(); public abstract void close();&#125; 或者： 1234interface Door &#123; public abstract void open(); public abstract void close();&#125; 但是现在如果我们需要门具有报警alarm( )的功能，那么该如何实现？下面提供两种思路： 1）将这三个功能都放在抽象类里面，但是这样一来所有继承于这个抽象类的子类都具备了报警功能，但是有的门并不一定具备报警功能； 2）将这三个功能都放在接口里面，需要用到报警功能的类就需要实现这个接口中的open( )和close( )，也许这个类根本就不具备open( )和close( )这两个功能，比如火灾报警器。 从这里可以看出， Door的open() 、close()和alarm()根本就属于两个不同范畴内的行为，open()和close()属于门本身固有的行为特性，而alarm()属于延伸的附加行为。因此最好的解决办法是单独将报警设计为一个接口，包含alarm()行为,Door设计为单独的一个抽象类，包含open和close两种行为。再设计一个报警门继承Door类和实现Alarm接口。 1234567891011121314151617181920interface Alram &#123; void alarm();&#125; abstract class Door &#123; void open(); void close();&#125; class AlarmDoor extends Door implements Alarm &#123; void oepn() &#123; //.... &#125; void close() &#123; //.... &#125; void alarm() &#123; //.... &#125;&#125; 参考文章：https://blog.csdn.net/u012092924/article/details/78342193https://www.cnblogs.com/dolphin0520/p/3811437.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java异常处理 Exception,error,运行时异常和一般异常有何异同]]></title>
    <url>%2F2018%2F08%2F19%2Fjava%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86-Exception-error-%E8%BF%90%E8%A1%8C%E6%97%B6%E5%BC%82%E5%B8%B8%E5%92%8C%E4%B8%80%E8%88%AC%E5%BC%82%E5%B8%B8%E6%9C%89%E4%BD%95%E5%BC%82%E5%90%8C%2F</url>
    <content type="text"><![CDATA[在java中，异常对象都是派生于Throwable类的一个实例。如果java内置的异常类不能够满足需求，用户还可以创建自己的异常类。 Exception 和 Error 都是继承了 Throwable 类，在 Java 中只有 Throwable 类型的实例才可以被抛出（throw）或者捕获（catch），它是异常处理机制的基本组成类型。Exception 和 Error 体现了 Java 平台设计者对不同异常情况的分类。Exception 是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理。 Error 是指在正常情况下，不大可能出现的情况，绝大部分的 Error 都会导致程序（比如 JVM 自身）处于非正常的、不可恢复状态。既然是非正常情况，所以不便于也不需要捕获，常见的比如 OutOfMemoryError 之类，都是 Error 的子类。 Exception 又分为可检查（checked）异常和不检查（unchecked）异常 unchecked exception（非检查异常）：包括运行时异常（RuntimeException）和派生于Error类的异常。对于运行时异常，java编译器不要求必须进行异常捕获处理或者抛出声明，由程序员自行决定。 checked exception（检查异常，编译异常，必须要处理的异常）也：称非运行时异常（运行时异常以外的异常就是非运行时异常），java编译器强制程序员必须进行捕获处理，比如常见的IOExeption和SQLException。对于非运行时异常如果不进行捕获或者抛出声明处理，编译都不会通过。 常见的RuntimeException（运行时异常）: IndexOutOfBoundsException(下标越界异常) NullPointerException(空指针异常) NumberFormatException （String转换为指定的数字类型异常） ArithmeticException -（算术运算异常 如除数为0） ArrayStoreException - （向数组中存放与声明类型不兼容对象- - 异常） SecurityException -（安全异常） IOException（其他异常） FileNotFoundException（文件未找到异常。） IOException（操作输入流和输出流时可能出现的异常。） EOFException （文件已结束异常） 参考文章：https://blog.csdn.net/qq_28849965/article/details/80310153https://blog.csdn.net/m0_37531231/article/details/79502778]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[final,finally,finalize的区别]]></title>
    <url>%2F2018%2F08%2F19%2Ffinal-finally-finalize%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[final,finally,finalize的区别1.简单区别： final用于声明属性，方法和类，分别表示属性不可交变，方法不可覆盖，类不可继承。 finally是异常处理语句结构的一部分，表示总是执行。 finalize是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，供垃圾收集时的其他资源回收，例如关闭文件等。 中等区别： final：java中的关键字，修饰符。A).如果一个类被声明为final，就意味着它不能再派生出新的子类，不能作为父类被继承。因此，一个类不能同时被声明为abstract抽象类的和final的类。B).如果将变量或者方法声明为final，可以保证它们在使用中不被改变. 1)被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取，不可修改。 2)被声明final的方法只能使用，不能重载。finally：java的一种异常处理机制。 finally是对Java异常处理模型的最佳补充。finally结构使代码总会执行，而不管无异常发生。使用finally可以维护对象的内部状态，并可以清理非内存资源。特别是在关闭数据库连接这方面，如果程序员把数据库连接的close()方法放到finally中，就会大大降低程序出错的几率。 finalize：Java中的一个方法名。Java技术使用finalize()方法在垃圾收集器将对象从内存中清除出去前，做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没被引用时对这个对象调用的。它是在Object类中定义的，因此所的类都继承了它。子类覆盖finalize()方法以整理系统资源或者执行其他清理工作。finalize()方法是在垃圾收集器删除对象之前对这个对象调用的。 3.详细区别：这是一道再经典不过的面试题了，我们在各个公司的面试题中几乎都能看到它的身影。final、finally和finalize虽然长得像孪生兄弟一样，但是它们的含义和用法却是大相径庭。 final final关键字我们首先来说说final。它可以用于以下四个地方:1).定义变量，包括静态的和非静态的。2).定义方法的参数。3).定义方法。4).定义类。 定义变量 第一种情况：如果final修饰的是一个基本类型，就表示这个变量被赋予的值是不可变的，即它是个常量；如果final修饰的是一个对象，就表示这个变量被赋予的引用是不可变的这里需要提醒大家注意的是，不可改变的只是这个变量所保存的引用，并不是这个引用所指向的对象。 第二种情况：final的含义与第一种情况相同。实际上对于前两种情况，一种更贴切的表述final的含义的描述，那就是，如果一个变量或方法参数被final修饰，就表示它只能被赋值一次，但是JAVA虚拟机为变量设定的默认值不记作一次赋值。被final修饰的变量必须被初始化。初始化的方式以下几种：1.在定义的时候初始化。2.final变量可以在初始化块中初始化，不可以在静态初始化块中初始化。3.静态final变量可以在定义时初始化，也可以在静态初始化块中初始化，不可以在初始化块中初始化。4.final变量还可以在类的构造器中初始化，但是静态final变量不可以。 通过下面的代码可以验证以上的观点：1234567891011121314151617181920212223242526272829303132333435363738public class FinalTest&#123; public final int A=10; //在定义时初始化 public final int B;&#123;B=20;&#125; //在初始化块中初始化 //非静态final变量不能在静态初始化块中初始化 //public final int C;static&#123;//C=30; &#125; //静态常量，在定义时初始化 public static final int STATIC_D=40; //静态常量，在静态初始化块中初始化 public static final int STATIC_E;static&#123;STATIC_E = 50;&#125; //静态变量不能在初始化块中初始化 //public static final int STATIC_F;&#123;STATIC_F=60;&#125; public final int G; //静态final变量不可以在构造器中初始化 //public static final int STATIC_H; //在构造器中初始化 public finalTest()&#123; G=70; //静态final变量不可以在构造器中初始化 //STATIC_H=80; //给final的变量第二次赋值时，编译会报错 //A=99; //STATIC_D=99; &#125; //final变量未被初始化，编译时就会报错 //public final int L; //静态final变量未被初始化，编译时就会报错 //public static final int STATIC_J;&#125; 我们运行上面的代码之后出了可以发现final变量（常量和静态final变量（静态常量被初始化时，编译会报错。用final修饰的变量（常量比非final的变量（普通变量拥更高的效率，因此我们在际编程中应该尽可能多的用常量来代替普通变量。 定义方法 当final用来定义一个方法时，它表示这个方法不可以被子类重写，但是并不影响它被子类继承。我们写段代码来验证一下：123456789101112131415public class ParentClass&#123; public final void TestFinal()&#123; System.out.println("父类--这是一个final方法"); &#125;&#125;public class SubClass extends ParentClass&#123; //子类无法重写（override父类的final方法，否则编译时会报错 /* public void TestFinal()&#123; System.out.println("子类--重写final方法"); &#125; */ public static void main(String[]args)&#123; SubClass sc = new SubClass(); sc.TestFinal(); &#125;&#125; 这里需要特殊说明的是，具有private访问权限的方法也可以增加final修饰，但是由于子类无法继承private方法，因此也无法重写它。编译器在处理private方法时，是照final方来对待的，这样可以提高该方法被调用时的效率。不过子类仍然可以定义同父类中private方法具同样结构的方法，但是这并不会产生重写的效果，而且它们之间也不存在必然联系。 定义类 最后我们再来回顾一下final用于类的情况。这个大家应该也很熟悉了，因为我们最常用的String类就是final的。由于final类不允许被继承，编译器在处理时把它的所方法都当作final的，因此final类比普通类拥更高的效率。而由关键字abstract定义的抽象类含必须由继承自它的子类重载实现的抽象方法，因此无法同时用final和abstract来修饰同一个类。同样的道理，final也不能用来修饰接口。 final的类的所方法都不能被重写，但这并不表示final的类的属性（变量值也是不可改变的，要想做到final类的属性值不可改变，必须给它增加final修饰，请看下面的例子： 12345678910public final class FinalTest&#123; int i =20; final int j=50; public static void main(String[] args)&#123; FinalTest ft = new FinalTest(); ft.i = 99;/*final类FinalTest的属性值 i是可以改变的，因为属性值i前面没final修饰*/ //ft.j=49;//报错....因为j属性是final的不可以改变。 System.out.println(ft.i); &#125;&#125; 运行上面的代码试试看，结果是99，而不是初始化时的10。 finally语句 接下来我们一起回顾一下finally的用法。finally只能用在try/catch语句中并且附带着一个语句块，表示这段语句最终总是被执行。请看下面的代码： 123456789101112public final class FinallyTest&#123; public static void main(String[] args)&#123; try&#123; throw new NullPointerException(); &#125;catch(NullPointerException e)&#123; System.out.println("程序抛出了异常"); &#125;finally&#123; //这里总会被执行，不受break,return影响另如数据库连接的close()一般写在这里，可以降低程序的出错几率 System.out.println("执行了finally语句块"); &#125; &#125;&#125; 运行结果说明了finally的作用： 1.程序抛出了异常 2.执行了finally语句块请大家注意，捕获程序抛出的异常之后，既不加处理，也不继续向上抛出异常，并不是良好的编程习惯，它掩盖了程序执行中发生的错误，这里只是方便演示，请不要学习。 那么，没一种情况使finally语句块得不到执行呢？return、continue、break这个可以打乱代码顺序执行语句的规律。那我们就来试试看，这个语句是否能影响finally语句块的执行：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public final class FinallyTest &#123; //测试return语句 //结果显示：编译器在编译return new ReturnClass();时， //将它分成了两个步骤，new ReturnClass()和return，前一个创建对象的语句是在finally语句块之前被执行的， //而后一个return语句是在finally语句块之后执行的，也就是说finally语句块是在程序退出方法之前被执行的 public ReturnClass testReturn() &#123; try &#123; return new ReturnClass(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println("执行了finally语句"); &#125; return null; &#125; //测试continue语句 public void testContinue()&#123; for(int i=0; i&lt;3; i++)&#123; try &#123; System.out.println(i); if(i == 1)&#123; System.out.println("con"); &#125; &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println("执行了finally语句"); &#125; &#125; &#125; //测试break语句 public void testBreak() &#123; for (int i=0; i&lt;3; i++) &#123; try &#123; System.out.println(i); if (i == 1) &#123; break; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println("执行了finally语句"); &#125; &#125; &#125; public static void main(String[] args) &#123; FinallyTest ft = new FinallyTest(); // 测试return语句 ft.testReturn(); System.out.println(); // 测试continue语句 ft.testContinue(); System.out.println(); // 测试break语句 ft.testBreak(); &#125;&#125;class ReturnClass &#123; public ReturnClass() &#123; System.out.println("执行了return语句"); &#125;&#125; 上面这段代码的运行结果如下：12345678执行了return语句执行了finally语句执行了finally语句con执行了finally语句执行了finally语句执行了finally语句执行了finally语句 很明显，return、continue和break都没能阻止finally语句块的执行。从输出的结果来看，return语句似乎在finally语句块之前执行了，事实真的如此吗？我们来想想看，return语句的作用是什么呢？是退出当前的方法，并将值或对象返回。如果 finally语句块是在return语句之后执行的，那么return语句被执行后就已经退出当前方法了，finally语句块又如何能被执行呢？因此，正确的执行顺序应该是这样的：编译器在编译return new ReturnClass();时，将它分成了两个步骤，new ReturnClass()和return，前一个创建对象的语句是在finally语句块之前被执行的，而后一个return语句是在finally语句块之后执行的，也就是说finally语句块是在程序退出方法之前被执行的。同样，finally语句块是在循环被跳过（continue和中断（break之前被执行的 总结： 总结：finally块的语句在try或catch中的return语句执行之后返回之前执行且finally里的修改语句可能影响也可能不影响try或catch中 return已经确定的返回值，若finally里也有return语句则覆盖try或catch中的return语句直接返回。 finalize方法 最后，我们再来看看finalize，它是一个方法，属于java.lang.Object类，它的定义如下：protected void finalize()throws Throwable{}众所周知，finalize()方法是GC（garbagecollector运行机制的一部分,在此我们只说说finalize()方法的作用是什么呢？finalize()方法是在GC清理它所从属的对象时被调用的，如果执行它的过程中抛出了无法捕获的异常（uncaughtexception，GC将终止对改对象的清理，并且该异常会被忽略；直到下一次GC开始清理这个对象时，它的finalize()会被再次调用。 参考文章：https://www.cnblogs.com/gjfbk/p/9072059.htmlhttps://www.cnblogs.com/smart-hwt/p/8257330.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象的特征：继承封装和多态]]></title>
    <url>%2F2018%2F08%2F16%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%9A%E7%BB%A7%E6%89%BF%E5%B0%81%E8%A3%85%E5%92%8C%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[面向对象的特征：继承、封装和多态 封装是指将某事物的属性和行为包装到对象中，这个对象只对外公布需要公开的属性和行为，而这个公布也是可以有选择性的公布给其它对象。在java中能使用private、protected、public三种修饰符或不用（即默认defalut）对外部对象访问该对象的属性和行为进行限制。 继承是子对象可以继承父对象的属性和行为，亦即父对象拥有的属性和行为，其子对象也就拥有了这些属性和行为。这非常类似大自然中的物种遗传。 多态不是很好解释：更倾向于使用java中的固定用法，即overriding（重写）和overload（重载）。多态则是体现在overriding（重写）上，而overload（重载）则不属于面向对象中多态的范畴，因为overload（重载）概念在非面向对象中也存在。overriding（重写）是面向对象中的多态，因为overriding（重写）是与继承紧密联系，是面向对象所特有的。多态是指父对象中的同一个行为能在其多个子对象中有不同的表现。也就是说子对象可以使用重写父对象中的行为，使其拥有不同于父对象和其它子对象的表现，这就是overriding（重写）。 多态的定义：指允许不同类的对象对同一消息做出响应。即同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用） 实现多态的技术称为：动态绑定（dynamic binding），是指在执行期间判断所引用对象的实际类型，根据其实际的类型调用其相应的方法。 多态的作用：消除类型之间的耦合关系。 现实中，关于多态的例子不胜枚举。比方说按下 F1 键这个动作，如果当前在 Flash 界面下弹出的就是 AS 3 的帮助文档；如果当前在 Word 下弹出的就是 Word 帮助；在 Windows 下弹出的就是 Windows 帮助和支持。同一个事件发生在不同的对象上会产生不同的结果。 多态存在的三个必要条件： 一、要有继承； 二、要有重写； 三、父类引用指向子类对象。 父类引用指向子类对象指的是： 例如父类Animal，子类Cat,Dog。其中Animal可以是类也可以是接口，Cat和Dog是继承或实现Animal的子类。Animal animal = new Cat();即声明的是父类，实际指向的是子类的一个对象。那这么使用的优点是什么，为什么要这么用？可以用这几个关键词来概括：多态、动态链接，向上转型也有人说这是面向接口编程，可以降低程序的耦合性，即调用者不必关心调用的是哪个对象，只需要针对接口编程就可以了，被调用者对于调用者是完全透明的。让你更关注父类能做什么,而不去关心子类是具体怎么做的,你可以随时替换一个子类,也就是随时替换一个具体实现,而不用修改其他.以后结合设计模式（如工厂模式，代理模式）和反射机制可能有更深理解。下面介绍java的多态性和其中的动态链接，向上转型：面向对象的三个特征：封装、继承和多态；封装隐藏了类的内部实现机制，可以在不影响使用者的前提下修改类的内部结构，同时保护了数据；继承是为了重用父类代码，子类继承父类就拥有了父类的成员。方法的重写、重载与动态连接构成多态性。Java之所以引入多态的概念，原因之一是它在类的继承问题上和C++不同，后者允许多继承，这确实给其带来的非常强大的功能，但是复杂的继承关系也给C++开发者带来了更大的麻烦，为了规避风险，Java只允许单继承，派生类与基类间有IS-A的关系（即“猫”is a “动物”）。这样做虽然保证了继承关系的简单明了，但是势必在功能上有很大的限制，所以，Java引入了多态性的概念以弥补这点的不足，此外，抽象类和接口也是解决单继承规定限制的重要手段。同时，多态也是面向对象编程的精髓所在。理解多态，首先要知道“向上转型”。我定义了一个子类Cat，它继承了Animal类，那么后者就是前者是父类。我可以通过Cat c = new Cat();实例化一个Cat的对象，这个不难理解。但当我这样定义时：Animal a = new Cat();这代表什么意思呢？很简单，它表示我定义了一个Animal类型的引用，指向新建的Cat类型的对象。由于Cat是继承自它的父类Animal，所以Animal类型的引用是可以指向Cat类型的对象的。这就是“向上转型”。那么这样做有什么意义呢？因为子类是对父类的一个改进和扩充，所以一般子类在功能上较父类更强大，属性较父类更独特， 定义一个父类类型的引用指向一个子类的对象既可以使用子类强大的功能，又可以抽取父类的共性。 所以，父类类型的引用可以调用父类中定义的所有属性和方法，而对于子类中定义而父类中没有的方法，父类引用是无法调用的；那什么是动态链接呢？当父类中的一个方法只有在父类中定义而在子类中没有重写的情况下，才可以被父类类型的引用调用； 对于父类中定义的方法，如果子类中重写了该方法，那么父类类型的引用将会调用子类中的这个方法，这就是动态连接。 参考文章：https://www.cnblogs.com/mengchunchen/p/7890729.htmlhttps://www.cnblogs.com/ChrisMurphy/p/5054256.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>编程语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django发送HTML邮件]]></title>
    <url>%2F2018%2F08%2F09%2FDjango%E5%8F%91%E9%80%81HTML%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Django发送HTML邮件 之前关于使用Django发送邮件已经写过一篇文章，不会在Django中发送邮件的话，可以先看这个传送门 在这个基础上只需要稍加配置就可以有一个比较好看的HTML而不是单调几句话的页面，话不多说，直接上代码models.py的新生类 123456789101112131415161718192021class NewStudent(models.Model): DEPARTMENT_CHOICE=&#123; ('3','技术部'), ('2','新媒体'), ('1','办公室'), &#125; # unique=True 解决异步问题后将唯一性加上 email = models.EmailField(default=None, blank=True, null=False, verbose_name='邮箱') name = models.CharField(max_length=50, default=None, blank=True, null=True, verbose_name='姓名') student_id = models.CharField(max_length=9, default=None, blank=True, null=False, verbose_name='学号') qq = models.CharField(max_length=11,null=False,verbose_name='QQ号') choice = models.CharField(max_length=10,choices=DEPARTMENT_CHOICE,default=0,null=False,verbose_name='部门') code = models.CharField(max_length=20,default=None, verbose_name="邮箱验证码") is_success = models.BooleanField(default=False,verbose_name='邮箱已验证') register_time = models.DateTimeField(auto_now_add=True,verbose_name='注册时间') class Meta: verbose_name = u"新生信息" verbose_name_plural = verbose_name 邮件发送的函数 123456789101112131415161718#new_student是前端传过来的新生信息#EmailMultiAlternatives是邮件信息相关的model，邮件发送的博文中有描述def save_studentinfo(request, new_student): code = random_str(16) new_student.code = code active_url = str(EMAIL_ACTIVE_URL) + str(code) context = &#123; 'student_id' : str(new_student.student_id), 'name' : str(new_student.name), 'active_url' : str(active_url), &#125; # 发送的html模板的名称 email_template_name = 'email_template.html' t = loader.get_template(email_template_name) html_content = t.render(context) msg = EmailMultiAlternatives(EMAIL_TITLE, html_content, DEFAULT_FROM_EMAIL, [new_student.email]) msg.attach_alternative(html_content, "text/html") msg.send() 下面是email_template.html的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;meta charset="UTF-8"&gt;&lt;head&gt; &lt;title&gt;Vinta&lt;/title&gt; &lt;meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2 user-scalable = yes"&gt; &lt;style&gt; html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video &#123; margin: 0; padding: 0; border: 0; font-size: 100%; font: inherit; vertical-align: baseline; &#125; body &#123; line-height: 1; &#125; body &#123; background: #424242; /* 标准的语法 */ font-family: "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, serif; font-size: 14px; font-weight: 400; line-height: 1.5em; &#125; h1, h2, h3, h4 &#123; font-family: "HelveticaNeue", "Helvetica Neue", Helvetica, Arial, serif; color: #000000; font-style: normal; line-height: 1em; &#125; h1 &#123; font-size: 18px; text-transform: uppercase; font-weight: 700; margin-bottom: 15px; &#125; h2 &#123; font-size: 16px; font-weight: 700; margin-top: 20px; margin-bottom: 5px; &#125; h3 &#123; font-size: 15px; color: #5e5e5e; font-style: italic; &#125; h4 &#123; font-size: 16px; font-style: italic; font-weight: 400; margin-bottom: 0px; position: absolute; top: -7px; width: 130px; margin-left: -65px; left: 50%; &#125; #wrapper &#123; width: 940px; margin: 0 auto; &#125; .logo &#123; width: 276px; height: 58px; padding: 40px 0px; margin: 0 auto; &#125; /*----- main content of page -----*/ #content &#123; background:#ffffff; width: 620px; padding: 40px 160px; float: left; box-shadow: 0px 1px 2px 0px #000000; -moz-box-shadow: 0px 1px 2px 0px #000000; -webkit-box-shadow: 0px 1px 2px 0px #000000; text-align: left; &#125; .launch &#123; font-size: 2em; font-weight: bolder; width: 402px; height: 108px; margin: 0 auto; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="wrapper"&gt; &lt;div class="logo"&gt;&lt;/div&gt; &lt;!--content starts--&gt; &lt;div id="content"&gt; &lt;div class="launch"&gt;&lt;img src="http://pd2qkcgty.bkt.clouddn.com/logo.png" height="70" width="auto" style="position: relative;left: -80%;top: -40%"/&gt;&lt;br&gt;致学弟学妹的一封信&lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;hr&gt; &lt;h1&gt; &#123;&#123; student_id &#125;&#125;&#123;&#123; name &#125;&#125;同学你好：&lt;/h1&gt; &lt;h2 style="line-height: 30px"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;首先祝贺你正式成为计软网安院科协的一员，从加入院科协的那一刻开始，你将经历一段从没经历和感受过的奇妙旅程。我们希望未来的一年中你能在科协这个大家庭中结交志同道合的朋友，从科协牛人中获取经验，成为技术上的大牛，或者在科协活动的举办中锻炼自己的活动组织能力。我们更希望一年后的你能像我们一样在科协为未来的学弟学妹提供技术和学习上的帮助。 &lt;br&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;愿初来南邮的你，在未来的一年中能在计软网安院科协这片沃土上，施展自己的才华，活出自己的精彩。(๑•̀ㅂ•́)و✧&lt;/h2&gt; &lt;br&gt; &lt;br&gt; &lt;a href="&#123;&#123; active_url &#125;&#125;"&gt; &lt;button style="width: 620px;border-radius: 3px;background: #3498db;border: 4px;height: 35px;border: #2e6da4;color: white;font-family: 宋体;font-weight: bolder"&gt; 点击完成验证 &lt;/button&gt; &lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;hr&gt; &lt;div style="width: 80%;height:100%;float: left"&gt; &lt;p style="position: relative;width: 450px;"&gt; 加入科协，你需要什么？&lt;br&gt; 你需要的是 Interest——兴趣 Passion——激情 Perseverance——毅力 世界本没路，走的人多了就有了路，没有谁天生就会，一切都是通过不断学习获得。从零开始，你将在这里慢慢成长。在这里你将会获得毕生的朋友，因为你们志同道合。&lt;/p&gt; &lt;/div&gt; &lt;div style="width: 20%;float: left;height: 100%"&gt; &lt;img src="http://pd2qkcgty.bkt.clouddn.com/sacc_QR_Code.png" alt="微信二维码" id="img-left" style="width: 110px;height: 110px"&gt; &lt;/div&gt; &lt;hr&gt; &lt;p style="text-align: right"&gt;南京邮电大学计软网安院科协&lt;br&gt; njupt.sacc@outlook.com&lt;/p&gt; &lt;br&gt; &lt;p style="text-align: center"&gt;sacc期待你的加入&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 最后展示一下效果:PC端： 移动端：]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[玄学解决BUG]]></title>
    <url>%2F2018%2F07%2F30%2F%E7%8E%84%E5%AD%A6%E8%A7%A3%E5%86%B3BUG%2F</url>
    <content type="text"><![CDATA[在网上汇总了一些企图玄学的图案，留着万一哪天自己能用到佛祖保佑 永无BUG 永不修改12345678910111213141516171819202122&lt;!--////////////////////////////////////////////////////////////////////--&gt;&lt;!--// _ooOoo_ //--&gt;&lt;!--// o8888888o //--&gt;&lt;!--// 88&quot; . &quot;88 //--&gt;&lt;!--// (| ^_^ |) //--&gt;&lt;!--// O\ = /O //--&gt;&lt;!--// ____/`-&amp;#45;&amp;#45;&apos;\____ //--&gt;&lt;!--// .&apos; \\| |// `. //--&gt;&lt;!--// / \\||| : |||// \ //--&gt;&lt;!--// / _||||| -:- |||||- \ //--&gt;&lt;!--// | | \\\ - /// | | //--&gt;&lt;!--// | \_| &apos;&apos;\-&amp;#45;&amp;#45;/&apos;&apos; | | //--&gt;&lt;!--// \ .-\__ `-` ___/-. / //--&gt;&lt;!--// ___`. .&apos; /&amp;#45;&amp;#45;.&amp;#45;&amp;#45;\ `. . ___ //--&gt;&lt;!--// .&quot;&quot; &apos;&lt; `.___\_&lt;|&gt;_/___.&apos; &gt;&apos;&quot;&quot;. //--&gt;&lt;!--// | | : `- \`.;`\ _ /`;.`/ - ` : | | //--&gt;&lt;!--// \ \ `-. \_ __\ /__ _/ .-` / / //--&gt;&lt;!--// ========`-.____`-.___\_____/___.-`____.-&apos;======== //--&gt;&lt;!--// `=-&amp;#45;&amp;#45;=&apos; //--&gt;&lt;!--// ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ //--&gt;&lt;!--// 佛祖保佑 永无BUG 永不修改 //--&gt;&lt;!--////////////////////////////////////////////////////////////////////--&gt; 12345678910111213141516171819202122/** * _ooOoo_ * o8888888o * 88&quot; . &quot;88 * (| -_- |) * O\ = /O * ___/`---&apos;\____ * . &apos; \\| |// `. * / \\||| : |||// \ * / _||||| -:- |||||- \ * | | \\\ - /// | | * | \_| &apos;&apos;\---/&apos;&apos; | | * \ .-\__ `-` ___/-. / * ___`. .&apos; /--.--\ `. . __ * .&quot;&quot; &apos;&lt; `.___\_&lt;|&gt;_/___.&apos; &gt;&apos;&quot;&quot;. * | | : `- \`.;`\ _ /`;.`/ - ` : | | * \ \ `-. \_ __\ /__ _/ .-` / / * ======`-.____`-.___\_____/___.-`____.-&apos;====== * `=---=&apos; * ............................................. * 佛曰：bug泛滥，我已瘫痪！ */ 123456789101112131415161718192021222324252627282930313233343536373839&lt;!-- ´´´´´´´´██´´´´´´´ ´´´´´´´████´´´´´´ ´´´´´████████´´´´ ´´`´███▒▒▒▒███´´´´´ ´´´███▒●▒▒●▒██´´´ ´´´███▒▒▒▒▒▒██´´´´´ ´´´███▒▒▒▒██´ 项目：vue-user-center ´´██████▒▒███´´´´´ 语言： ES6-babel ´██████▒▒▒▒███´´ 框架： vue+vue-router+vuex+iview+axios ██████▒▒▒▒▒▒███´´´´ 构建工具： webpack ´´▓▓▓▓▓▓▓▓▓▓▓▓▓▒´´ 版本控制： git-github ´´▒▒▒▒▓▓▓▓▓▓▓▓▓▒´´´´´ css预处理: less ´.▒▒▒´´▓▓▓▓▓▓▓▓▒´´´´´ 代码风格：eslint-standard ´.▒▒´´´´▓▓▓▓▓▓▓▒ 编辑器： phpstorm ..▒▒.´´´´▓▓▓▓▓▓▓▒ 数据库: mysql ´▒▒▒▒▒▒▒▒▒▒▒▒ 服务器端脚本: php go ´´´´´´´´´███████´´´´´ author: codeRabbit ´´´´´´´´████████´´´´´´´ ´´´´´´´█████████´´´´´´ ´´´´´´██████████´´´´ 大部分人都在关注你飞的高不高，却没人在乎你飞的累不累，这就是现实！ ´´´´´´██████████´´´ 我从不相信梦想，我，只，相，信，自，己！ ´´´´´´´█████████´´ ´´´´´´´█████████´´´ ´´´´´´´´████████´´´´´ ________▒▒▒▒▒ _________▒▒▒▒ _________▒▒▒▒ ________▒▒_▒▒ _______▒▒__▒▒ _____ ▒▒___▒▒ _____▒▒___▒▒ ____▒▒____▒▒ ___▒▒_____▒▒ ███____ ▒▒ ████____███ █ _███_ _█_███——————————————————————————女神保佑，代码无bug——————————————————————--&gt; 神兽保佑 代码无BUG！12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// ┏┓ ┏┓// ┏┛┻━━━┛┻┓//// ┃ ┃//// ┃ ━ ┃//// ┃ ┳┛ ┗┳ ┃//// ┃ ┃//// ┃ ┻ ┃//// ┃ ┃//// ┗━┓ ┏━┛//// ┃ ┃ 神兽保佑// // ┃ ┃ 代码无BUG！//// ┃ ┗━━━┓//// ┃ ┣┓//// ┃ ┏┛//// ┗┓┓┏━┳┓┏┛//// ┃┫┫ ┃┫┫//// ┗┻┛ ┗┻┛//## ┏┓ ┏┓# ┏┛┻━━━┛┻┓# ┃ ┃ # ┃ ━ ┃# ┃ ＞ ＜┃# ┃ ┃# ┃ . ⌒ ..┃# ┃ ┃# ┗━┓ ┏━┛# ┃ ┃ Codes are far away from bugs with the animal protecting # ┃ ┃ 神兽保佑,代码无bug# ┃ ┃ # ┃ ┃ # ┃ ┃# ┃ ┃ # ┃ ┗━━━┓# ┃ ┣┓# ┃ ┏┛# ┗┓┓┏━┳┓┏┛# ┃┫┫ ┃┫┫# ┗┻┛ ┗┻┛# ┏┓ ┏┓+ +# ┏┛┻━━━┛┻┓ + +# ┃ ┃ # ┃ ━ ┃ ++ + + +# ████━████ ┃+# ┃ ┃ +# ┃ ┻ ┃# ┃ ┃ + +# ┗━┓ ┏━┛# ┃ ┃ # ┃ ┃ + + + +# ┃ ┃ Codes are far away from bugs with the animal protecting # ┃ ┃ + 神兽保佑,代码无bug # ┃ ┃# ┃ ┃ + # ┃ ┗━━━┓ + +# ┃ ┣┓# ┃ ┏┛# ┗┓┓┏━┳┓┏┛ + + + +# ┃┫┫ ┃┫┫# ┗┻┛ ┗┻┛+ + + + 12345678910111213141516171819# 代码无BUG!# ,----------------, ,---------,# ,-----------------------, ,&quot; ,&quot;|# ,&quot; ,&quot;| ,&quot; ,&quot; |# +-----------------------+ | ,&quot; ,&quot; |# | .-----------------. | | +---------+ |# | | | | | | -==----&apos;| |# | | I LOVE DOS! | | | | | |# | | Bad command or | | |/----|`---= | |# | | C:\&gt;_ | | | ,/|==== ooo | ;# | | | | | // |(((( [33]| ,&quot;# | `-----------------&apos; |,&quot; .;&apos;| |(((( | ,&quot;# +-----------------------+ ;; | | |,&quot;# /_)______________(_/ //&apos; | +---------+# ___________________________/___ `,# / oooooooooooooooo .o. oooo /, \,&quot;-----------# / ==ooooooooooooooo==.o. ooo= // ,`\--&#123;)B ,&quot;# /_==__==========__==_ooo__ooo=_/&apos; /___________,&quot;# 1234567891011//// █████▒█ ██ ▄████▄ ██ ▄█▀ ██████╗ ██╗ ██╗ ██████╗// ▓██ ▒ ██ ▓██▒▒██▀ ▀█ ██▄█▒ ██╔══██╗██║ ██║██╔════╝// ▒████ ░▓██ ▒██░▒▓█ ▄ ▓███▄░ ██████╔╝██║ ██║██║ ███╗// ░▓█▒ ░▓▓█ ░██░▒▓▓▄ ▄██▒▓██ █▄ ██╔══██╗██║ ██║██║ ██║// ░▒█░ ▒▒█████▓ ▒ ▓███▀ ░▒██▒ █▄ ██████╔╝╚██████╔╝╚██████╔╝// ▒ ░ ░▒▓▒ ▒ ▒ ░ ░▒ ▒ ░▒ ▒▒ ▓▒ ╚═════╝ ╚═════╝ ╚═════╝// ░ ░░▒░ ░ ░ ░ ▒ ░ ░▒ ▒░// ░ ░ ░░░ ░ ░ ░ ░ ░░ ░// ░ ░ ░ ░ ░// ░ 1234567891011121314151617181920212223242526272829/* _______________########_______________________ ______________##########_____________________________________############___________________________________#############_________________________________##__###########_______________________________###__######_#####______________________________###_#######___####____________________________###__##########_####__________________________####__###########_####_______________________#####___###########__#####____________________######___###_########___#####__________________#####___###___########___######_______________######___###__###########___######____________######___####_##############__######__________#######__#####################_#######_________#######__##############################_______#######__######_#################_#######______#######__######_######_#########___######______#######____##__######___######_____######______#######________######____#####_____#####________######________#####_____#####_____####__________#####________####______#####_____###____________#####______;###________###______#________________##_______####________####______________ 葱官赐福 百无禁忌 */]]></content>
      <categories>
        <category>企图玄学</category>
      </categories>
      <tags>
        <tag>企图玄学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP算法]]></title>
    <url>%2F2018%2F04%2F28%2FKMP%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[KMP算法适用于字符串匹配，今天通过相关视频，大致了理解了其实现原理以及步骤，学习的视频链接如下（这是未优化版本）：https://www.bilibili.com/video/av6239731/?p=11https://www.bilibili.com/video/av3246487?from=search&amp;seid=8682896714663607035https://study.163.com/course/courseLearn.htm?courseId=468002#/learn/video?lessonId=1023415&amp;courseId=468002由于我大概理解了原理以及代码实现，所以我到时候后期复习时建议直接看最后一个链接的视频。我在网上参考了部分其他人使用的C/C++实现的KMP算法实现，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX 101void get_next( int *next,char *a,int la) /*求NEXT[]的值*/&#123; int i=1,j=0 ; next[1] = 0 ; while ( i &lt;= la) /*核心部分*/ &#123; if( a[i] == a[j] || j == 0 ) &#123; j ++ ; i ++ ; if( a[i] == a[j]) next[i] = next[j]; else next[i] = j ; &#125; else j = next[j] ; &#125;&#125;int str_kmp( int *next, char *A ,char *a, int lA,int la)/* EASY*/&#123; int i,j,k ; i = 1 ; j = 1 ; while ( i&lt;=lA &amp;&amp; j &lt;= la ) &#123; if(A[i] == a[j] || j == 0 ) &#123; i ++ ; j ++ ; &#125; else j = next[j] ; &#125; if ( j&gt; la) return i-j+1 ; else return -1 ;&#125;int main(void)&#123; int n,k; int next[MAX]=&#123;0&#125; ; int lA=0,la =0 ; char A[MAX],a[MAX] ; scanf("%s %s",A,a) ; lA = strlen(A); la = strlen(a); for(k=la-1; k&gt;= 0 ;k --) a[k+1] = a[k] ; for(k=lA-1; k&gt;= 0 ;k --) A[k+1] = A[k] ; get_next(next,a,la) ; k = str_kmp(next,A,a,lA,la); if ( -1 == k) printf("Not Soulation!!! "); else printf("%d ",k) ; system("pause"); return 0 ;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>KMP算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2018%2F04%2F25%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[其实这节课并没有听太懂，但是为了防止忘记，所以直接将郝斌老师的代码扔到了博客上希望我之后多次看之后能真正理解这个排序方法，还有就是郝斌老师的数据结构课程看完了，并没有图的相关，看来我要开始自学图了，加油！！！！1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# include &lt;stdio.h&gt;int FindPos(int * a, int low, int high);void QuickSort(int * a, int low, int high);int main(void)&#123; int a[6] = &#123;-2, 1, 0, -985, 4, -93&#125;; int i; QuickSort(a, 0, 5); //第二个参数表示第一个元素的下标 第三个参数表示最后一个元素的下标 for (i=0; i&lt;6; ++i) printf("%d ", a[i]); printf("\n"); return 0;&#125;void QuickSort(int * a, int low, int high)&#123; int pos; if (low &lt; high) &#123; pos = FindPos(a, low, high); QuickSort(a, low, pos-1); QuickSort(a, pos+1, high); &#125; &#125;int FindPos(int * a, int low, int high)&#123; int val = a[low]; while (low &lt; high) &#123; while (low&lt;high &amp;&amp; a[high]&gt;=val) --high; a[low] = a[high]; while (low&lt;high &amp;&amp; a[low]&lt;=val) ++low; a[high] = a[low]; &#125;//终止while循环之后low和high一定是相等的 a[low] = val; return high; //high可以改为low, 但不能改为val 也不能改为a[low] 也不能改为a[high]&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树以及链式二叉树的常见操作]]></title>
    <url>%2F2018%2F04%2F24%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E4%BB%A5%E5%8F%8A%E9%93%BE%E5%BC%8F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[二叉树是树这部分中最重要的知识之一，今天看完了郝斌老师关于树部分的知识，并对老师在看上所说的链式二叉树代码进行了实现 一、二叉树的中的一些专有名词的解释：1.先序遍历:指先访问根节点，再先序遍历左子树，再先序遍历右子树2.中序遍历:指先中序遍历左子树，再访问根节点，再中序遍历右子树3.后序遍历:指先后序遍历左子树，再后序遍历右子树，再访问根节点 下面是郝斌老师上课时关于这三种遍历的视频截图先序遍历： 中序遍历： 后序遍历： 二、通过先序与中序求后序以及通过中序与后序求先序不论是先序还是后序，我们分别可以从先序的第一个以及后序的最后一个来确认二叉树的根节点，然后通过中序可得出该二叉树的左子树部分由哪些组成，以及右子树部分由哪些组成。1.在先序与中序求后序时，先序中谁先出现，谁就是子树的根节点2.在中序与后序求后序时，后序中谁后出现，谁就是子树的根节点 下面是郝斌老师关于求二叉树时的视频截图:已知先序中序求后序 已知中序后序求先序 三、链式二叉树的常见操作链式二叉树是常见二叉树的程序实现方法，根据郝斌老师的课程，我对他上课所敲的代码进行了实现，具体代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/* 该二叉树的树状图如下： A * * B C * D * E *///程序实现代码#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct BTNode&#123; char data; struct BTNode * pLeft; struct BTNode * pRight;&#125;BTNode;struct BTNode * createBTree(void);//创建二叉树void PreTraverseBTree(struct BTNode * pT);//前序遍历void InTraverseBTree(struct BTNode * pT);//中序遍历void PostTraverseBTree(struct BTNode * pT);//后序遍历int main()&#123; struct BTNode * pT = createBTree(); printf("该二叉树的前序遍历为:\n"); PreTraverseBTree(pT); printf("\n该二叉树的中序遍历为:\n"); printf("\n该二叉树的后序遍历为:\n"); PostTraverseBTree(pT); printf("\n"); return 0;&#125;struct BTNode * createBTree()&#123; BTNode * pA = (struct BTNode*)malloc(sizeof(BTNode)); BTNode * pB = (struct BTNode*)malloc(sizeof(BTNode)); BTNode * pC = (struct BTNode*)malloc(sizeof(BTNode)); BTNode * pD = (struct BTNode*)malloc(sizeof(BTNode)); BTNode * pE = (struct BTNode*)malloc(sizeof(BTNode)); //给树的每个节点添加数据 pA-&gt;data = 'A'; pB-&gt;data = 'B'; pC-&gt;data = 'C'; pD-&gt;data = 'D'; pE-&gt;data = 'E'; //给树的相关节点通过指针连接 pA-&gt;pLeft = pB; pA-&gt;pRight = pC; pB-&gt;pLeft = pB-&gt;pRight = NULL; pC-&gt;pLeft = pD; pC-&gt;pRight = NULL; pD-&gt;pLeft = NULL; pD-&gt;pRight = pE; pE-&gt;pLeft = pE-&gt;pRight = NULL; return pA;&#125;void PreTraverseBTree(struct BTNode * pT)&#123; if(pT != NULL)&#123; printf("%c ",pT-&gt;data); if(pT-&gt;pLeft != NULL)&#123; PreTraverseBTree(pT-&gt;pLeft); &#125; if(pT-&gt;pRight != NULL)&#123; PreTraverseBTree(pT-&gt;pRight); &#125; &#125;&#125;void InTraverseBTree(struct BTNode * pT)&#123; if(pT != NULL)&#123; if(pT-&gt;pLeft != NULL)&#123; InTraverseBTree(pT-&gt;pLeft); &#125; printf("%c ",pT-&gt;data); if(pT-&gt;pRight != NULL)&#123; InTraverseBTree(pT-&gt;pRight); &#125; &#125;&#125;void PostTraverseBTree(struct BTNode * pT)&#123; if(pT != NULL)&#123; if(pT-&gt;pLeft != NULL)&#123; PostTraverseBTree(pT-&gt;pLeft); &#125; if(pT-&gt;pRight != NULL)&#123; PostTraverseBTree(pT-&gt;pRight); &#125; printf("%c ",pT-&gt;data); &#125;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>二叉树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[汉诺塔递归问题]]></title>
    <url>%2F2018%2F04%2F23%2F%E6%B1%89%E8%AF%BA%E5%A1%94%E9%80%92%E5%BD%92%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[汉诺塔递归是一个用到了递归思想的经典问题，看过相关借时候其实我仍然还是没有完全理解，但是先把代码扔在博客上，慢慢消化这个问题123456789101112131415161718192021222324252627282930313233# include &lt;stdio.h&gt;void hannuota(int n, char A, char B, char C)//指将A上的盘子借助B移到C&#123; /* 如果是1个盘子 直接将A柱子上的盘子从A移到C 否则 先将A柱子上的n-1个盘子借助C移到B 直接将A柱子上的盘子从A移到C 最后将B柱子上的n-1个盘子借助A移到C 最上面盘子为1最下面为n */ if (1 == n) &#123; printf("将编号为%d的盘子直接从%c柱子移到%c柱子\n", n, A, C); &#125; else &#123; hannuota(n-1, A, C, B); printf("将编号为%d的盘子直接从%c柱子移到%c柱子\n", n, A, C); hannuota(n-1, B, A, C); &#125;&#125;int main(void)&#123; int n; printf("请输入要移动盘子的个数: "); scanf("%d", &amp;n); hannuota(n, 'A', 'B', 'C'); return 0;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>递归</tag>
        <tag>汉诺塔问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环队列的常见操作]]></title>
    <url>%2F2018%2F04%2F22%2F%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[与栈相比，队列我个人感觉简单一些，不过对于一般的队列，都是循环队列，这是为了防止内存的浪费，使为队列分配的内存可以循环使用，而且一般动态分配一个长度为n的循环队列的话，真正用来储存数据的只有n-1，因为要留一个空节点使队列尾的下标等于该空节点的下标，通过该空节点用来区分队满与队空，队满是判断条件是（rear+1）% len = front其中rear为队尾数据的下标，front为队头的下标，len为长度。下面是我在观看郝斌老师的视频后的总结，对原来郝斌老师的代码加上的释放队列空间以及清空队列数据的这两个操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Queue&#123; int len; int front; int rear; int * pBase;&#125;Queue;void init(Queue *);//初始化队列void en_queue(Queue *,int);//入队void traverse_queue(Queue *);//遍历void out_queue(Queue *);//出队void destroy(Queue *);//释放void clear(Queue *);//重置int main()&#123; Queue Q; printf("请输入您需要的循环队列的长度:\n"); scanf("%d",&amp;(Q.len)); init(&amp;Q); en_queue(&amp;Q, 1); en_queue(&amp;Q, 2); en_queue(&amp;Q, 3); en_queue(&amp;Q, 4); en_queue(&amp;Q, 5); en_queue(&amp;Q, 6); en_queue(&amp;Q, 7); en_queue(&amp;Q, 8); traverse_queue(&amp;Q); out_queue(&amp;Q); traverse_queue(&amp;Q); out_queue(&amp;Q); traverse_queue(&amp;Q); out_queue(&amp;Q); traverse_queue(&amp;Q); out_queue(&amp;Q); traverse_queue(&amp;Q); destroy(&amp;Q); printf("%p",Q.pBase); return 0;&#125;void init(Queue* pQ)&#123; pQ-&gt;pBase = (int*)malloc(sizeof(int)*(pQ-&gt;len)); pQ-&gt;front = 0; pQ-&gt;rear = 0;&#125;void en_queue(Queue* pQ,int val)&#123; if ((pQ-&gt;rear+1)%pQ-&gt;len == pQ-&gt;front)&#123; printf("队列已满!%d入队失败\n",val); &#125;else&#123; pQ-&gt;pBase[pQ-&gt;rear] = val; pQ-&gt;rear = (pQ-&gt;rear+1) % pQ-&gt;len; &#125; return;&#125;void traverse_queue(Queue* pQ)&#123; if (pQ-&gt;rear == pQ-&gt;front)&#123; printf("队列为空!\n"); &#125;else&#123; int i = pQ-&gt;front; while (i != pQ-&gt;rear) &#123; printf("%d ",pQ-&gt;pBase[i]); i = (i+1) % pQ-&gt;len; &#125; &#125; printf("\n"); return;&#125;void out_queue(Queue *pQ)&#123; if (pQ-&gt;rear == pQ-&gt;front)&#123; printf("队列为空!\n"); &#125;else&#123; int val = pQ-&gt;pBase[pQ-&gt;front]; pQ-&gt;front = (pQ-&gt;front+1) % pQ-&gt;len; printf("出队的元素值为:%d\n",val); &#125; return;&#125;void destroy(Queue *pQ)&#123; pQ-&gt;len=-1;//因为pQ不是动态分配的，所以不要咬释放变量pQ的空间 free(pQ-&gt;pBase);//释放动态分配的数组的空间 return;&#125;void clear(Queue *pQ)&#123; pQ-&gt;len = 0; pQ-&gt;front = 0; pQ-&gt;rear = 0; return;&#125; 注：个人认为循环队列的主要的一个巧妙的方法就是用取余这个方法，使循环队列的循环功能得以实现]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>循环队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈的常见操作]]></title>
    <url>%2F2018%2F04%2F20%2F%E6%A0%88%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[今天正好下午没课，就接着昨天所看的课程一口气把郝斌老师有关栈的视频也刷完了，通过他用代码实现栈的常见操作使我对栈也有了一个进一步的认识，下面是我看过视频后参考郝斌老师的代码对栈常见功能的代码实现。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node* pNext;&#125; Node,* PNode;typedef struct Stack&#123;//创建两个指针分别指向栈顶和栈底 PNode pTop; PNode pBottom;//总是指向栈的头节点&#125; Stack,* PStack;void init(PStack);//初始化一个栈void push(PStack,int);//入栈void traverse(PStack);//遍历void pop(PStack,int*);//出栈void clear(PStack);//重置栈中的数据int main() &#123; Stack S; int val; init(&amp;S); push(&amp;S,1); push(&amp;S,2); push(&amp;S,3); push(&amp;S,4); push(&amp;S,5); push(&amp;S,6); traverse(&amp;S); pop(&amp;S,&amp;val); printf("pop的值为%d\n",val); traverse(&amp;S); clear(&amp;S); traverse(&amp;S); return 0;&#125;void init(PStack pS)&#123; pS-&gt;pTop = (PNode)malloc(sizeof(Node)); if(pS == NULL)&#123; printf("分配内存失败!\n"); exit(-1); &#125;else&#123; pS-&gt;pBottom = pS-&gt;pTop; pS-&gt;pBottom-&gt;pNext = NULL; &#125; return;&#125;void push(PStack pS,int val)&#123; PNode pNew = (PNode)malloc(sizeof(Node)); pNew-&gt;data = val; pNew-&gt;pNext = pS-&gt;pTop; pS-&gt;pTop = pNew; return;&#125;void traverse(PStack pS)&#123; if(pS-&gt;pTop == pS-&gt;pBottom)&#123; printf("栈为空!\n"); return; &#125;else&#123; PNode p = pS-&gt;pTop; while (p != pS-&gt;pBottom) &#123; printf("%d ",p-&gt;data); p = p-&gt;pNext; &#125; printf("\n"); &#125; return;&#125;void pop(PStack pS,int* val)&#123;//这里的int *val是为了让用户知道pop的数据是什么 if(pS-&gt;pTop == pS-&gt;pBottom)&#123; printf("栈为空!\n"); return; &#125;else&#123; *val = pS-&gt;pTop-&gt;data; PNode p = pS-&gt;pTop; pS-&gt;pTop = pS-&gt;pTop-&gt;pNext; free(p); p = NULL; &#125; return;&#125;void clear(PStack pS)&#123; if(pS-&gt;pTop == pS-&gt;pBottom)&#123; printf("栈为空!\n"); return; &#125;else&#123; PNode p = pS-&gt;pTop; PNode q; while (p != pS-&gt;pBottom) &#123; q = p; p = p-&gt;pNext; free(q); q = NULL; &#125; pS-&gt;pTop = pS-&gt;pBottom; printf("已clear成功!\n"); return; &#125;&#125; 注：个人感觉有链表基础再去理解栈并不难，我认为所谓的栈只不过是给常见的链表加一个总是指向栈底的指针和一个总是指向栈顶的指针，并且每次添加使其总是添加在栈顶，并更新一下指向栈顶的那个指针便可。和链表相比，栈和链表的区别在链表的头节点是在链表的头部，而对于栈，头节点则是在栈低，即指向栈底的指针总是指向头节点。以上都是我个人学习后总结的一些看法，如果我的总结有不对的地方，请各位看到了之后帮我指出，谢谢(°ω°)ﾉ”]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表的常见操作]]></title>
    <url>%2F2018%2F04%2F19%2F%E9%93%BE%E8%A1%A8%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[这两天一直在看郝斌的数据结构课程，之前上课大多是是理论，但是实际用代码实现还是比较困难，跟着郝斌老师重新温习了一遍链表的课程，受益匪浅，虽然郝斌老师实现所用代码和教材上的代码稍有区别，但是大致思想相同，而且有的部分感觉郝斌老师的方法更加优秀，所以对他上课时代码进行了重现，完成了链表的常见操作，具体代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node* pNext;&#125; Node,*PNode;PNode create_list(void);//创建链表void traverse_list(PNode);//遍历链表void is_empty(PNode);//判断链表是否为空int length_list(PNode);//判断链表的长度void insert_list(PNode,int,int*);//插入节点void delete_list(PNode,int,int*);//删除节点void sort_list(PNode);//对链表进行排序int main() &#123; PNode pHead = NULL; int val; int pos; pHead = create_list(); traverse_list(pHead); int len = length_list(pHead); printf("链表的长度为%d\n",len); printf("请输入要插入的位置以及数值:\n"); scanf("%d %d",&amp;pos,&amp;val); insert_list(pHead, pos, &amp;val); traverse_list(pHead); printf("请输入要删除的位置!\n"); scanf("%d",&amp;pos); delete_list(pHead, pos, &amp;val); traverse_list(pHead); sort_list(pHead); printf("排序后的链表数据如下:"); traverse_list(pHead); return 0;&#125;PNode create_list(void)&#123; int len; int i; int val; PNode pHead = (PNode)malloc(sizeof(Node)); if(pHead == NULL)&#123; printf("分配内存失败!\n"); exit(-1); &#125; PNode pTail = pHead;//这里是创建一个指向尾节点的变量 pTail-&gt;pNext = NULL; printf("请输入要生成的链表的节点数:\n"); scanf("%d",&amp;len); for(i=0;i&lt;len;i++)&#123; printf("请输入第%d个节点的值:\n",i+1); scanf("%d",&amp;val); PNode pNew = (PNode)malloc(sizeof(Node)); if(pNew == NULL)&#123; printf("分配内存失败!\n"); &#125; pNew-&gt;data = val; pTail-&gt;pNext = pNew; pNew-&gt;pNext = NULL; pTail = pNew; &#125; return pHead; &#125;void traverse_list(PNode pHead)&#123; PNode p = pHead-&gt;pNext; printf("链表中的数据为:"); while (p != NULL) &#123; printf("%d ",p-&gt;data); p = p-&gt;pNext; &#125; printf("\n"); return;&#125;void is_empty(PNode pHead)&#123; if(pHead-&gt;pNext == NULL)&#123; printf("该链表为空!\n"); return; &#125;else&#123; printf("该链表不为空!\n"); return; &#125;&#125;int length_list(PNode pHead)&#123; PNode p = pHead-&gt;pNext; int len = 0; while (p != NULL) &#123; p = p-&gt;pNext; len++; &#125; return len;&#125;void insert_list(PNode pHead,int pos,int* val)&#123; PNode p = pHead; int i = 0; while (p != NULL &amp;&amp; i&lt;pos-1) &#123;//使指针最后指向需要插入的节点的前一个节点 p = p-&gt;pNext; i++; &#125; if(i&gt;pos-1 || p==NULL)&#123;//当该节点为空，则说明该节点为尾节点的下一个节点，即输入不合法 printf("输入不正确!\n"); &#125;else&#123; PNode pNew = (PNode)malloc(sizeof(Node)); if(pNew == NULL)&#123; printf("动态分配内存失败!\n"); exit(-1); &#125;else&#123; pNew-&gt;data = *val; pNew-&gt; pNext = p-&gt;pNext; p-&gt;pNext = pNew; &#125; &#125; return;&#125;void delete_list(PNode pHead,int pos,int* val)&#123; PNode p = pHead; int i = 0; while (p-&gt;pNext != NULL &amp;&amp; i&lt;pos-1) &#123; p = p-&gt;pNext; i++; &#125; if (i&gt;pos-1 || p-&gt;pNext == NULL)&#123; printf("输入不正确!\n"); exit(-1); &#125;else&#123; PNode q = p-&gt;pNext; *val = p-&gt;pNext-&gt;data; p-&gt;pNext = p-&gt;pNext-&gt;pNext; free(q); q = NULL;//将q中的野指针赋值为空 printf("删除成功，删除的节点值为%d",*val); &#125; return; &#125;void sort_list(PNode pHead)&#123; int i, j, t; int len = length_list(pHead); PNode p, q; for (i=0,p=pHead-&gt;pNext; i&lt;len-1; i++,p=p-&gt;pNext) &#123; for (j=i+1,q=p-&gt;pNext; j&lt;len; j++,q=q-&gt;pNext) &#123; if (p-&gt;data &gt; q-&gt;data) //类似于数组中的: a[i] &gt; a[j] &#123; t = p-&gt;data;//类似于数组中的: t = a[i]; p-&gt;data = q-&gt;data; //类似于数组中的: a[i] = a[j]; q-&gt;data = t; //类似于数组中的: a[j] = t; &#125; &#125; &#125; return;&#125; 注：在这个代码的插入以及删除时使用的while以及if判断语句极为巧妙，使得不合法输入都可避免，简化了代码还提高了代码的健壮性，我思考了很久才大致理解了其思路，这个后期复习时需要重点的温习一下。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用github+hexo搭建一个博客]]></title>
    <url>%2F2018%2F04%2F12%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%2Bhexo%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[本文章是查阅了一下四个博客文章后所做的总结：https://blog.csdn.net/gdutxiaoxu/article/details/53576018https://www.cnblogs.com/fengxiongZz/p/7707219.htmlhttps://segmentfault.com/a/1190000009009697#articleHeader15https://blog.csdn.net/qq_33699981/article/details/72716951 整体项目需要使用git，node.js等，以及写博客时需要使用markdown语法，最好在部署前对这些有些了解，当然就算不了解也并不影响整个部署 一、配置环境以及准备1.安装并配置node.js以及git2.在github中新建一个项目（记得点击添加README，项目名最好是“github用户名.github.io”）3.在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，发现该项目已经被部署到网络上，能够通过外网来访问它。4.在合适的地方新建一个文件夹，进入文件夹使用下面的命令安装Hexo1npm install hexo -g 5.使用下面的命令查看是否安装成功1hexo -v 6.输入下面的命令初始化文件夹1hexo init 7.输入下面的命令安装所需要的组件1npm install 8.输入下面的命令首次体验hexo1hexo g 9.输入下面的命令开启服务器（若端口被占用则使用 hexo server -p 端口号 ）来改变端口号1hexo s 出现该页面则说明成功了10.将Hexo与github page联系起来（1）配置Git个人信息(如果你之前已经配置好git个人信息，请跳过这一个步骤)a.设置Git的user name和email：(如果是第一次的话）12git config --global user.name &quot;your_name&quot;git config --global user.email &quot;your_email&quot; b.生成密钥1ssh-keygen -t rsa -C &quot;your_email&quot; 11.配置Deployment在_config.yml文件中，找到Deployment，然后按照如下修改：1234deploy: type: git repo: git@github.com:yourname/yourname.github.io.git branch: master 二、写博客，发布文章1.使用下面的命令新建一个博文（新建后在hexo\source\ _posts中将会看到你新建的.md后缀的文件）1hexo new post &quot;article title&quot; 2.使用markdown编辑器打开并进行编辑文章3.使用下面的命令进行生成和部署，成功后便可在https://yourName.github.io访问您的博文12hexo g // 生成hexo d // 部署 注：若提示 deloyer not found:git 则说明缺少一个扩展，运行 npm install --save hexo-deployer-git 命令即可解决三、修改主题hexo提供了许许多多的主题，我们可以轻松的修改一个自己喜欢的主题，具体方法如下： 1.在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。 为了描述方便，在以下说明中，将前者称为 站点配置文件， 后者称为 主题配置文件。在文件夹的themes中使用 git clone 命令下载自己喜欢的主题，如我们需要使用NexT主题，则可以使用12cd your-hexo-site #进入你的hexo项目的根文件夹git clone https://github.com/iissnan/hexo-theme-next themes/next 2.启用主题 与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 启用 NexT 主题1theme: next 此时即可使用浏览器访问 http://localhost:4000 ，检查站点是否正确运行。 当你看到站点的外观与下图所示类似时即说明你已成功安装 NexT 主题。现在，你已经成功安装并启用了 NexT 主题。下一步我们将要更改一些主题的设定，包括个性化以及集成第三方服务。选择 Scheme Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： 1234Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白Mist - Muse 的紧凑版本，整洁有序的单栏外观Pisces - 双栏 Scheme，小家碧玉似的清新Scheme 的切换通过更改 主题配置文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面 注释 # 即可。 选择 Pisce Scheme 123#scheme: Muse#scheme: Mistscheme: Pisces 3.next主题的hexo博客个性化设置博客的可个性化设置较多，比如背景设置，博客头，作者，简介，以及插入一下第三方的插件等。在此我就不做赘述，最后我在当时部署时总结了这两个不错的博客教程https://segmentfault.com/a/1190000009009697#articleHeader15https://blog.csdn.net/qq_33699981/article/details/72716951 4.设置语言 编辑站点配置文件， 将 language 设置成你所需要的语言。建议明确设置你所需要的语言，例如选用简体中文，配置如下：1language: zh-Hans 语言 代码 设定实例 English en language: en 简体中文 zh-Hans language:zh-Hans Français fr-FR language:fr-FR Português pt language:pt 繁體中文 zh-hk或者zh-tw language:zh-hk Русский язык ru language:ru Deutsch de language:de 日本語 ja language:ja Indonesian id language:id]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django集成邮件发送功能]]></title>
    <url>%2F2018%2F03%2F16%2FDjango%E9%9B%86%E6%88%90%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[Django集成邮件发送功能由于之前的一些情况，为了防止有人用无效邮箱进行注册，所以这次在报名时添加了邮箱发送以及验证功能，用于验证邮箱是否有效，当然了，在强大的Django之下，集成邮件功能并不难，接下来我们看一下如何有效的在Django中集成邮件功能吧 1.创建项目 （这里我就不赘述了，之前专门谢了一篇文章用于记录如何创建一个新的Django项目，传送门） 2.在settings.py中配置与邮件相关的配置代码如下：12345EMAIL_HOST = 'smtp.qq.com' #邮箱服务商EMAIL_PORT = 587 #端口EMAIL_HOST_USER = 'user'EMAIL_HOST_PASSWORD = 'password'DEFAULT_FROM_EMAIL = 'your Email Address' 只需要上面短短几行，就ok了，是不是很简单 3.邮件发送相关函数models.py中添加邮箱验证相关类123456789101112class EmailVerifyRecord(models.Model): # 验证码 code = models.CharField(max_length=20, verbose_name=u"验证码") email = models.EmailField(max_length=50, verbose_name=u"邮箱") # 包含注册验证和找回验证 send_time = models.DateTimeField(verbose_name=u"发送时间", default=datetime.now) class Meta: verbose_name = u"邮箱验证码" verbose_name_plural = verbose_name def __str__(self): return self.email 验证码随机数生成函数123456789# 生成随机字符串用来验证邮箱def random_str(randomlength=8): str = '' chars = 'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789' length = len(chars) - 1 random = Random() for i in range(randomlength): str += chars[random.randint(0, length)] return str 邮箱发送函数1234567891011121314def sendEmail(email): email_record = EmailVerifyRecord() # 将给用户发的信息保存在数据库中 code = random_str(16) #这个是生成验证连接的函数 email_record.code = code email_record.email = email email_record.save() # 初始化为空 email_title = "" email_body = "" email_title = "注册激活链接" email_body = " 同学你好，欢迎参加本次计算机基础知识大赛，"+"请点击下面的链接激活你的账号:http://127.0.0.1:8000/users/active/&#123;0&#125;".format(code) # 发送邮件 send_status = send_mail(email_title, email_body, DEFAULT_FROM_EMAIL, [email]) 4.身份验证身份验证的url1url(r'^active/(?P&lt;active_code&gt;.*)/$', views.user_active, name="user_active"), # 提取出active后的所有字符赋给active_code 身份验证相关函数(通过url中传输的active_code，来判断是否验证成功)12345678910111213def user_active(request,active_code): print("可以1") all_records = EmailVerifyRecord.objects.filter(code=active_code) if all_records: for record in all_records: email = record.email # 通过邮箱查找到对应的用户 user = models.User.objects.get(email=email) # 激活用户 user.is_active = True user.save() print("可以") print("不行") 5.总结 通过上面的四步就可以把邮箱功能加入到Django项目中，为用户注册，密码找回等功能提供了一个验证本人身份的好方式项目代码链接在这里传送门]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在搭建报名系统时遇到的django报错]]></title>
    <url>%2F2018%2F03%2F13%2F%E5%9C%A8%E6%90%AD%E5%BB%BA%E6%8A%A5%E5%90%8D%E7%B3%BB%E7%BB%9F%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84django%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[报错1：1The view sign_up.views.new_user didn't return an HttpResponse object. It returned None instead. views.py代码1234567891011def new_user(request): if request.method != 'POST': form = UserInfoForm() else: form = UserInfoForm(request.POST) if form.is_valid(): form.save() return HttpResponseRedirect(reverse('sign_up:index')) context = &#123;'form':form&#125; return render(request,'new_user.html',context) models.py代码12345678class User_info(models.Model): name = models.CharField(max_length=100) student_id = models.CharField(max_length=9) college = models.CharField(max_length=100) date_added = models.DateField(auto_now_add=True) def __str__(self): return self.name new_user.html代码1234567891011&#123;% extends 'sign_up/index.html' %&#125;&#123;% block header %&#125; &lt;div&gt;请填写信息&lt;/div&gt;&#123;% endblock %&#125;&#123;% block content %&#125; &lt;form action="&#123;% url 'sign_up:new_user' %&#125;" method="post"&gt; &#123;% csrf_token %&#125; &#123;% form.as_p %&#125; &lt;button type="submit"&gt;Add user&lt;/button&gt; &lt;/form&gt;&#123;% endblock %&#125; 错误原因：当时报错时查询各种资料以及函数都没有解决。。。最后发现是由于views.py文件中的没有返回值，return错位导致。 正确代码如下： 1234567891011def new_user(request): if request.method != 'POST': form = UserInfoForm() else: form = UserInfoForm(request.POST) if form.is_valid(): form.save() return HttpResponseRedirect(reverse('sign_up:index')) context = &#123;'form':form&#125; return render(request,'new_user.html',context) 报错2： 1Invalid block tag on line 8: 'form.as_p', expected 'endblock'. Did you forget to register or load this tag? 错误原因：前端模版中form.as_p应该放在12345678910&#123;&#123; &#125;&#125;```中，而不是`&#123;% %&#125;`中----### 报错3：![这里写图片描述](/images/在搭建报名系统时遇到的django报错/2B46DB12-FF96-4472-BB3D-4C5BFC6059C4.png)```pythonno such table: sign_up_actor_info 错误原因：应当使用python manage.py makemigrations以及python manage.py migrate来迁移数据库以及修改数据库即可 报错4：1Specifying a namespace in include() without providing an app_name 错误原因：python3 Django 环境下，如果你遇到namespace没有注册以及在根目录下urls.py中的include方法的第二个参数namespace添加之后就出错的问题。请在[app_name]目录下的urls.py中的urlpatterns前面加上app_name=’[app_name]’， [app_name]代表你的应用的名称。 报错5：错误原因：将根目录下的1url(r&apos;^users/$&apos;,include(&apos;users.urls&apos;,namespace=&apos;users&apos;)), 中的$删除 报错6： 1234567891011121314@login_requireddef new_actor(request): if request.method != 'POST': form = ActorInfoForm() else: form = ActorInfoForm(data=request.POST) if form.is_valid(): new_actor = form.save() new_actor.owner = request.user new_actor.save() return HttpResponseRedirect(reverse('sign_up:index')) context = &#123;'form':form&#125; return render(request, 'sign_up/new_actor.html', context) 错误原因：在第一次form.save时应该是用参数commit=False，使其不上传到数据库，因为后面还要对其信息进行添加 正确代码如下： 1234567891011121314@login_requireddef new_actor(request): if request.method != 'POST': form = ActorInfoForm() else: form = ActorInfoForm(data=request.POST) if form.is_valid(): new_actor = form.save(commit=False) #commit=False指不要提交到数据库 new_actor.owner = request.user new_actor.save() return HttpResponseRedirect(reverse('sign_up:index')) context = &#123;'form':form&#125; return render(request, 'sign_up/new_actor.html', context) 报错7： 1'QuerySet' object has no attribute 'owner' 12345678910111213141516@login_requireddef edit_actor(request,actor_id): actor = Actor_info.objects.filter(id=actor_id) if actor.owner != request.user: raise Http404 if request.method == 'POST': college = request.POST.get('college') student_id = request.POST.get('student_id') name = request.POST.get('name') Actor_info.objects.filter(id=actor_id).update( college=college,student_id=student_id,name=name ) return HttpResponseRedirect(reverse('sign_up:actor',args=&#123;'actor_id':actor_id&#125;)) context = &#123;'actor':actor&#125; return render(request,'sign_up/edit_actor.html',context=context) 错误原因：应该使用get而不是filter 正确代码如下： 12345678910111213141516@login_requireddef edit_actor(request,actor_id): actor = Actor_info.objects.get(id=actor_id) if actor.owner != request.user: raise Http404 if request.method == 'POST': college = request.POST.get('college') student_id = request.POST.get('student_id') name = request.POST.get('name') Actor_info.objects.filter(id=actor_id).update( college=college,student_id=student_id,name=name ) return HttpResponseRedirect(reverse('sign_up:actor',args=[actor_id])) context = &#123;'actor':actor&#125; return render(request,'sign_up/edit_actor.html',context=context) 报错8： 12Reverse for 'actor' with no arguments not found. 1 pattern(s) tried: ['actors/(?P&lt;actor_id&gt;\\d+)/$']Request Method: POST 12345678910111213141516@login_requireddef edit_actor(request,actor_id): actor = Actor_info.objects.get(id=actor_id) if actor.owner != request.user: raise Http404 if request.method == 'POST': college = request.POST.get('college') student_id = request.POST.get('student_id') name = request.POST.get('name') Actor_info.objects.filter(id=actor_id).update( college=college,student_id=student_id,name=name ) return HttpResponseRedirect(reverse('sign_up:actor'),args=[actor_id]) context = &#123;'actor':actor&#125; return render(request,'sign_up/edit_actor.html',context=context) 错误原因：args为reverse函数的参数，而不是HttpResponseRedirect的参数 正确代码如下： 12345678910111213141516@login_requireddef edit_actor(request,actor_id): actor = Actor_info.objects.get(id=actor_id) if actor.owner != request.user: raise Http404 if request.method == 'POST': college = request.POST.get('college') student_id = request.POST.get('student_id') name = request.POST.get('name') Actor_info.objects.filter(id=actor_id).update( college=college,student_id=student_id,name=name ) return HttpResponseRedirect(reverse('sign_up:actor',args=[actor_id])) context = &#123;'actor':actor&#125; return render(request,'sign_up/edit_actor.html',context=context)]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask使用orm连接数据库]]></title>
    <url>%2F2018%2F02%2F12%2Fflask%E4%BD%BF%E7%94%A8orm%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[注：使用前请安装flask_sqlalchemy包，由于python-mysql不适用于python3.6，所以我们使用pymysql进行替代 1.flask的app文件中输入以下代码123456789101112131415161718(文件名为：learn_flask_mysql_orm.py)from flask import Flaskfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)app.config["SQLALCHEMY_DATABASE_URI"]="mysql+pymysql://root:@127.0.0.1:3306/learn_flask_mysql"#注：请勿忘记添加'+pymysql'，否则将会出现没有安装mysqldb的报错db = SQLAlchemy(app)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 2.在flask文件夹中新建一个models.py文件，并输入以下代码12345678910111213from learn_flask_mysql_orm import dbclass User(db.Model): __tablename__ = 'user' user_id = db.Column(db.INT,primary_key=True) user_name = db.Column(db.TEXT) def __init__(self,user_id,user_name): self.user_id = user_id self.user_name = user_name def __str__(self): return "id:&#123;&#125; name:&#123;&#125;".format(self.user_id,self.user_name) 3.在flask中新建一个manage.py文件，输入以下代码1234567891011121314151617181920from flask_script import Managerfrom models import Userfrom learn_flask_mysql_orm import app,dbmanager = Manager(app)@manager.commanddef save(): user = User(11,'zhangjia11') db.session.add(user) db.session.commit()@manager.commanddef query_all(): users = User.query.all() for user in users: print(user)if __name__ == "__main__": manager.run() 之后使用python manage.py save即可保存数据，使用python manage.py qurey_all即可查询数据]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask连接mysql数据库]]></title>
    <url>%2F2018%2F02%2F12%2Fflask%E8%BF%9E%E6%8E%A5mysql%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[在flask所在文件夹中新建一个models.py文件，输入以下代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import pymysqldef get_conn(): host = "127.0.0.1" port = 3306 #注：此处应该使用数字而不是字符串 db = 'learn_flask_mysql' user = 'root' password = '' conn = pymysql.connect( host=host, user=user, password=password, port=port, db=db, ) return connclass User(object): def __init__(self,user_id,user_name): self.user_id = user_id self.user_name = user_name def save(self): conn = get_conn() cursor = conn.cursor() sql = "INSERT INTO user(user_id, user_name) VALUES (%s,%s)" cursor.execute(sql,(self.user_id,self.user_name)) conn.commit() cursor.close() conn.close() @staticmethod def query_all(): conn = get_conn() cursor = conn.cursor() sql = "SELECT * from user" cursor.execute(sql) rows = cursor.fetchall() users = [] for row in rows: user = User(row[0],row[1]) users.append(user) conn.commit() cursor.close() conn.close() return users def __str__(self): return "id:&#123;&#125; name:&#123;&#125;".format(self.user_id,self.user_name) 2.在flask新建文件夹中新建一个manage.py，并输入以下代码1234567891011121314151617181920from flask_script import Managerfrom learn_flask_mysql import appfrom models import Usermanager = Manager(app)@manager.commanddef save(): user = User(1,'csdn') user.save()@manager.commanddef query_all(): users = User.query_all() for user in users: print(user)if __name__ == "__main__": manager.run() 之后使用python manage.py save即可保存数据，使用python manage.py qurey_all即可查询数据]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask扩展flask-script文档中文翻译]]></title>
    <url>%2F2018%2F02%2F12%2FFlask%E6%89%A9%E5%B1%95flask-script%E6%96%87%E6%A1%A3%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[本文转载自https://my.oschina.net/lijsf/blog/158828 Flask扩展flask-script文档中文翻译 Flask-Script扩展提供向Flask插入外部脚本的功能。包括运行一个开发用的服务器，一个定制的 Python shell，设置数据库的脚本，cronjobs，以及其他的运行在web应用之外的命令行任务。Flask-Script和Flask本身的工作方式类似。只需要定义和添加能从命令行中被Manager实例调用的命令即可。123456789101112# manage.pyfrom flask.ext.script import Managerfrom myapp import appmanager = Manager(app)@manager.commanddef hello(): print "hello"if __name__ == "__main__": manager.run() 只要像上面一样定义你自己的命令脚本，就可以在命令行中如下调用他们： python manage.py hello Flask-Script的源代码和bug追踪见 GitHub 安装Flask-Script 可以使用pip或者easy_install安装： pip install Flask-Script 或者下载最新开发版本： git clone https://github.com/techniq/flask-script.gitcd flask-scriptpython setup.py develop 如果你使用virtualenv，需保证把Flask-Script和你的Flask应用安装在同一virtualenv环境下 创建并且运行命令 首先，创建一个Python模块运行你的命令脚本。可以任意起名，例如manage.py。无需把所有的命令都放在同一个文件里，例如，在一个大型项目中，可以把相关联的命令放在不同的文件里。在你的manage.py文件中，必须有一个Manager实例。Manager类将追踪所有的在命令行中调用的命令和处理过程的调用运行 情况:123456789from flask.ext.script import Managerapp = Flask(__name__)# configure your appmanager = Manager(app)if __name__ == "__main__": manager.run() 调用 manager.run()将启动Manger实例接收命令行中的命令。Manager只有一个参数：一个Flask实例。如果你想用工场模式，那也可以是一个函数或者其他的返回Flask实例的玩意儿。其次，创建并且加入命令。有三种方法可创建命令：创建Command的子类使用 @command 修饰符使用 @option 修饰符下面是一个简单的例子，创建一个Hello命令，该命令只是简单的输出“hello word”。1234567from flask.ext.script import Commandclass Hello(Command): "prints hello world" def run(self): print "hello world" 再把上面创建的Hello命令加入Manager实例：1manager.add_command('hello', Hello()) 很明显，上面的语句需要在manager.run()之前运行。现在执行下面的命令： python manage.py hellohello world 也可传给Command实例的run方法一个字典：1manager.run(&#123;'hello' : Hello()&#125;) Command class 必须定义一个run方法。定义的位置和参数依赖于你的定义的命令的参数。详见下文。运行下面的命令获取可以使用的命令及其描述的列表： python manage.py 通过运行下面的命令获取一个特定命令的帮助,这将输出这个命令的docstring。 python manage.py runserver -h 上面的第一种方法是最适用的，但也是最麻烦的。对于简单的命令，只需要使用Command实例的@command修饰符。1234@manager.commanddef hello(): "Just say hello" print "hello" 这种方法创建的命令的运行方式和Command类创建的运行方式是相同的。 python manage.py hellohello 如果用Comman类来实现，下面的命令将输出manage类的docstring： python manage.py -hJust say hello 最后，@option修饰符适用于更精细的命令行控制：123@manager.option('-n', '--name', help='Your name')def hello(name): print "hello", name 后面会有更详细的对@option的介绍。 增加命令行参数 #####大多数命令都带有参数。还是上面的例子，如果不仅仅是打印”hello world”，还想输出一个额外的名字，如： python manage.py hello –name=Joehello Joe 或者短参数： python manage.py hello -n Joe 为实现这一功能，需要使用Command类的option_list属性。12345678910from flask.ext.script import Command, Manager, Optionclass Hello(Command): option_list = ( Option('--name', '-n', dest='name'), ) def run(self, name): print "hello %s" % name 长参数和短参数都是存储在Option实例中。详见API部分。另一种方法是为你的Command类定义一个get——options方法，这将在希望依赖运行实例返回值来得到参数时非常有效。123456789101112class Hello(Command): def __init__(self, default_name='Joe'): self.default_name=default_name def get_options(self): return [ Option('-n', '--name', dest='name', default=self.default_name), ] def run(self, name): print "hello", name 若使用@command修饰符，参数将直接自动的从函数的参数中获取：12345678910111213@manager.commanddef hello(name): print "hello", name``` &gt; python manage.py hello Joe&gt; hello Joe##### 或者使用可选参数：```python@manager.commanddef hello(name="Fred") print hello, name 调用方法如下： python manage.py hello –name=Joehello Joe 或者： python manage.py hello -n Joehello Joe 这里需要注意：-n 是由参数的第一个字母决定的。所以”name” &gt; “-n”其次，-h选项通常输出命令的帮助文档，所以避免使用h开头的参数。同时，需要注意选项参数是boolean值，例如：123456@manage.commanddef verify(verified=False): """ Checks if verified """ print "VERIFIED?", "YES" if verified else "NO" 只能这样调用： python manage.py verifyVERIFIED? NO python manage.py verify -vVERIFIED? YES python manage.py verify –verifiedVERIFIED? YES @command修饰符随便简单好用，但在复杂情况下，@option是更好的选择：12345678910111213@manager.option('-n', '--name', dest='name', default='joe')def hello(name): print "hello", name``` ##### 可以增加更多的选项参数：```python@manager.option('-n', '--name', dest='name', default='joe')@manager.option('-u', '--url', dest='url', default=None)def hello(name, url): if url is None: print "hello", name else: print "hello", name, "from", url 可以这样调用： python manage.py hello -n Joe -u reddit.com hello Joe from reddit.com 或者： python manage.py hello –name=Joe –url=reddit.comhello Joe from reddit.com 向manager加入配置项 配置项也可以传给Manager实例。这是你可以设置传给Flask应用的配置项以便一条命令即可完成。例如，你可以使用一个标 志来为你的应用设置配置文件。例如：1234567def create_app(config=None): app = Flask(__name__) if config is not None: app.config.from_pyfile(config) # configure your app... return app 可以使用命令行定义配置文件参数，例如使用一条命令设置数据库，可以根据生产环境和开发环境选用不同的配置文件。为实现传配置参数，可以使用add_option()方法，这和Option的参数一样。1manager.add_option(&apos;-c&apos;, &apos;--config&apos;, dest=&apos;config&apos;, required=False) 同其他Flask-Script配置一样，可以在任何地方使用上面的语句，但确保在manager.run()之前执行。假设你有下面的命令：123456@manager.commanddef hello(name): uppercase = app.config.get('USE_UPPERCASE', False) if uppercase: name = name.upper() print hello, name python manage.py hello joe -c dev.cfghello JOE 注意，”config“选项并没有传给上面的hello命令。为保证manage的选项能正常工作，需要传一个工厂函数给Manager的构造器，而不是一个Flask实例。上面既是可以简单的示 例。 获取用户输入 Flask-Script拥有一组helper函数来获取用户在命令行中的输入，例如：123456789101112from flask.ext.script import Manager, prompt_boolfrom myapp import appfrom myapp.models import dbmanager = Manager(app)@manager.commanddef dropdb(): if prompt_bool( "Are you sure you want to lose all your data"): db.drop_all() 执行如下： python manage.py dropdbAre you sure you want to lose all your data ? [N] #####从下文API中获取更多关于prompt functions的内容。 默认命令 Flask-Script拥有一对预设的命令，你可以加入或者定制：Server and Shell。Server命令运行Flask的开发server，它带有一个可选的端口参数，默认是5000。12345678from flask.ext.script import Server, Managerfrom myapp import create_appmanager = Manager(create_app)manager.add_command("runserver", Server())if __name__ == "__main__": manager.run() 运行如下： python manage.py runserver Server命令有一组命令行参数，运行python manage.py runserver -h 获取详细信息。你也可以在构造函数中重新定义默认 行为：1server = Server(host=&quot;0.0.0.0&quot;, port=9000) 无需赘言，开发Server不是为生产环境准备的。Shell名令启动一个Python shell。可以穿进去一个make_context参数，这个参数必须是一个字典。默认情况下，将返回你的 Flask应用实例。12345678910111213from flask import appfrom flask.ext.script import Shell, Managerfrom myapp import appfrom myapp import modelsfrom myapp.models import dbdef _make_context(): return dict(app=app, db=db, models=models)manager = Manager(create_app)manager.add_command("shell", Shell(make_context=_make_context)) 这将对于你希望在shell引入一组默认的包非常有利，无需再输入很多import语句。Shell命令将使用IPthon，如果安装了的话。否则，默认使用标准Python shell。你可以用两种方法关闭这一 行为：传use_ipython参数给Shell构造器，或者在命令行中传标记–no-ipython。1shell = Shell(use_ipython=False) 也有一个shell修饰符，你可以在函数上下文中使用。123@manager.shelldef make_shell_context(): return dict(app=app, db=db, models=models) 这将使这个命令成为shell的默认执行的命令。 python manage.py shell 默认命令shell和runserver是默认引入的，并且带有这两个命令的默认选项。若你想用其他命令替代默认的命令，只要重写 add_command()或者修饰符。若你传给Manager的构造器一个with_default_commands=False参数，则这些命令不会被载入。1manager = Manager(app, with_default_commands=False)]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask的外部脚本的使用]]></title>
    <url>%2F2018%2F02%2F12%2Fflask%E7%9A%84%E5%A4%96%E9%83%A8%E8%84%9A%E6%9C%AC%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.在flask文件夹中新建一个manage.py文件，在文件中输入以下代码12345678from flask_script import Managerfrom learn_flask_script import appmanager = Manager(app)@manager.commanddef hello(): print("hello world") 在终端中使用python manage.py hello即可运行该脚本 2.在flask文件夹中新建一个manage.py文件，在文件中输入以下代码12345678from flask_script import Managerfrom learn_flask_script import appmanager = Manager(app)@manager.option('-m','--msg',dest='msg_val',default='world')def hello_world(msg_val): print('hello '+msg_val) 在终端中使用python manage.py hello -m csdn或者python manage.py hello --msg_val=csdn即可运行该脚本运行结果为hello csdn]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask连接sqlite3数据库]]></title>
    <url>%2F2018%2F02%2F12%2Fflask%E8%BF%9E%E6%8E%A5sqlite3%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.在flask文件夹中新建一个models.py文件用来定义模型，内部代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243import sqlite3 #导入sqlite3包def get_conn(): #定义该函数用来连接数据库 return sqlite3.connect("test.db")class User(object): def __init__(self,id,name): self.id = id self.name = name def save(self): sql = "insert into user VALUES (?,?)"#sql语句 conn = get_conn()#连接数据库 cursor = conn.cursor()#定义一个游标 cursor.execute(sql,(self.id,self.name))#执行sql语句 conn.commit()#提交数据库改动 cursor.close()#关闭游标 conn.close()#关闭数据库连接 ''' staticmethod相当于一个定义在类里面的函数，所以如果一个方法既不跟实例 相关也不跟特定的类相关，推荐将其定义为一个staticmethod，这样不仅使代 码一目了然，而且似的利于维护代码。 ''' @staticmethod def query(): sql = "select * from user" conn = get_conn() cursor = conn.cursor() rows = cursor.execute(sql) users = [] for row in rows: user = User(row[0],row[1]) users.append(user) conn.commit() cursor.close() conn.close() return users def __str__(self): return 'id:&#123;&#125;--name:&#123;&#125;'.format(self.id,self.name)#注此处的是点不是逗号 2.在flask文件夹中新建一个manage.py文件 ###（1）导入flask_script包，导入sqlite3包，导入models.py中定义的模型，导入flask中的应用，所以最终头部代码如下1234from flask_script import Managerfrom learn_flask_script import appimport sqlite3from models import User ###（2）创建数据库，代码如下123456789@manager.commanddef init_db(): sql = "create table user (id INT,name TEXT)" conn = sqlite3.connect("test.db") cursor = conn.cursor() cursor.execute(sql) conn.commit() cursor.close() conn.close() ###（3）保存数据，代码如下1234@manager.commanddef save(): user = User(1,'csdn') user.save() ###（4）查询数据，代码如下12345@manager.commanddef query_all(): users = User.query() for user in users: print(user) 整体代码如下12345678910111213141516171819202122232425262728293031from flask_script import Managerfrom learn_flask_script import appimport sqlite3from models import Usermanager = Manager(app)@manager.commanddef init_db(): sql = "create table user (id INT,name TEXT)" conn = sqlite3.connect("test.db") cursor = conn.cursor() cursor.execute(sql) conn.commit() cursor.close() conn.close()@manager.commanddef save(): user = User(1,'zhangjia') user.save()@manager.commanddef query_all(): users = User.query() for user in users: print(user)if __name__ == "__main__": manager.run() 3.在终端中使用命令即可运行数据库的创建，添加以及查询]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask路由，消息提醒，以及异常处理]]></title>
    <url>%2F2018%2F02%2F11%2Fflask%E8%B7%AF%E7%94%B1%EF%BC%8C%E6%B6%88%E6%81%AF%E6%8F%90%E9%86%92%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.创建一个小的flask应用12345678910（文件名为app.py）from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() 上述代码便是一个最简单的flask应用，使用python app.py即可运行，命令行运行情况如下12$ python app.py * Running on http://127.0.0.1:5000/ 2.flask的路由 ###（1）下面的代码表示在http://127.0.0.1:5000/hello路径下可以在网页中看到返回值`hello`123456from flask import Flaskapp = Flask(__name__)@app.route('/hello')def hello(id): return("hello") ###（2）下面的代码表示可以在获取路由中的值，如在路径中输入路径http://127.0.0.1:5000/hello/123，页面中将会显示返回值`hello 123`123456from flask import Flaskapp = Flask(__name__)@app.route('/hello/&lt;id&gt;')def hello(id): return("hello "+id) ###（3）下面的代码同样可以获取路由中的值，如在路径中输入http://127.0.0.1:5000/hello?id=123，页面将会显示返回值`hello 123`1234567from flask import Flask,requestapp = Flask(__name__)@app.route('/hello')def hello(): id = request.args.get('id') return("hello "+id) 3.flask反向路由 第二个函数将会通过url_for函数来获取函数名为hello的函数的路由，即在路径中输入http://127.0.0.1:5000/hello_url，网页将会显示`hello /hello`1234567891011from flask import Flask,url_forapp = Flask(__name__)@app.route('/hello')def hello(): return("hello")#反向路由@app.route('/hello_url')def hello1(): return("hello "+url_for('hello')) 4.前后端数据传输 ###（1）后端向前端传输数据 下面的代码最后在在路径中输入http://127.0.0.1:5000/hello即可看到页面返回了hello csdnhello 1 后端代码123456789from flask import Flask,render_templateapp = Flask(__name__)@app.route('/hello')def hello(): user = User(1,"csdn") context = &#123;"user":user&#125; return render_template("hello.html",context=context) #注意，此处的传参数方式与Django中的不同，前一个context指传到前端的参数名 前端代码123456789101112（文件名为hello.html）&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;user_index&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello &#123;&#123; context.user.user_name &#125;&#125;&lt;/h1&gt;&lt;h1&gt;Hello &#123;&#123; context.user.user_id &#125;&#125;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; ###（2）前端向后端提交数据 下面的代码最后在在路径中输入http://127.0.0.1:5000/hello即可返回一个表单，`if request.method == ‘GET’:`用来判断是否是第一次请求，当时第一次请求时将返回一个空表单，如不是则将表单提交到后端进行处理后端代码12345678910111213141516171819202122232425from flask import Flask,render_template,request,flashapp = Flask(__name__)app.secret_key = '123'@app.route("/hello",methods=['POST','GET'])def login_index(): if request.method == 'GET': return render_template('hello.html') else: form = request.form user_name = form.get("user_name") password = form.get("password") if not user_name: flash("Please input username") return render_template("hello.html") if not password: flash("Please input password") return render_template("hello.html") if user_name == "csdn" and password == "12345678": flash("Login succeed") return render_template("hello.html") else: flash("username or password is wrong") return render_template("hello.html") 前端代码1234567891011121314151617（文件名为hello.html）&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello&lt;/h1&gt;&lt;form action="/login" method="post"&gt; &lt;input type="text" name="user_name"&gt; &lt;input type="password" name="password"&gt; &lt;input type="submit" name="submit" value="submit"&gt;&lt;/form&gt;&lt;h2&gt;&#123;&#123; get_flashed_messages()[0] &#125;&#125;&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt; 5.消息提醒 ####（1）使用消息提示是需要配置secret_key ####（2）flash获得的为一个列表，所以前端使用get_flashed_messages()[0]来获得消息提醒 后端代码12345678from flask import Flask,render_template,flashapp = Flask(__name__)app.secret_key = &apos;123&apos;@app.route(&quot;/hello&quot;)def hello(): flash(&quot;csdn&quot;) return render_template(&quot;hello.html&quot;) 前端代码123456789101112（文件名为hello.html）&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello&lt;/h1&gt;&lt;h2&gt;&#123;&#123; get_flashed_messages()[0] &#125;&#125;&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt; 6.异常处理 ###（1）404错误 当路由中输入错误路径，将会返回404页面后端代码123456from flask import Flask,render_templateapp = Flask(__name__)@app.errorhandler(404)def hello(error): return render_template("404.html") 前端代码1234567891011（文件名为404.html）&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;抱歉您找的页面不存在&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; ###（2）有条件的跳转到404页面 当在路由中输入http://127.0.0.1:5000/error_login/1是返回登录成功页面，反之返回404页面####后端代码123456789from flask import Flask,render_template,abortapp = Flask(__name__)@app.route("/hello/&lt;id&gt;")def hello(id): if int(id) == 1: return render_template("hello.html") else: abort(404) 前端代码1234567891011（文件名为hello.html）&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;您已登录成功&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>flask</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django网站部署前期准备]]></title>
    <url>%2F2018%2F02%2F10%2FDjango%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87%2F</url>
    <content type="text"><![CDATA[建立虚拟环境 ####（1）创建一个空文件夹，并命名为learning_log ####（2）在终端中切换到该文件夹中使用virtualenv ll_env创建一个虚拟环境，如创建失败请使用pip install virtualenv命令来安装virtualenv包 ####（3）完成上述命令后learning_log文件加如下所示 激活虚拟环境 ####（1）使用source ll_env/bin/activate命令激活虚拟环境，激活后终端前将出现（ll_env） 注：如果需要停止虚拟环境，请使用deactivate命令来关闭虚拟环境 安装，新建Django项目 ####（1）创建并激活虚拟环境之后，我们就可以安装Django了安装命令为pip install django ####（2）使用django-admin.py startproject learning_log .来新建项目（注意不要忽略.，如没有.则manage.py文件将不与ll_env文件夹在同一目录，会影响后期部署） 创建数据库 ####（1）使用命令python manage.py migrate来修改数据库，我们将这称为迁移数据库，首次执行将新建一个必要的数据表 查看项目 ####（1）使用python manage.py runserver来启动该项目，启动之后便可在http://127.0.0.1:8000/打开该网站，若8000端口被占用可使用`python manage.py runserver 8001`使网站在8001端口打开，如仍被占用，请继续向后增加，直至成功，关闭请按ctrl+c 创建应用程序 ####（1）使用python manage.py startapp learning_logs命令创建一个名为learning_logs的app,运行后文件夹中将出现learning_logs文件夹 定义模型 ####（1）打开models.py文件添加以下代码12345678910from django.db import modelsfrom django.contrib.auth.models import User# Create your models here.class Topic(models.Model): text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) def __str__(self): return self.text DateTimeField(auto_now_add=True)是指日期自动添加，str函数表示了该模型的简单显示，该模型的返回值等于text字符串 ###激活模型 ####（1）在settings.py文件中将learning_logs这个app添加1234567891011INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', #我的应用 'learning_logs',] ####（2）我们使用python manage.py makemigrations命令，makemigrations是Django确认数据库如何修改，输出表示django创建了一个0001_initial.py的迁移文件，之后使用python manage.py migrate这种迁移数据库来修改数据库 #####注：管理数据的正常顺序为：修改models.py，对修改使用makemigrations，最后使用migrate来迁移数据 ###Django管理网站 ####（1）创建超级用户，命令为python manage.py createsuperuser ####（2）向管理网站注册模型，在admin.py文件中添加以下代码123456from django.contrib import admin# Register your models here.from learning_logs.models import Topic,Entryadmin.site.register(Topic) ####之后登录http://127.0.0.1:8000/admin通过Django的后台来进行网站管理，在这个网站中寻在Topic模型，可以在这里添加Topic ####（3）同理我们可以定义一个Entry模型，models.py文件中添加的代码如下12345678910class Entry(models.Model): topic = models.ForeignKey(Topic) text = models.TextField() date_added = models.DateTimeField(auto_now_add=True) class Meta: verbose_name_plural = 'entries' def __str__(self): return self.text[:50] + '...' 这里的str指返回text的前50的字符，Meta指当多个entry时为entries，如没有默认则为entrys####（4）进行数据迁移 ####（5）向管理网站注册entry，在admin.py中添加以下代码1admin.site.register(Entry)]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow由于未初始化变量所导致的错误]]></title>
    <url>%2F2017%2F12%2F07%2Ftensorflow%E7%94%B1%E4%BA%8E%E6%9C%AA%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%98%E9%87%8F%E6%89%80%E5%AF%BC%E8%87%B4%E7%9A%84%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[初始代码12345678910111213141516171819202122232425262728import pandas as pdimport numpy as npimport tensorflow as tftrain_input_data = pd.read_excel('new_data/4.12.2.xlsx',header=None,sheetname='train')train_input_data = np.array(train_input_data).ravel()train_input_data = list(train_input_data)train_input_data = tf.constant(train_input_data,dtype=tf.float32)train_output_data = train_input_datatest_input_data = pd.read_excel('new_data/4.12.2.xlsx',header=None,sheetname='test')test_input_data = np.array(test_input_data).ravel()test_input_data = list(test_input_data)test_input_data = tf.constant(test_input_data,shape=[270,1],dtype=tf.float32)test_output_data = test_input_dataprint(type(train_input_data))fc_mean, fc_var = tf.nn.moments(train_input_data,axes=[0])scale = tf.Variable(tf.ones([1]))shift = tf.Variable(tf.zeros([1]))epslion = 0.001train_input_data = tf.nn.batch_normalization(train_input_data,fc_mean,fc_var,shift,scale,epslion)sess = tf.Session()sess.run(train_input_data) 报错：12345678910111213141516171819202122232425262728293031323334353637383940414243444546Traceback (most recent call last): File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call return fn(*args) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1306, in _run_fn status, run_metadata) File "/Users/anaconda/lib/python3.6/contextlib.py", line 89, in __exit__ next(self.gen) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status pywrap_tensorflow.TF_GetCode(status))tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable_1 [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=["loc:@Variable_1"], _device="/job:localhost/replica:0/task:0/cpu:0"](Variable_1)]]During handling of the above exception, another exception occurred:Traceback (most recent call last): File "/Users/PycharmProjects/太阳能预测/test.py", line 30, in &lt;module&gt; a = sess.run(train_input_data) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 895, in run run_metadata_ptr) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1124, in _run feed_dict_tensor, options, run_metadata) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run options, run_metadata) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call raise type(e)(node_def, op, message)tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value Variable_1 [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=["loc:@Variable_1"], _device="/job:localhost/replica:0/task:0/cpu:0"](Variable_1)]]Caused by op 'Variable_1/read', defined at: File "/Users/PycharmProjects/太阳能预测/test.py", line 21, in &lt;module&gt; shift = tf.Variable(tf.zeros([1])) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py", line 199, in __init__ expected_shape=expected_shape) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variables.py", line 330, in _init_from_args self._snapshot = array_ops.identity(self._variable, name="read") File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py", line 1400, in identity result = _op_def_lib.apply_op("Identity", input=input, name=name) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op op_def=op_def) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 2630, in create_op original_op=self._default_original_op, op_def=op_def) File "/Users/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1204, in __init__ self._traceback = self._graph._extract_stack() # pylint: disable=protected-accessFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_1 [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=["loc:@Variable_1"], _device="/job:localhost/replica:0/task:0/cpu:0"](Variable_1)]] 错误原因：在初始的代码中，由于没有使用tf.global_variables_initializer()函数来对变量初始化，导致代码出现错误解决方法：在代码中加入tf.global_variables_initializer()函数来对数据进行初始化之后便可解决正确代码12345678910111213141516171819202122232425import pandas as pdimport numpy as npimport tensorflow as tftrain_input_data = pd.read_excel('new_data/4.12.2.xlsx',header=None,sheetname='train')train_input_data = np.array(train_input_data).ravel()train_input_data = list(train_input_data)train_input_data = tf.constant(train_input_data,dtype=tf.float32)train_output_data = train_input_datatest_input_data = pd.read_excel('new_data/4.12.2.xlsx',header=None,sheetname='test')test_input_data = np.array(test_input_data).ravel()test_input_data = list(test_input_data)test_input_data = tf.constant(test_input_data,shape=[270,1],dtype=tf.float32)test_output_data = test_input_datafc_mean, fc_var = tf.nn.moments(train_input_data,axes=[0])scale = tf.Variable(tf.ones([1]))shift = tf.Variable(tf.zeros([1]))epslion = 0.001train_input_data = tf.nn.batch_normalization(train_input_data,fc_mean,fc_var,shift,scale,epslion)sess = tf.Session()init = tf.global_variables_initializer()sess.run(init)]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Tensorflow实现CNN进行MNIST数字识别]]></title>
    <url>%2F2017%2F11%2F18%2F%E4%BD%BF%E7%94%A8Tensorflow%E5%AE%9E%E7%8E%B0CNN%E8%BF%9B%E8%A1%8CMNIST%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485from __future__ import print_functionimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# number 1 to 10 datamnist = input_data.read_data_sets('MNIST_data', one_hot=True)def compute_accuracy(v_xs, v_ys):#返回准确度 global prediction y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs, keep_prob: 1&#125;) correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys, keep_prob: 1&#125;) return resultdef weight_variable(shape):#定义weight initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape):#定义bias initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W):#定义卷积层 # stride [1, x_movement, y_movement, 1] # Must have strides[0] = strides[3] = 1 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x):#定义池化层 # stride [1, x_movement, y_movement, 1] return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28ys = tf.placeholder(tf.float32, [None, 10])keep_prob = tf.placeholder(tf.float32)x_image = tf.reshape(xs, [-1, 28, 28, 1])#-1指将所有图片的例子维度不管他，之后再加上所有的维度，然后将xs图片信息转变成28x28的矩阵，1指只有一个通道，即黑白的图片# print(x_image.shape) # [n_samples, 28,28,1]## conv1 layer ##W_conv1 = weight_variable([5,5, 1,32]) # 指筛选器 5x5, 步数 1, 深度 32b_conv1 = bias_variable([32])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32h_pool1 = max_pool_2x2(h_conv1) # output size 14x14x32## conv2 layer ##W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64，激活去线性化h_pool2 = max_pool_2x2(h_conv2) # output size 7x7x64## fc1 layer ##W_fc1 = weight_variable([7*7*64, 1024])b_fc1 = bias_variable([1024])# [n_samples, 7, 7, 64] -&gt;&gt; [n_samples, 7*7*64]h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)## fc2 layer ##W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)# the error between prediction and real datacross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # losstrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)sess = tf.Session()# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12 and int((tf.__version__).split('.')[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_prob: 0.5&#125;) if i % 50 == 0: print(compute_accuracy( mnist.test.images[:1000], mnist.test.labels[:1000]))]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python常用的程序调试方法]]></title>
    <url>%2F2017%2F11%2F17%2FPython%E5%B8%B8%E7%94%A8%E7%9A%84%E7%A8%8B%E5%BA%8F%E8%B0%83%E8%AF%95%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[程序能一次写完并正常运行的概率很小，基本不超过1%，总会有各种各样的bug需要修正。有的bug很简单，看看错误信息就知道，有的bug很复杂，我们需要知道出错时，哪些变量的值是正确的，哪些变量的值是错误的，因此，需要一整套调试程序的手段来修复bug。下面我们来看下常用的Python调试方法 1. 断点打印法第一种方法简单直接粗暴有效，就是用print把可能有问题的变量打印出来看看：err.py1234567def foo(s): n = int(s) print &apos;&gt;&gt;&gt; n = %d&apos; % n return 10 / ndef main(): foo(&apos;0&apos;)main() 执行后在输出中查找打印的变量值：12345$ python err.py&gt;&gt;&gt; n = 0Traceback (most recent call last): ...ZeroDivisionError: integer division or modulo by zero 用print最大的坏处是将来还得删掉它，想想程序里到处都是print，运行结果也会包含很多垃圾信息。所以，我们又有第二种方法。 2. 断言####凡是用print来辅助查看的地方，都可以用断言（assert）来替代： err.py123456def foo(s): n = int(s) assert n != 0, &apos;n is zero!&apos; return 10 / ndef main(): foo(&apos;0&apos;) assert的意思是，表达式n != 0应该是True，否则，后面的代码就会出错。如果断言失败，assert语句本身就会抛出AssertionError：1234$ python err.pyTraceback (most recent call last): ...AssertionError: n is zero! 程序中如果到处充斥着assert，和print相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert：1234$ python -O err.pyTraceback (most recent call last): ...ZeroDivisionError: integer division or modulo by zero 关闭后，你可以把所有的assert语句当成pass来看。 3. logging日志把print替换为logging是第3种方式，和assert比，logging不会抛出错误，而且可以输出到文件：####err.py12345import loggings = &apos;0&apos;n = int(s)logging.info(&apos;n = %d&apos; % n)print 10 / n logging.info()就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何信息。怎么回事？别急，在import logging之后添加一行配置再试试：12import logginglogging.basicConfig(level=logging.INFO) 看到输出了：123456$ python err.pyINFO:root:n = 0Traceback (most recent call last): File &quot;err.py&quot;, line 8, in &lt;module&gt; print 10 / nZeroDivisionError: integer division or modulo by zero 这就是logging的好处，它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 4. pdb调试第4种方式是启动Python的调试器pdb，让程序以单步方式运行，可以随时查看运行状态。我们先准备好程序：err.py123s = &apos;0&apos;n = int(s)print 10 / n 运行：123$ python -m pdb err.py&gt; /Users/PythonTab/Github/sicp/err.py(2)&lt;module&gt;()-&gt; s = &apos;0&apos; 以参数-m pdb启动后，pdb定位到下一步要执行的代码-&gt; s = ‘0’。输入命令l来查看代码：123456(Pdb) l 1 # err.py 2 -&gt; s = &apos;0&apos; 3 n = int(s) 4 print 10 / n[EOF] 输入命令n可以单步执行代码：123456(Pdb) n/Users/PythonTab/Github/sicp/err.py(3)&lt;module&gt;()-&gt; n = int(s)(Pdb) n/Users/PythonTab/Github/sicp/err.py(4)&lt;module&gt;()-&gt; print 10 / n 任何时候都可以输入命令p 变量名来查看变量：1234(Pdb) p s&apos;0&apos;(Pdb) p n0 输入命令q结束调试，退出程序：12345(Pdb) nZeroDivisionError: &apos;integer division or modulo by zero&apos;&gt; /Users/PythonTab/Github/sicp/err.py(4)&lt;module&gt;()-&gt; print 10 / n(Pdb) q 这种通过pdb在命令行调试的方法理论上是万能的，但实在是太麻烦了，如果有一千行代码，要运行到第999行得敲多少命令啊。还好，我们还有另一种调试方法。pdb.set_trace()这个方法也是用pdb，但是不需要单步执行，我们只需要import pdb，然后，在可能出错的地方放一个pdb.set_trace()，就可以设置一个断点：err.py12345import pdbs = &apos;0&apos;n = int(s)pdb.set_trace() # 运行到这里会自动暂停print 10 / n 运行代码，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行：12345678910$ python err.py &gt; /Users/PythonTab/Github/sicp/err.py(7)&lt;module&gt;()-&gt; print 10 / n(Pdb) p n0(Pdb) cTraceback (most recent call last): File &quot;err.py&quot;, line 7, in &lt;module&gt; print 10 / nZeroDivisionError: integer division or modulo by zero 这个方式比直接启动pdb单步调试效率要高很多，但也高不到哪去。5. IDE调试如果要比较爽地设置断点、单步执行，就需要一个支持调试功能的IDE。目前比较好的Python IDE有PyCharm，另外，Eclipse加上pydev插件也可以调试Python程序。 小结写程序花费的时间往往要小于调试的时间，这个是基本规律。虽然用IDE调试起来比较方便，但是最后你会发现，logging才是终极武器。断点调试也是高手的终极利器！ 本文章为转载，原文章链接：http://www.pythontab.com/html/2017/pythonhexinbiancheng_1115/1182.html?ref=myread]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow卷积神经网络常用结构]]></title>
    <url>%2F2017%2F11%2F17%2FTensorflow%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B8%B8%E7%94%A8%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1.卷积层Tensorflow对卷积神经网络有着很好的支持，下面的程序实现了一个卷积层的前向传播。1234567891011121314151617181920212223import tensorflow as tffilter_weight = tf.get_variable( #前两个维度代表过滤器的尺寸，第三个维度表示当前层的深度， #第四个表示过滤器的深度。 'weight',[5, 5, 3, 16], initializer=tf.truncated_normal_initializer(stddev=0.1))biases = tf.get_variable( #由于当前过滤器的深度为16，所以偏置相为16，也是神经网络中下一层节点矩阵深度 'biases',[16],initializer=tf.constant_initializer(0.1))conv = tf.nn.conv2d( input,filter_weight,strides=[1,1,1,1],padding='SAME')bias = tf.nn.bias_add(conv,biases)#通过ReLU函数激活，去线性化actived_conv = tf.nn.relu(bias) 注：1.tf.nn.conv2d提供了一个简单方便的函数来实现卷积层前向传播的算法。该函数第一个输入为当前层的节点矩阵，注意该矩阵为一个四维矩阵，后面三个维度对应一个节点矩阵，第一维输入一个batch。如input[0,:,:,:]指输入第一张图片，input[1,:,:,:]指输入第二张图片。该函数提供的第二个参数是卷积层的权重，第三个为不同维度上的步长，虽然第三个参数提供的一个长度为4的数组，但是第一维和第四维一定为1，因为卷积层的步数只对矩阵的长宽有效。最后一个参数为填充（padding）的方法，一共有’SAME’和’VALID’两种选择，其中SAME是指添加全0填充，VAILID表示不添加。2.tf.nn.bias_add提供一个方便的函数给每个节点加上一个偏置项。注意这里不能直接使用方法，因为矩阵上不同位置的节点都需要加上同样的偏置项。2.池化层1pool = tf.nn.max_pool(actived_conv,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME') tf.nn.max_pool实现了最大池化层的前向传播，他的参数和tf.nn.conv2d函数类似。ksize提供了过滤器的尺寸，strides提供了步长信息，padding提供了是否需要使用全0填充。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow持久化原理及数据格式]]></title>
    <url>%2F2017%2F11%2F16%2FTensorflow%E6%8C%81%E4%B9%85%E5%8C%96%E5%8E%9F%E7%90%86%E5%8F%8A%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Tensorflow持久化原理及数据格式 Tensorflow是一个通过图的形式来表述计算的编程系统，Tensorflow中所有的计算都会被表达成计算图上的节点。Tensorflow通过元图（MetaGraph）来记录计算图中的信息，以及运行计算图中节点所需要的元数据。以下代码给出了MetaGraphDef类型的定义1234567message MetaGraphDef&#123; MeatInfoDef meta_info_def = 1; GraphDef graph_def = 2; SaverDef saver_def = 3; map&lt;string,CollectionDef&gt; collection_def = 4; map&lt;string,SignatureDef&gt; signature_def = 5;&#125; 保存MetaGraphDef信息的文件默认以.meta为后缀名，在之前的例子中文件test.ckpt.meta中存储的就是元图的数据。由于得到的是二进制文件不方便查看。为了方便调试，Tensorflow提供了export_meta_graph函数，这个函数支持以json格式导出MetaGraphDef。下面为实现的代码123456789import tensorflow as tfv1 = tf.Variable(tf.constant(1.0, shape=[1]), name ="v1")v2 = tf.Variable(tf.constant(2.0, shape=[1]), name ="v2")result1 = v1 + v2saver = tf.train.Saver()#通过export_meta_graph函数导出Tensorflow的计算图的元图，并保存为json格式saver.export_meta_graph("test/test.ckpt.json", as_text=True) meta_info_def 属性meta_info_def属性通过MetaInfoDef定义，它记录了Tensorflow计算图中的元数据以及Tensorflow程序中所有使用到的运算方法的信息。下面的MetaInfoDef的定义1234567message MetaInfoDef&#123; #saver没有特殊指定，默认属性都为空。meta_info_def属性里只有stripped_op_list属性不能为空。 string meta_graph_version = 1;#该属性不能为空 OpList stripped_op_list = 2;#该属性记录了计算图中使用到的所有运算方法的信息，该函数只记录运算信息，不记录计算的次数 google.protobuf.Any any_info = 3; repeated string tags = 4;&#125; 元数据包括计算图的版本号（meta_graph_version属性）以及用户指定的一些标签（tags属性）。 OpList类型是一个OpDef类型的列表，以下代码给出OpDef类型的定义：12345678910111213message opDef&#123; string name = 1;#定义了运算的名称 repeated ArgDef input_arg = 2; #定义了输入，属性是列表 repeated ArgDef output_arg =3; #定义了输出，属性是列表 repeated AttrDef attr = 4;#给出了其他运算的参数信息 string summary = 5; string description = 6; OpDeprecation deprecation = 8; bool is_commutative = 18; bool is_aggregate = 16 bool is_stateful = 17; bool allows_uninitialized_input = 19;&#125;; 下面给出一个比较有代表性的运算来辅助说明OpDef的数据结构。1234567891011121314151617181920212223242526op &#123; name: "Add" input_arg&#123; name: "x" type_attr:"T" &#125; input_arg&#123; name: "y" type_attr:"T" &#125; output_arg&#123; name: "z" type_attr:"T" &#125; attr&#123; name:"T" type:"type" allow_values&#123; list&#123; type:DT_HALF type:DT_FLOAT ... &#125; &#125; &#125;&#125; 上面给出的是名称为Add的运算。这个运算的输入有两个，输出有一个，输入输出属性均指定了属性typr_attr，并且这个属性的值为T。在OpDef的attr的属性中。必须要出现名称（name）为T的属性。以上样例中，这个属性指定了运算输入输出允许的参数类型 （allowed_values）。 graph_def属性graph_def属性主要记录了计算图中的节点信息。Tensorflow计算图中的一个节点对应Tensorflow中的一个运算。因为meta_info_def中已经包含所有运算的具体信息，所以graph_def属性指关注运算的连接结构。GraphDef主要包含了一个NodeDef类型的列表。以下代码给出GraphDef和NodeDef类型中包含的信息：1234567891011121314151617181920212223242526272829message GraphDef&#123; #GraphDef的主要信息存储在node属性中，他记录了Tensorflow计算图上所有的节点信息。 repeated NodeDef node = 1; VersionDef versions = 4; #主要储存了Tensorflow的版本号&#125;;message NodeDef&#123; #NodeDef类型中有一个名称属性name，他是一个节点的唯一标识符，在程序中，通过节点的名称来获得相应的节点。 string name = 1; ''' op属性给出了该节点使用的Tensorflow运算方法的名称。 通过这个名称可以在TensorFlow计算图元图的meta_info_def属性中找到该运算的具体信息。 ''' string op = 2; ''' input属性是一个字符串列表，他定义了运算的输入。每个字符串饿的取值格式为弄的：src_output node部分给出节点名称，src_output表明了这个输入是指定节点的第几个输出。 src_output=0时可以省略src_output部分 ''' repeated string input = 3; #制定了处理这个运算的设备，可以是本地或者远程的CPU or GPU。属性为空时自动选择 string device = 4; #制定了和当前运算有关的配置信息 map&lt;string, AttrValue&gt; attr = 5;&#125;; 下面列举test.ckpt.meta.json具体介绍graph_def属性1234567891011121314151617181920212223242526272829303132333435graph def &#123; node &#123; name: "v1" op: "Variable" attr &#123; key:"_output_shapes" value &#123; list&#123; shape &#123; dim &#123; size: 1 &#125; &#125; &#125; &#125; &#125; &#125; attr &#123; key :"dtype" value &#123; type: DT_FLOAT &#125; &#125; ... &#125; node &#123; name :"add" op :"Add" input :"v1/read" #read指读取变量v1的值 input: "v2/read" ... &#125; node &#123; name: "save/control_dependency" #指系统在完成tensorflow模型持久化过程中自动生成一个运算。 op:"Identity" ... &#125; versions &#123; producer :9 #给出了文件使用时的Tensorflow版本号。 &#125;&#125; saver_def属性1234567891011121314message SaverDef &#123; string filename_tensor_name = 1; string save_tensor_name = 2; string restore_op_name = 3; int32 max_to_keep = 4; bool sharded = 5; float keep_checkpoint_every_n_hours = 6; enum CheckpointFormatVersion &#123; LEGACY = 0; V1 = 1; V2 = 2; &#125; CheckpointFormatVersion version = 7;&#125; 下面给出test.ckpt.meta.json文件中saver_def属性的内容。1234567891011121314151617saver_def &#123; filename_tensor_name :"save/Const:0” #给出了保存文件的张量名，这个张量就是节点save/Const的第一个输出。 save_tensor_name :"save/control_dependency: 0” #给出了持久化模型运算所对应的节点名称 restore_op_name: "save/restore_all" #和持久性模型运算对应的是加载模型的运算的名称 max_to_keep:5 keep_checkpoint_every_n_hours :10000.0 ''' 上面两个属性设定了tf.train.Saver类清理之前保存的模型的策略。比如当max_to_keep为5时，第六次调用 saver.save时，第一次保存的模型就会被自动删除，通过设置keep_checkpoint_every_n_hours，每n小 时可以在max_to_keep的基础上保存一个模型 ''' collection_def属性collection_def属性是一个集合名称到集合内容的映射，其中集合的名称为字符串，而集合内容为CollectionDef Protocol Buffer。以下代码给出CollectionDef类型的定义12345678910111213141516171819202122232425262728message CollectionDef &#123; message Nodelist &#123; #用于维护计算图上的节点集合 repeated string value = 1; &#125; message BytesList &#123; #维护字符串或者系列化之后的Procotol Buffer的集合。例如张量是通过Protocol Buffer表示的，而张量的集合是通过BytesList维护的。 repeated bytes value = 1 ; &#125; message Int64List &#123; repeated int64 value = 1[packed = true]; &#125; message FloatList &#123; repeated float value = 1[packed = true] ; &#125; message AnyList &#123; repeated google.protobuf.Any value= 1; &#125; oneof kind &#123; NodeList node_list = 1; BytesList bytes_lista = 2; Int64List int64_list = 3; Floatlist float_list = 4; AnyList any_list = 5; &#125;&#125; 下面给出了test.ckpt.meta.json文件中的collection_def属性的内容1234567891011121314151617181920collection_def &#123; #可训练变量的集合 key: "trainable_variables" value &#123; bytes_list &#123; value; "\n\004v1:0\022\tv1/Assign\032\tv1/read:0" value: "\n\004v2:0\022\tv2/Assign\032\cv2/read:0" &#125; &#125;&#125;collection_def &#123; #所有变量的集合 key: "variables" value &#123; bytes_list &#123; value:"\n\004v1:0\022\tv1/Assign\032\tv1/read:0" value:"\n\004v2:0\022\tv2/Assign\032\tv2/read:0" &#125; &#125;&#125;]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow模型持久化的代码实现]]></title>
    <url>%2F2017%2F11%2F16%2FTensorflow%E6%A8%A1%E5%9E%8B%E6%8C%81%E4%B9%85%E5%8C%96%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1.存储模型123456789101112import tensorflow as tfv1 = tf.Variable(tf.constant(1.0, shape=[1]),name="v1")v2 = tf.Variable(tf.constant(2.0, shape=[1]),name="v2")result = v1 + v2init_op = tf.global_variables_initializer()saver = tf.train.Saver()#声明tf.train.Saver()类用于保存模型with tf.Session() as sess: sess.run(init_op) saver.save(sess, "test/test.ckpt")#将模型保存于test文件夹中 运行该程序后，将在test文件中出现以下的文件 2.加载模型12345678910111213import tensorflow as tfv1 = tf.Variable(tf.constant(1.0, shape=[1]),name="v1")v2 = tf.Variable(tf.constant(4.0, shape=[1]),name="v2")result = v1 + v2saver = tf.train.Saver()with tf.Session() as sess: saver.restore(sess, "test/test.ckpt")#加载已经保存的模型，并通过已经保存的模型中的变量的值来计算加法 print(sess.run(result)) #计算输出结果为[3.]不是[5.]是因为该程序加载了之前保存的模型，所以计算的变量也是保存的模型中的变量 如果不希望重复定义变量以及运算，可以直接加载已经持久化的图12345678 import tensorflow as tfsaver = tf.train.import_meta_graph("test/test.ckpt.meta")with tf.Session() as sess: saver.restore(sess, "test/test.ckpt") #通过张量的名称来获取张量 print(sess.run(tf.get_default_graph().get_tensor_by_name("add:0"))) #输出[3.] 该程序默认保存和加载了Tensorflow计算图中定义的全部变量。 3.加载或者保存部分变量1saver = tf.train.Saver([v1])#该命令只用来加载变量v1 4.加载或者保存时对变量重命名123456789101112# 这里声明的变量名称和已经保存的模型中变量的名称不同V1=tf.Variable(tf.constant(1.0,shape=[1]),name="other-v1")V2=tf.variable(tf.constant (2.0，shape=[1]),name="other-v2")# 如果直接使用tf.train.Saver () 来加载模型会报变量找不到的错误。下面显示了报错信息:# tensorflow.python.framewotk.erors.NotFoundError: Tensor name "other-v2"# not found in checkpoint files /test/test.ckpt# 使用一个字典(aictionary) 来重命名变量可以就可以加载原来的模型了。这个字典指定了# 原来名称为v1的变量现在加载到变量v1中(名称为other-v1),名称为v2的变量# 加载到变量v2 中(名称为other-v2)。saver=tf.train.Saver(&#123;"v1"=v1,"v2":v2&#125;) 如果直接通过saver=tf.train.Saver默认构造的函数来加载保存的模型，那么程序会报变量找不到的错误。因为保存时候的变量和加载时的变量的名称不一致。因此可以通过字典来将保存时和加载是的变量联系起来。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow的变量管理]]></title>
    <url>%2F2017%2F11%2F10%2FTensorflow%E7%9A%84%E5%8F%98%E9%87%8F%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Tensorflow中常见的变量初始化函数 初始化函数 功能 主要参数 tf.constant_initializer 将变量初始化为给定的常量 常量的取值 tf.random_normal_initializer 将变量初始化为满足正态分布的随机值 正态分布的均值和标准差 tf.truncated_normal_initializer 将变量初始化为满足正态分布的随机值，但如果随机出来的值偏离平均值超过2个标准差，那么这个数将会被重新随机 正态分布的均值和标准差 tf.random_uniform_initializer 将变量初始化为满足平均分布的随机值 最大，最小值 tf.uniform_unit_scaling_initializer 将变量初始化为满足平均分布但不影响输出量级的随机值 factor(产生随机值时乘以的系数) tf.zero_initializer 将变量设置为0 变量维度 tf.ones_initializer 将变量设置为1 变量维度 使用tf.variable_scope函数来控制tf.get_variable函数获取已经创建的函数1234567891011121314151617181920#在名字为foo的命名空间内创建名字为v的变量with tf.variable_scope("foo"): v = tf.get_variable("v", [1], initializer=tf.constant_initializer(1.0))'''命名空间foo中已经存在变量v，所以下面的代码将会报错:Variable foo/v already exitst,disallowed.Did you mean to set reuse=True in VarScope?''' #with tf.variable_scope("foo"): # v = tf.get_variable("v", [1])#在生成上下文管理器时，将参数reuse 设置为True.这样tf.get_variable 函数将直接获取已经声明的变量 with tf.variable_scope("foo", reuse=True): v1 = tf.get_variable("v", [1])print v == v1 #输出为True,代表v,v1代表的是相同的TensorFlow 中变量。'''将参数reuse 设置为True 时，tf.variable_scope 将只能获取已经创建过的变量。因为在命名空间bar 中还没有创建变量v，所以下面的代码将会报错:Variable foo/v not exitst,disallowed.Did you mean to set reuse=None in VarScope?'''#with tf.variable_scope("bar", reuse=True): # v = tf.get_variable("v", [1]) 上面的样例简单地说明了通过tf.variable_scope 函数可以控制tf.get_variable函数的语义。当tf.variable_scope 函数使用参数reuse=True 生成上下文管理器时，这个上下文管理器内所有的tf.get_variable 函数会直接获取已经创建的变量。如果变量不存在，则tf.get_variable函数将报错; 相反，如果tf.variabe_scope 函数使用参数reuse=None 或者reuse=False 创建上下文管理器，tf.get_variable 操作将创建新的变量。如果同名的变量已经存在，则tf.get_variable函数将报错。TensorFlow 中tf.variable_scope函数是可以嵌套的。下面的程序说明了当tf.variable_scope函数嵌套时，reuse参数的取值是如何确定的。12345678910111213141516with tf.variable_scope("root"):#可以通过tf.variable_scope()函数来获取当前上下文管理器中reuse参数的取值 print tf.get_variable_scope().reuse#输出False，即最外层reuse为False。 #新建一个嵌套的上下文管理器，并指定reuse 为True。 with tf.variable_scope("foo", reuse=True): print tf.get_variable_scope().reuse #新建一个嵌套的上下文管理器但不指定reuse,这时reuse的取值会和外面一层保持一致。 with tf.variable_scope("bar"): print tf.get_variable_scope().reuse#输出True。 print tf.get_variable_scope().reuse #输出False。退出reuse 设置为True 的上下文之后的值又回到了False。 tf.variable_scope函数生成的上下文管理器也会创建个TensorFlow中的命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前级缀。所以，tf.variable_scope函数除了可以控制tf.get_variable执行的功能之外，这个函数也提供了一个管理变量命名空间的方式。以下代码显示了如何通过tf.variable_scope来管理变量的名称。123456789101112131415161718192021222324252627v1 = tf.get_variable("v", [1])print v1.name#输出v:0"v" 为变量的名称，“:0”表示这个变量是生成变量这个运算的第一个结果。with tf.variable_scope("foo",reuse=True): v2 = tf.get_variable("v", [1])print v2.name#输出foo/v:O.在tf.variabie_scope中创建的变量，名称前面会加入命名空间的名称，并通过/来分隔命名空间的名称和变量的名称。with tf.variable_scope("foo"): with tf.variable_scope("bar"): v3 = tf.get_variable("v", [1]) print v3.name#输出foo/bar 'v :0。命名空间可以嵌套，同时变量的名称也会加入所有命名空间的名称作为前缀。 v4 = tf.get_variable("v1", [1])print v4.name#输出foo/v1:0。当命名空间退出之后，变量名称也就不会再被加入其前缀了。#创建一个名称为空的命名空间，并设置为reuse=Truewith tf.variable_scope("",reuse=True): v5 = tf.get_variable("foo/bar/v", [1]) ''' 可以直接通过带命名空间名称的变量名来获取其他命名空 间下的变量。比如这里通过指定名称foo/bar/v来获取在命名空间foo/bar/中创建的变量。 ''' print v5 == v3 #输出True。 v6 = tf.get_variable("v1", [1]) print v6 == v4 #输出True.]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium下载文件]]></title>
    <url>%2F2017%2F11%2F09%2Fselenium%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[一、Firefox文件下载 Web容许我们设置默认的文件下载路劲，文件会自动下载并且存放在指定的目录下。12345678910111213from selenium import webdriverimport osfp = webdriver.FirefoxProfile()fp.set_preference("browser.download.folderList",0)fp.set_preference("browser.download.manager.showhenStarting",True)fp.set_preference("browser.download.dir",os.getcwd())fp.set_preference("browser.helperApps.neverAsk.saveToDisk","applaction/octet-stream")#下载文件类型driver = webdriver.Firefox(firefox_profile = fp)driver.get("http://pypi.Python.org/pypi/selenium")driver.find_element_by_xpath("//*[@id='download-button']/a").click() driver.find_element_by_xpath("//*[@id='content']/div[3]/table/tbody/tr[3]/td[1]/span/a[1]").click() 为了让Firefox浏览器能实现文件下载，需要通过FirefoxProfile（）对其做一些设置。browser.download.foladerList :设置成0代表下载到浏览器默认下载路径，设置成2则可以保存到指定的目录。browser.download.manager.showWhenStarting :是否显示开始：True为显示开始，Flase为不显示开始。browser.download.dir :用于指定所下载文件的目录。os.getcwd（）函数不需要传递参数。用于返回当前的目录。browser.helperApps.neverAsk.saveToDisk :对所给文件类型不再弹出框进行询问。12345678910111213141516from selenium import webdriverfrom time import sleepprofile = webdriver.FirefoxProfile()profile.set_preference('browser.download.dir', 'd:\\') #现在文件存放的目录profile.set_preference('browser.download.folderList', 2)profile.set_preference('browser.download.manager.showWhenStarting', False)profile.set_preference('browser.helperApps.neverAsk.saveToDisk', 'application/zip')driver = webdriver.Firefox(firefox_profile=profile)driver.get('http://sahitest.com/demo/saveAs.htm')#driver.find_element_by_xpath('//a[text()="testsaveas.zip"]').click()driver.find_element_by_xpath('/html/body/a[1]').click()sleep(20)driver.quit() 二、Chrome文件下载 download.default_directory:设置下载路径profile.default_content_settings.popups:设置为0禁止弹出窗口1234567891011from selenium import webdriver from time import sleepoptions = webdriver.ChromeOptions() prefs = &#123;'profile.default_content_settings.popups': 0, 'download.default_directory': 'd:\\'&#125;options.add_experimental_option('prefs', prefs) driver = webdriver.Chrome(executable_path='D:\\chromedriver.exe', chrome_options=options) driver.get('http://sahitest.com/demo/saveAs.htm') driver.find_element_by_xpath('//a[text()="testsaveas.zip"]').click() sleep(3) driver.quit()]]></content>
      <categories>
        <category>selenium</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium中的常见报错]]></title>
    <url>%2F2017%2F11%2F09%2Fselenium%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[1.1ImportError: cannot import name ‘webdriver&apos; 1selenium.common.exceptions.WebDriverException: Message: &apos;chromedriver&apos; executable needs to be in PATH 1selenium.common.exceptions.WebDriverException: Message: &apos;chromedriver.exe&apos; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home原因是由于没有将chromedriver.exe放在/usr/bin/目录里在导入是要注意： 第四行123456789101112131415161718192021222324driver = webdriver.Chrome(r&quot;/usr/bin/chromedriver”) ``` ##### 中应该是chromedrive.exe的路径---### 2.```python======================================================================ERROR: test_search_in_python_org (__main__.mytest)----------------------------------------------------------------------Traceback (most recent call last): File &quot;mystyle.py&quot;, line 12, in test_search_in_python_org driver.get(&quot;www.baidu.com&quot;) File &quot;/Users/zhangjia/anaconda/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 309, in get self.execute(Command.GET, &#123;&apos;url&apos;: url&#125;) File &quot;/Users/zhangjia/anaconda/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py&quot;, line 297, in execute self.error_handler.check_response(response) File &quot;/Users/zhangjia/anaconda/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py&quot;, line 194, in check_response raise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: unknown error: unhandled inspector error: &#123;&quot;code&quot;:-32000,&quot;message&quot;:&quot;Cannot navigate to invalid URL&quot;&#125; (Session info: chrome=61.0.3163.100) (Driver info: chromedriver=2.32.498537 (cb2f855cbc7b82e20387eaf9a43f6b99b6105061),platform=Mac OS X 10.12.6 x86_64)----------------------------------------------------------------------Ran 1 test in 1.896sFAILED (errors=1) 错误原因： 第十二行应该是http://www.baidu.com不是www.baidu.com]]></content>
      <categories>
        <category>selenium</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[selenium中如何定位伪元素]]></title>
    <url>%2F2017%2F11%2F09%2Fselenium%E4%B8%AD%E5%A6%82%E4%BD%95%E5%AE%9A%E4%BD%8D%E4%BC%AA%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[定位方法：driver.find_element_by_css_selector(‘div.panel-body&gt;div’)]]></content>
      <categories>
        <category>selenium</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python字符串前缀 u和r的区别]]></title>
    <url>%2F2017%2F11%2F09%2Fpython%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%89%8D%E7%BC%80-u%E5%92%8Cr%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[在python2里面，u表示unicode string，类型是unicode, 没有u表示byte string，类型是 str。在python3里面，所有字符串都是unicode string, u前缀没有特殊含义了。r都表示raw string. 与特殊字符的escape规则有关，一般用在正则表达式里面。r和u可以搭配使用，例如ur”abc”。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3格式化输出]]></title>
    <url>%2F2017%2F11%2F09%2Fpython3%E6%A0%BC%E5%BC%8F%E5%8C%96%E8%BE%93%E5%87%BA%2F</url>
    <content type="text"><![CDATA[python格式化字符串有%和{}两种 字符串格式控制符. 字符串输入数据格式类型(%格式操作符号)123456789101112131415%% 百分号标记%c 字符及其ASCII码%s 字符串%d 有符号整数(十进制)%u 无符号整数(十进制)%o 无符号整数(八进制)%x 无符号整数(十六进制)%X 无符号整数(十六进制大写字符)%e 浮点数字(科学计数法)%E 浮点数字(科学计数法，用E代替e)%f 浮点数字(用小数点符号)%g 浮点数字(根据值的大小采用%e或%f)%G 浮点数字(类似于%g)%p 指针(用十六进制打印值的内存地址)%n 存储输出字符的数量放进参数列表的下一个变量中 字符串格式控制%[(name)][flag][width][.][precision]typename:可为空，数字(占位),命名(传递参数名,不能以数字开头)以字典格式映射格式化，其为键名flag:标记格式限定符号,包含+-#和0,+表示右对齐(会显示正负号),-左对齐,前面默认为填充空格(即默认右对齐)，0表示填充0，#表示八进制时前面补充0,16进制数填充0x,二进制填充0bwidth:宽度(最短长度,包含小数点,小于width时会填充)precision:小数点后的位数,与C相同type:输入格式类型，请看上面还有一种format_spec格式12345678910&#123;[name][:][[fill]align][sign][#][0][width][,][.precision][type]&#125;用&#123;&#125;包裹name命名传递给format以命名=值 写法,非字典映射,其他和上面相同fill = &lt;any character&gt; #fill是表示可以填写任何align = &quot;&lt;&quot; | &quot;&gt;&quot; | &quot;=&quot; | &quot;^&quot; #align是对齐方式，&lt;是左对齐， &gt;是右对齐，^是居中对齐。sign = &quot;+&quot; | &quot;-&quot; | &quot; &quot; #sign是符号， +表示正号， -表示负号width = integer #width是数字宽度，表示总共输出多少位数字precision = integer #precision是小数保留位数type = &quot;b&quot; | &quot;c&quot; | &quot;d&quot; | &quot;e&quot; | &quot;E&quot; | &quot;f&quot; | &quot;F&quot; | &quot;g&quot; | &quot;G&quot; | &quot;n&quot; | &quot;o&quot; | &quot;s&quot; | &quot;x&quot; | &quot;X&quot; | &quot;%&quot; #type是输出数字值是的表示方式比如b是二进制表示；比如E是指数表示；比如X是十六进制表示例子 ###例子12345678910111213141516171819202122232425print("&#123;:,&#125;".format(123456))#输出1234,56print("&#123;a:w^8&#125;".format(a="8"))#输出www8wwww,填充wprint("%.5f" %5)#输出5.000000print("%-7s3" %("python"))#输出python 3print("%.3e" %2016)#输出2.016e+03,也可以写大Eprint("%d %s" %(123456,"myblog"))#输出123456 myblogprint("%(what)s is %(year)d" % &#123;"what":"this year","year":2016&#125;)#输出this year is 2016 print("&#123;0&#125;&#123;1&#125;".format("hello","fun"))#输出hellofun,这与CSharp的格式化字符(占位符)相似print("&#123;&#125;&#123;&#125;&#123;&#125;".format("spkk",".","cn"))#输出spkk.cnprint("&#123;a[0]&#125;&#123;a[1]&#125;&#123;a[2]&#125;".format(a=["spkk",".","cn"]))#输出spkk.cnprint("&#123;dict[host]&#125;&#123;dict[dot]&#125;&#123;dict[domain]&#125;".format(dict=&#123;"host":"www","domain":"spkk.cn","dot":"."&#125;))#输出www.spkk.cnprint("&#123;a&#125;&#123;b&#125;".format(a="python",b="3"))#输出python3 print("&#123;who&#125; &#123;doing&#125; &#123;0&#125;".format("python",doing="like",who="I"))#输出I like python]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow中一些常见的函数]]></title>
    <url>%2F2017%2F11%2F09%2FTensorflow%E4%B8%AD%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Tensorflow中一些常见的函数 1.tf.constant(value,dtype=None,shape=None,name=&#39;Const&#39;)创建一个常量tensor，按照给出value来赋值，可以用shape来指定其形状。value可以是一个数，也可以是一个list。 如果是一个数，那么这个常亮中所有值的按该数来赋值。 如果是list,那么len(value)一定要小于等于shape展开后的长度。赋值时，先将value中的值逐个存入。不够的部分，则全部存入value的最后一个值。 2.tf.nn.relu####使用激活函数relu进行计算 类似的还有 tf.sigmoid , tf.tanh3.tf.matmul进行矩阵相乘 4.tf.placeholder运行时传入参数,placeholder（type,strucuct…)它的第一个参数是你要保存的数据的数据类型，大多数是tensorflow中的float32数据类型，后面的参数就是要保存数据的结构，比如要保存一个1×2的矩阵，则struct=[1 2]。 5.tf.Variable（initializer， name)initializer是初始化参数，可以为tf.random_normal，tf.constant，tf.constant等，name就是变量的名字，用法如下：12345678910111213import tensorflow as tf;import numpy as np; import matplotlib.pyplot as plt; a1 = tf.Variable(tf.random_normal(shape=[2,3], mean=0, stddev=1), name='a1') a2 = tf.Variable(tf.constant(1), name='a2') a3 = tf.Variable(tf.ones(shape=[2,3]), name='a3') with tf.Session() as sess: sess.run(tf.initialize_all_variables()) print sess.run(a1) print sess.run(a2) print sess.run(a3) 输出的结果如下：123456输出：[[ 0.76599932 0.99722123 -0.89361787] [ 0.19991693 -0.16539733 2.16605783]]1[[ 1. 1. 1.] [ 1. 1. 1.]] 6.tf.truncated_normal(shape, mean, stddev)shape表示生成张量的维度，mean是均值，stddev是标准差。这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。###示例代码：12345678import tensorflow as tf; import numpy as np; import matplotlib.pyplot as plt; c = tf.truncated_normal(shape=[10,10], mean=0, stddev=1) with tf.Session() as sess: print sess.run(c) 输出结果如下：123456789101112131415161718192021输出：[[ 1.95758033 -0.68666345 -1.83860338 0.78213859 -1.08119416 -1.44530308 0.38035342 0.57904619 -0.57145643 -1.22899497] [-0.75853795 0.48202974 1.03464043 1.19210851 -0.15739718 0.8506189 1.18259966 -0.99061841 -0.51968449 1.38996458] [ 1.05636907 -0.02668529 0.64182931 0.4110294 -0.4978295 -0.64912242 1.27779591 -0.01533993 0.47417602 -1.28639436] [-1.65927458 -0.364887 -0.45535028 0.078814 -0.30295736 1.91779387 -0.66928798 -0.14847915 0.91875714 0.61889237] [-0.01308221 -0.38468206 1.34700036 0.64531708 1.15899456 1.09932268 1.22457981 -1.1610316 0.59036094 -1.97302651] [-0.24886213 0.82857937 0.09046989 0.39251322 0.21155456 -0.27749416 0.18883201 0.08812679 -0.32917103 0.20547724] [ 0.05388507 0.45474565 0.23398806 1.32670367 -0.01957406 0.52013856 -1.13907862 -1.71957874 0.75772947 -1.01719368] [ 0.27155915 0.05900437 0.81448066 -0.37997526 -0.62020499 -0.88820189 1.53407145 -0.01600445 -0.4236775 -1.68852305] [ 0.78942037 -1.32458341 -0.91667277 -0.00963761 0.76824385 -0.5405798 -0.73307443 -1.19854116 -0.66179073 0.26329204] [ 0.59473759 -0.37507254 -1.21623695 -1.30528259 1.18013096 -1.32077384 -0.59241474 -0.28063133 0.12341146 0.48480138]] 7.tf.train.ExponentialMovingAverage这个函数用于更新参数，就是采用滑动平均的方法更新参数。这个函数初始化需要提供一个衰减速率（decay），用于控制模型的更新速度。这个函数还会维护一个影子变量（也就是更新参数后的参数值），这个影子变量的初始值就是这个变量的初始值，影子变量值的更新方式如下：1shadow_variable = decay * shadow_variable + (1-decay) * variable shadow_variable是影子变量，variable表示待更新的变量，也就是变量被赋予的值，decay为衰减速率。decay一般设为接近于1的数（0.99,0.999）。decay越大模型越稳定，因为decay越大，参数更新的速度就越慢，趋于稳定。tf.train.ExponentialMovingAverage这个函数还提供了自己动更新decay的计算方式：decay= min（decay，（1+steps）/（10+steps））其中steps是迭代的次数，可以自己设定。示例代码如下：123456789101112131415161718192021222324252627import tensorflow as tf; import numpy as np; import matplotlib.pyplot as plt; v1 = tf.Variable(0, dtype=tf.float32) step = tf.Variable(tf.constant(0)) ema = tf.train.ExponentialMovingAverage(0.99, step) maintain_average = ema.apply([v1]) with tf.Session() as sess: init = tf.initialize_all_variables() sess.run(init) print sess.run([v1, ema.average(v1)]) #初始的值都为0 sess.run(tf.assign(v1, 5)) #把v1变为5 sess.run(maintain_average) print sess.run([v1, ema.average(v1)]) # decay=min(0.99, 1/10)=0.1, v1=0.1*0+0.9*5=4.5 sess.run(tf.assign(step, 10000)) # steps=10000 sess.run(tf.assign(v1, 10)) # v1=10 sess.run(maintain_average) print sess.run([v1, ema.average(v1)]) # decay=min(0.99,(1+10000)/(10+10000))=0.99, v1=0.99*4.5+0.01*10=4.555 sess.run(maintain_average) print sess.run([v1, ema.average(v1)]) #decay=min(0.99,&lt;span style="font-family: Arial, Helvetica, sans-serif;"&gt;(1+10000)/(10+10000)&lt;/span&gt;&lt;span style="font-family: Arial, Helvetica, sans-serif;"&gt;)=0.99, v1=0.99*4.555+0.01*10=4.6&lt;/span&gt; 输出结果如下：12345输出：[0.0, 0.0][5.0, 4.5][10.0, 4.5549998][10.0, 4.6094499] 解释：每次更新完以后，影子变量的值更新，varible的值就是你设定的值。如果在下一次运行这个函数的时候你不在指定新的值，那就不变，影子变量更新。如果指定，那就variable改变，影子变量也改变 8.tf.clip_by_value将张量中的一个值限制在一个范围之内如：12a = tf.constant([[1.0,2.0,3.0],[4.0,5.0,6.0]])print(tf.clip_by_value(a,2.5,4.5).eval()) 输出[[2.5,2.5,3],[4,4.5,4.5]]由此可见小于2.5的换成了2.5，大于4.5的都换成了4.5Tensor.eval() 和 Operation.run() 方法代替 Session.run()。 9.tf.reduce_mean指求平均值如：tf.reduce_mean(([1,2,3],[4,5,6]))等于：（1+2+3+4+5+6）／6 = 3.5 10.p * tf.log(q)交叉熵损失函数 H(p,q) = -求和符(p(x)log(q(x)))可以直接用Tensorflow交叉熵函数来计算 p * tf.log(q) 11.tf.nn.softmax_cross_entropy_with_logits(y，y_) 直接通过这个公式来实现softmax回归之后的交叉熵损失函数（y指原是神经网络的输出结果，y_指标准答案）12.tf.reduce_sum该函数用来求和 13.tf.greater输入两个张量并比较大小，当维度不同时，进行类似于numpy广播的处理，当第一个参数大于第二个是为True反之为False 14.tf.select####（目前已经更新为tf.where）第一个参数为选择条件，当满足时使用第二个参数，不满足时返回第三个参数15.tf.train.AdamOptimizer该函数来控制学习速度。AdamOptimizer 通过使用动量（参数的移动平均数）来改善传统梯度下降，促进超参数动态调整。 16.tf.contrib.layers.l1_regularize可以计算解决过拟合化问题的l1正则化项的值 #lambda表示正则化的权重 17.tf.contrib.layers.l2_regularize可以计算解决过拟合化问题的l2正则化项的值 18.tf.add_to_collection把变量放入一个集合，把很多变量变成一个列表 19.tf.train.ExpontialMovingAverage实现滑动平均模型 20.tf.assign(x,y)将y的值赋值给x&nbsp; 30.12mnist = input_data.resd_data_sets("文件路径”) mnist.train.next_batch(batch_size) #input_data.resd_data_sets 函数生成的类还提供了mnist.train.next_batch函数，他将从所有的训练数据中读取一小部分作为训练数据batch 31.tf.trainable_variables返回的是需要训练的变量列表tf.all_variables返回的是所有变量的列表例如：1234567891011121314import tensorflow as tf; import numpy as np; import matplotlib.pyplot as plt; v = tf.Variable(tf.constant(0.0, shape=[1], dtype=tf.float32), name='v') v1 = tf.Variable(tf.constant(5, shape=[1], dtype=tf.float32), name='v1') global_step = tf.Variable(tf.constant(5, shape=[1], dtype=tf.float32), name='global_step', trainable=False) ema = tf.train.ExponentialMovingAverage(0.99, global_step) for ele1 in tf.trainable_variables(): print ele1.name for ele2 in tf.all_variables(): print ele2.name 输出结果如下：123456输出：v:0v1:0v:0v1:0global_step:0 分析：上面得到两个变量，后面的一个得到上三个变量，因为global_step在声明的时候说明不是训练变量，用来关键字trainable=False。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow不同版本引起的错误]]></title>
    <url>%2F2017%2F11%2F08%2FTensorFlow%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC%E5%BC%95%E8%B5%B7%E7%9A%84%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[前4个是 V0.11 的API 用在 V1.0 的错误 AttributeError: ‘module’ object has no attribute ‘merge_all_summaries’ tf.merge_all_summaries() 改为：summary_op = tf.summary.merge_all() AttributeError: ‘module’ object has no attribute ‘SummaryWriter’ tf.train.SummaryWriter 改为：tf.summary.FileWriter AttributeError: ‘module’ object has no attribute ‘scalar_summary’ tf.scalar_summary 改为：tf.summary.scalar AttributeError: ‘module’ object has no attribute ‘histogram_summary’ histogram_summary 改为：tf.summary.histogram 下边这个是 V1.0 的API 用在 V0.11 的错误File “dis-alexnet_benchmark.py”, line 110, in alexnet_v2 biases_initializer=tf.zeros_initializer(),TypeError: zeros_initializer() takes at least 1 argument (0 given) 将 biases_initializer=tf.zeros_initializer() 改为：biases_initializer=tf.zeros_initializer 本文章为转载原文章链接为：http://blog.csdn.net/s_sunnyy/article/details/70999462]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用TensorBoard进行数据可视化]]></title>
    <url>%2F2017%2F11%2F08%2F%E4%BD%BF%E7%94%A8TensorBoard%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[使用TensorBoard进行数据可视化 除了python的matplotlib这个数据图像化的包，我们还可以使用Tensorflow自带的TensorBoard来将数据进行可视化，实现的代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import tensorflow as tfimport numpy as npdef add_layer(inputs, in_size, out_size, n_layer, activation_function=None): layer_name = 'layer%s' % n_layer with tf.name_scope("layer"): with tf.name_scope("weight"): Weights = tf.Variable(tf.random_normal([in_size,out_size]),name="W") tf.summary.histogram(layer_name+'/Weights',Weights) with tf.name_scope("biases"): biases = tf.Variable(tf.zeros([1, out_size])+0.1,name="b") tf.summary.histogram(layer_name+'/biases',biases) with tf.name_scope("Wx_plus_b"): Wx_plus_b = tf.matmul(inputs, Weights)+biases if activation_function == None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) tf.summary.histogram(layer_name+'/outputs',outputs) return outputsx_data = np.linspace(-1,1,300)[:,np.newaxis]noise = np.random.normal(0,0.05,x_data.shape)y_data = np.square(x_data) - 0.5 + noisewith tf.name_scope("inputs"): xs = tf.placeholder(tf.float32,[None,1],name="x_input") ys = tf.placeholder(tf.float32,[None,1],name="y_input") l1 = add_layer(xs, 1, 10, n_layer=1, activation_function=tf.nn.relu)prediction = add_layer(l1, 10, 1, n_layer=2, activation_function=None) with tf.name_scope("loss"): loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1]),name="loss") tf.summary.scalar('loss',loss)with tf.name_scope("train"): train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)init = tf.global_variables_initializer()sess = tf.Session()merged = tf.summary.merge_all()writer = tf.summary.FileWriter("logs/",sess.graph)sess.run(init)for i in range(1000): sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;) if i%50==0: result = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;) writer.add_summary(result,i) 代码进行保存以及运行之后，将会在指定文件夹中看到保存的文件，这是我们需要在终端中运行下面的命令，然后将网址复制到浏览器即可打开1$ tensorboard --logdir=&apos;logs/&apos; #&apos;logs&apos;是指保存的文件所在的文件夹 注： 1.1tf.name_scope("name") 该函数中的name是指GRAPHS中的 以及每个神经网络的节点打开后里面的各种量的名字这是部分节点点开后的样子 这是所有节点点开后的样子 2.1tf.summary.histogram(layer_name+'name',name) 该函数是用来绘制histogram中图可视化绘图 3.1tf.summary.scalar('name',name) 该函数主要适用于损失函数在scalar的可视化绘图 4.1tf.summary.FileWriter("logs/",sess.graph) 该函数是将TensorBoard文件保存到指定路径，例如”logs/“即是指路径，正常运行后将会在logs文件夹下产生一个文件 5.1tf.summary.merge_all() 该函数是将所有summary文件合并到同一个文件里并在下一步的tf.summary.FileWriter(“logs/“,sess.graph)进行保存 6.1add_summary(result,i) 该函数是将result这个数据加入到writer = tf.summary.FileWriter(“logs/“,sess.graph)这个函数的write中去，其中i是指步数，例如本函数中就是50布记录一个点 7.运行应该在ipython中运行，如果在jupyter中运行可能会产生报错]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
        <tag>TensorBoard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过python的matplotlib包将Tensorflow数据进行可视化]]></title>
    <url>%2F2017%2F11%2F08%2F%E9%80%9A%E8%BF%87python%E7%9A%84matplotlib%E5%8C%85%E5%B0%86Tensorflow%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[使用matplotlib中的一些函数将tensorflow中的数据可视化，更加便于分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltdef add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1, 1, 300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediction and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step#initialize_all_variables已被弃用,使用tf.global_variables_initializer代替。 init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion() #使plt不会在show之后停止而是继续运行plt.show()for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) #在每一次绘图之前先讲上一次绘图删除，使得画面更加清晰 except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, 'r-', lw=5) #'r-'指绘制一个红色的线 plt.pause(1) #指等待一秒钟 运行结果如下：（实际效果应该是动态的，应当使用ipython运行，使用jupyter运行则图片不是动态的） 注意：initialize_all_variables已被弃用,使用tf.global_variables_initializer代替。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow进行简单的神经网络的尝试（建造神经网络）]]></title>
    <url>%2F2017%2F11%2F08%2FTensorflow%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8D%95%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B0%9D%E8%AF%95%EF%BC%88%E5%BB%BA%E9%80%A0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[通过对一个简单的函数的轨迹的预测来进行一个较为简单的神经网络的构建1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#导入程序所需要的包import tensorflow as tfimport numpy as np#该函数用来进行添加神经网络的层数def add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1,1,300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediction and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split('.')[1]) &lt; 12: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;)) 运行的结果如下： 主要的注意事项：了解tensorflow的Session会话，懂得各个神经网络层间的数据关系]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python matplotlib中常用函数]]></title>
    <url>%2F2017%2F11%2F04%2Fpython-matplotlib%E4%B8%AD%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[python matplotlib中常用函数 绘图 常见设置 散点图 柱状图 等高线 matplotlib绘制3D图 子图像 动态图 绘图123import matplotlib.pyplot as plt #倒入matplotlib库plt.plot(x,y) #绘制y关于x的函数plt.show() #将图形绘出 常见设置设置坐标轴一、1234567891011121314151617# x轴和y轴的值域plt.xlim((-1,2))plt.ylim((-2,3))# color为线的颜色，linewidth为线宽度，linestyle为样式（-为实线，--为虚线）plt.plot(x,y,color='red',linewidth=1.0,linestyle='—')plt.figure #绘制一个新画布plt.figsize #花布尺寸# x和y轴plt.xtick()plt.ytick()例如：plt.xticks(new_ticks) #new_ticks 为-2，2分成十一等份plt.yticks([-1,0,1,2,3], ['level2','level2','level3','level4','level5']) 二、1234567891011121314plt.gca #获取当前的坐标轴spines['right'].set_color('red’) #右边框为红色# 分别把x轴与y轴的刻度设置为bottom与leftxaxis.set_ticks_position('bottom')yaxis.set_ticks_position('left’)# 分别v把bottom和left类型设置为data，交点为（0，0）spines['bottom'].set_position(('data',0))spines['left'].set_position(('data',0))例如：ax = plt.gca()ax.spines['right'].set_color(‘red')ax.spines['top'].set_color(‘red’) 三、123l1, = plt.plot(x,y1,color='red',linewidth=1.0,linestyle='—') #设置两条线为l1,l2 注：应该在后面加上，l2, = plt.plot(x,y2,color="blue",linewidth=5.0,linestyle="-")plt.legend(handles=[l1,l2],labels=['test1','test2'],loc='best’) #将l1，l2绘制于一张图中，其中名字分别是l1，l2，位置自动取在最佳位置 设置备注12345678910x0 = 0.5y0 = 2*x0 + 1# 画点plt.scatter(x0,y0,s=50,color='blue')# 画虚线plt.plot([x0,x0],[y0,0],'k--',lw=2)#[x0,x0],[y0,0]代表x0，y0点作虚线交于x0，0 k--代表颜色的虚线,lw代表宽度plt.annotate(r'$2x+1=%s$' % y0,xy=(x0,y0),xytext=(+30,-30),textcoords='offset points',fontsize=16,arrowprops=dict(arrowstyle='-&gt;',connectionstyle='arc3,rad=.2'))#xy=(x0,y0)指在x0，y0点，xytext=(+30,-30)指在点向右移动30，向下移动30,textcoords='offset points'指以点为起点#arrowprops=dict(arrowstyle='-&gt;',connectionstyle='arc3,rad=.2')指弧度曲线， .2指弧度plt.text(-2,2,r'$This\ is\ the\ text$',fontsize=16,color='red’) #-2,2指从-2，2开始写 散点图1234x = np.random.normal(0,1,500)y = np.random.normal(0,1,500)plt.scatter(x,y,s=50,color='blue',alpha=0.5) #s指点大小，alpha指透明度plt.show() 柱状图123456x = np.arange(10)y = 2**x + 10plt.bar(x,y,facecolor='#9999ff',edgecolor='white')#柱颜色，柱边框颜色for x,y in zip(x,y):#zip指把x，y结合为一个整体，一次可以读取一个x和一个y plt.text(x,y,'%.2f' % y,ha='center',va='bottom')#指字体在中间和柱最顶的顶部plt.show() 等高图12345678910111213141516def f(x,y): #用来生成高度 return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2)x = np.linspace(-3,3,100)y = np.linspace(-3,3,100)X,Y = np.meshgrid(x,y)#将x，y指传入网格中plt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8指图中的8+1根线，绘制等温线，其中cmap指颜色C = plt.contour(X,Y,f(X,Y),8,colors='black',linewidth=.5)#colors指等高线颜色plt.clabel(C,inline=True,fontsize=10)#inline=True指字体在等高线中plt.xticks(())plt.yticks(())plt.show() matplotlib绘制3D图123456789101112131415from mpl_toolkits.mplot3d import Axes3D#动态图所需要的包fig = plt.figure()ax = Axes3D(fig)x = np.arange(-4,4,0.25)#0.25指-4至4间隔为0.25y = np.arange(-4,4,0.25)X,Y = np.meshgrid(x,y)#x，y放入网格R = np.sqrt(X**2 + Y**2)Z = np.sin(R)ax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap('rainbow'))#rstride=1指x方向和y方向的色块大小ax.contourf(X,Y,Z,zdir='z',offset=-2,cmap='rainbow')#zdir指映射到z方向，-2代表映射到了z=-2ax.set_zlim(-2,-2)plt.show() 子图像12345678910plt.figure()plt.subplot(2,2,1)#建立一个两行两列的画布，第一个plt.plot([0,1],[0,1])plt.subplot(2,2,2)#第二个plt.plot([0,1],[0,1])plt.subplot(2,2,3)#第三个plt.plot([0,1],[0,1])plt.subplot(2,2,4)#第四个plt.plot([0,1],[0,1])plt.show() 12345678910plt.figure()plt.subplot(2,1,1)#建立一个两行两列的画布，第一个plt.plot([0,1],[0,1])plt.subplot(2,3,4)#第二个plt.plot([0,1],[0,1])plt.subplot(2,3,5)#第三个plt.plot([0,1],[0,1])plt.subplot(2,3,6)#第四个plt.plot([0,1],[0,1])plt.show() 动态图1234567891011121314from matplotlib import animation#动态图所需要的包fig,ax = plt.subplots()#子图像x = np.arange(0,2*np.pi,0.01)line, = ax.plot(x,np.sin(x))def animate(i): line.set_ydata(np.sin(x+i/10))#用来改变的y对应的值 return line,def init(): line.set_ydata(np.sin(x))#动态图初始图像 return line,ani = animation.FuncAnimation(fig=fig,func=animate,init_func=init,interval=20)#动态作图的方法，func动态图函数，init_func初始化函数，interval指图像改变的时间间隔plt.show() 注：若想看动态效果请在ipython中使用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
